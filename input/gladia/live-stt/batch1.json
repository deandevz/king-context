[
  {
    "title": "Live Transcription Quickstart",
    "path": "quickstart",
    "url": "https://docs.gladia.io/chapters/live-stt/quickstart",
    "keywords": [
      "live transcription",
      "real-time STT",
      "websocket",
      "audio streaming",
      "SDK",
      "speech-to-text",
      "live session",
      "audio chunks",
      "multi-channel"
    ],
    "use_cases": [
      "how to set up live transcription with Gladia",
      "how to send audio chunks via WebSocket",
      "how to handle multiple audio channels in real-time",
      "when to use partial vs final transcripts",
      "how to reconnect after WebSocket disconnection"
    ],
    "tags": ["live", "realtime", "streaming", "stt", "websocket", "quickstart"],
    "priority": 10,
    "content": "# Live Transcription Quickstart\n\nThe SDK simplifies real-time speech-to-text integration by abstracting the underlying API. Designed for developers, it offers:\n\n- Effortless implementation with minimal code to write.\n- Built-in resilience with automatic error handling (e.g., reconnection on network drops) ensures uninterrupted transcription. No need to manually manage retries or state recovery.\n\n## Install the SDK\n\n**JavaScript:**\n```bash\nnpm install @gladiaio/sdk\n```\n\n```javascript\nimport { GladiaClient } from \"@gladiaio/sdk\";\n```\n\n**Python:**\n```bash\npip install gladia\n```\n\n```python\nfrom gladia import GladiaClient\n```\n\n## Initiate your real-time session\n\nFirst, call the endpoint and pass your configuration. It's important to correctly define the properties `encoding`, `sample_rate`, `bit_depth` and `channels` as we need them to parse your audio chunks.\n\n**JavaScript:**\n```javascript\nconst gladiaClient = new GladiaClient({\n  apiKey: <YOUR_GLADIA_API_KEY>,\n});\n\nconst gladiaConfig = {\n  model: \"solaria-1\",\n  encoding: 'wav/pcm',\n  sample_rate: 16000,\n  bit_depth: 16,\n  channels: 1,\n  language_config: {\n    languages: [\"fr\"],\n    code_switching: false,\n  },\n};\n\nconst liveSession = gladiaClient.liveV2().startSession(gladiaConfig);\n```\n\n### Why initiate with POST instead of connecting directly to the WebSocket?\n\n- **Security**: Generate the WebSocket URL on your backend and keep your API key private. The init call returns a connectable URL and a session `id` that you can safely pass to web, iOS, or Android clients without exposing credentials in the app.\n- **Lower infrastructure load**: The secure URL is generated on your backend, the client can connect directly to Gladia's WebSocket server without a pass-through on your side, saving your own resources.\n- **Resilient reconnection and session continuity**: If the WebSocket disconnects (which can happen on unreliable networks), the session created by the init call lets the client reconnect without losing context.\n\n## Connect to the WebSocket\n\n**JavaScript:**\n```javascript\nliveSession.on(\"message\", (message) => {\n  // Handle messages from the API\n});\nliveSession.on(\"started\", (message) => {\n  // Handle start session message\n});\nliveSession.on(\"ended\", (message) => {\n  // Handle end session message\n});\nliveSession.on(\"error\", (message) => {\n  // Handle error message\n});\n```\n\n## Send audio chunks\n\n```javascript\nliveSession.sendAudio(audioChunk)\n```\n\n## Read messages\n\nDuring the whole session, we will send various messages through the WebSocket, the callback URL or webhooks. You can specify which kind of messages you want to receive in the initial configuration. See `messages_config` for WebSocket messages and `callback_config` for callback messages.\n\n```javascript\nliveSession.on(\"message\", (message) => {\n  if (message.type === 'transcript' && message.data.is_final) {\n    console.log(`${message.data.id}: ${message.data.utterance.text}`)\n});\n```\n\n**Need low-latency partial results?** Enable partial transcripts by setting `messages_config.receive_partial_transcripts: true`. Use the `is_final` property to distinguish between partial and final transcript messages.\n\n## Stop the recording\n\nOnce you're done, send us the `stop_recording` message. We will process remaining audio chunks and start the post-processing phase.\n\n```javascript\nliveSession.stopRecording()\n```\n\n## Get the final results\n\nIf you want to get the complete result, you can call the `GET /v2/live/:id` endpoint with the `id` you received from the initial request.\n\n```javascript\nconst response = await fetch(`https://api.gladia.io/v2/live/${sessionId}`, {\n  method: 'GET',\n  headers: {\n    'x-gladia-key': '<YOUR_GLADIA_API_KEY>',\n  },\n});\nconst result = await response.json();\nconsole.log(result)\n```\n\n## Using the API directly (without SDK)\n\n### Initiate your real-time session\n\n```javascript\nconst response = await fetch(\"https://api.gladia.io/v2/live\", {\n  method: \"POST\",\n  headers: {\n    \"Content-Type\": \"application/json\",\n    \"x-gladia-key\": \"<YOUR_GLADIA_API_KEY>\",\n  },\n  body: JSON.stringify({\n    encoding: \"wav/pcm\",\n    sample_rate: 16000,\n    bit_depth: 16,\n    channels: 1,\n  }),\n});\n\nconst { id, url } = await response.json();\n```\n\nResponse example:\n```json\n{\n  \"id\": \"636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n  \"url\": \"wss://api.gladia.io/v2/live?token=636c70f6-92c1-4026-a8b6-0dfe3ecf826f\"\n}\n```\n\n### Connect to the WebSocket\n\n```javascript\nimport WebSocket from \"ws\";\n\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function () {\n  // Connection is opened. You can start sending audio chunks.\n});\n\nsocket.addEventListener(\"error\", function (error) {\n  // An error occurred during the connection.\n});\n\nsocket.addEventListener(\"close\", function ({ code, reason }) {\n  // The connection has been closed\n  // If the \"code\" is equal to 1000, it means we closed intentionally the connection.\n  // Otherwise, you can reconnect to the same url.\n});\n\nsocket.addEventListener(\"message\", function (event) {\n  const message = JSON.parse(event.data.toString());\n  console.log(message);\n});\n```\n\n### Send audio chunks\n\n```javascript\n// as binary\nsocket.send(buffer);\n\n// as json\nsocket.send(\n  JSON.stringify({\n    type: \"audio_chunk\",\n    data: {\n      chunk: buffer.toString(\"base64\"),\n    },\n  })\n);\n```\n\n## Sending multiple audio tracks in real-time\n\nIf you have multiple audio sources (like different participants in a conversation) that you need to transcribe simultaneously, you can merge these separate audio tracks into a single multi-channel audio stream and send it over one WebSocket connection.\n\n### Merging multiple audio tracks into one multi-channel WebSocket\n\nBenefits:\n- Reduce the number of WebSocket connections from multiple to just one\n- Maintain speaker identity through channel mapping\n- Simplify synchronization of audio streams from multiple participants\n- Reduce network overhead and connection management complexity\n\n### Creating a multi-channel audio stream\n\n```typescript\nexport function interleaveAudio(channelsData: Buffer[], bitDepth = 16): Buffer {\n  const nbChannels = channelsData.length;\n  if (nbChannels === 1) {\n    return channelsData[0];\n  }\n\n  const bytesPerSample = bitDepth / 8;\n  const samplesPerChannel = channelsData[0].byteLength / bytesPerSample;\n  const audio = Buffer.alloc(nbChannels * samplesPerChannel * bytesPerSample);\n\n  for (let i = 0; i < samplesPerChannel; i++) {\n    for (let j = 0; j < nbChannels; j++) {\n      const sample = channelsData[j].subarray(\n        i * bytesPerSample,\n        (i + 1) * bytesPerSample\n      );\n      audio.set(sample, (i * nbChannels + j) * bytesPerSample);\n    }\n  }\n\n  return audio;\n}\n```\n\n### Example use case\n\nConsider a scenario with three participants in a room: Sami, Maxime, and Mark.\n\n```typescript\n// Collect audio buffers from each participant\nconst samiAudio = getSamiAudioBuffer();\nconst maximeAudio = getMaximeAudioBuffer();\nconst markAudio = getMarkAudioBuffer();\n\n// Merge into a multi-channel audio\n// Channel ordering: [0]=Sami, [1]=Maxime, [2]=Mark\nconst channelsData = [samiAudio, maximeAudio, markAudio];\nconst mergedAudio = interleaveAudio(channelsData, 16);\n\n// Initialize a single WebSocket session with multi-channel config\nconst response = await fetch(\"https://api.gladia.io/v2/live\", {\n  method: \"POST\",\n  headers: {\n    \"Content-Type\": \"application/json\",\n    \"x-gladia-key\": \"<YOUR_GLADIA_API_KEY>\",\n  },\n  body: JSON.stringify({\n    encoding: \"wav/pcm\",\n    sample_rate: 16000,\n    bit_depth: 16,\n    channels: 3, // Specify the number of channels\n  }),\n});\n\nconst { url } = await response.json();\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function () {\n  socket.send(mergedAudio);\n});\n```\n\n### Understanding the response\n\nEach transcription message will include a `channel` field that indicates which audio channel (and thus which participant) the transcription belongs to:\n\n```json\n{\n  \"type\": \"transcript\",\n  \"session_id\": \"de70f43f-3041-46e0-892c-8e7f53800a22\",\n  \"created_at\": \"2025-04-09T08:44:16.471Z\",\n  \"data\": {\n    \"id\": \"00_00000000\",\n    \"utterance\": {\n      \"text\": \"Hello, I'm Sami. I'm the first speaker\",\n      \"start\": 0.188,\n      \"end\": 2.852,\n      \"language\": \"en\",\n      \"channel\": 0\n    }\n  }\n}\n```\n\nThe channel numbers correspond to the order in which you added the audio tracks:\n- Channel 0 -> Sami (first in the array)\n- Channel 1 -> Maxime (second in the array)\n- Channel 2 -> Mark (third in the array)\n\n## Full code samples\n\nYou can find complete code samples in the Github repository:\n- [Typescript/Javascript](https://github.com/gladiaio/gladia-samples/tree/main/typescript)\n- [Python](https://github.com/gladiaio/gladia-samples/tree/main/python)\n- [Browser](https://github.com/gladiaio/gladia-samples/tree/main/javascript-browser)"
  },
  {
    "title": "Live Transcription - Migration from V1 to V2",
    "path": "migration-from-v1",
    "url": "https://docs.gladia.io/chapters/live-stt/migration-from-v1",
    "keywords": [
      "migration",
      "V1 to V2",
      "upgrade",
      "API changes",
      "configuration",
      "breaking changes",
      "websocket",
      "live transcription"
    ],
    "use_cases": [
      "how to migrate from Gladia V1 to V2 live API",
      "what changed between V1 and V2 configuration",
      "how to update WebSocket connection for V2",
      "when to upgrade to V2 live transcription"
    ],
    "tags": ["live", "realtime", "streaming", "stt", "migration", "v2"],
    "priority": 7,
    "content": "# Live Transcription - Migration Guide from V1 to V2\n\nLive transcription V2 is the latest real-time speech-to-text API from Gladia. It offers more features and has significant improvements in latency compared to V1. Please migrate sooner rather than later as support for V1 will be removed in the future.\n\n## Initiating the connection to the WebSocket\n\nIn V1, you always connect to the same WebSocket URL (`wss://api.gladia.io/audio/text/audio-transcription`) and send your configuration through the WebSocket connection.\n\nIn V2, you first generate a unique WebSocket URL with a call to `POST /v2/live` endpoint, and then connect to it. This URL contains a token that is unique to your live session. You'll be able to resume your session in case of a lost connection, or give the URL to a web client without exposing your Gladia API key.\n\n### V1\n```javascript\nimport WebSocket from 'ws';\n\nconst socket = new WebSocket('wss://api.gladia.io/audio/text/audio-transcription');\n\nsocket.addEventListener(\"open\", function() {\n  // Send configuration\n  socket.send(JSON.stringify({\n    'x_gladia_key': 'YOUR_GLADIA_API_KEY',\n    // ...config properties\n  }))\n\n  // Start sending audio chunks\n});\n```\n\n### V2\n```javascript\nimport WebSocket from 'ws';\n\nconst response = await fetch('https://api.gladia.io/v2/live', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'x-gladia-key': '<YOUR_GLADIA_API_KEY>',\n  },\n  body: JSON.stringify({\n    // ...config properties\n  }),\n});\n\nconst {url} = await response.json();\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function() {\n  // Start sending audio chunks\n});\n```\n\n## Configuration\n\nWith V2 offering more features, the configuration comes with some changes.\n\n### Audio encoding\n\n`encoding`, `bit_depth` and `sample_rate` are still present in V2, but with less options for now.\n\n- `wav` is the same as `wav/pcm`, V2 defaults to `wav/pcm`\n- `amb`, `mp3`, `flac`, `ogg/vorbis`, `opus`, `sphere` and `amr-nb` are no longer supported\n- `bit_depth` option `64` is no longer supported\n\nIf you're using an unsupported `encoding` or `bit_depth`, please contact Gladia.\n\n### Model\n\nOnly one model is supported in V2 for now, so omit the property `model`.\n\n### End-pointing and maximum audio duration\n\n`endpointing` is now declared in seconds instead of milliseconds.\n`maximum_audio_duration` has been renamed to `maximum_duration_without_endpointing`.\n\n**V1:**\n```json\n{\n  \"endpointing\": 800,\n  \"maximum_audio_duration\": 10\n}\n```\n\n**V2:**\n```json\n{\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 10\n}\n```\n\n### Language\n\n#### Automatic single language\n\nAutomatic single language behavior is the default in both V1 and V2, so you can just omit those parameters.\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"automatic single language\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [],\n    \"code_switching\": false\n  }\n}\n```\n\n#### Automatic multiple languages\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"automatic multiple languages\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [], // You can now specify the expected languages in V2 as guidance\n    \"code_switching\": true\n  }\n}\n```\n\n#### Manual\n\nLanguages are now specified with a 2-letter code.\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"manual\",\n  \"language\": \"english\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [\"en\"],\n    \"code_switching\": false\n  }\n}\n```\n\n### Frames format\n\nYou can send audio chunk as bytes or base64 and we'll detect the format automatically. The parameter `frames_format` is no longer present.\n\n### Audio enhancer\n\n`audio_enhancer` has been moved into the `pre_processing` object.\n\n**V1:**\n```json\n{\n  \"audio_enhancer\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"pre_processing\": {\n    \"audio_enhancer\": true\n  }\n}\n```\n\n### Word timestamps\n\n`word_timestamps` has been renamed to `words_accurate_timestamps` and moved into the `realtime_processing` object.\n\n**V1:**\n```json\n{\n  \"word_timestamps\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"realtime_processing\": {\n    \"words_accurate_timestamps\": true\n  }\n}\n```\n\n### Other properties\n\n`prosody`, `reinject_context` and `transcription_hint` are not supported for now. They may return in another form in the future.\n\n### Full config migration sample\n\n**V1:**\n```json\n{\n  \"encoding\": \"wav\",\n  \"bit_depth\": 8,\n  \"sample_rate\": 48000,\n  \"model\": \"accurate\",\n  \"endpointing\": 800,\n  \"maximum_audio_duration\": 10,\n  \"language_behaviour\": \"manual\",\n  \"language\": \"english\",\n  \"audio_enhancer\": true,\n  \"word_timestamps\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"bit_depth\": 8,\n  \"sample_rate\": 48000,\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 10,\n  \"language_config\": {\n    \"languages\": [\"en\"]\n  },\n  \"pre_processing\": {\n    \"audio_enhancer\": true\n  },\n  \"realtime_processing\": {\n    \"words_accurate_timestamps\": true\n  }\n}\n```\n\n## Send audio chunks\n\nIf you were sending chunks as bytes, nothing has changed. If you were sending them as base64, the format of the JSON messages changed in V2.\n\n**V1:**\n```json\n{\n  \"frames\": \"<base64 encoded>\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"audio_chunk\",\n  \"data\": {\n    \"chunk\": \"<base64 encoded>\"\n  }\n}\n```\n\n## Transcription message\n\nIn V1, we only send two kinds of messages through WebSocket: the \"connected\" message and the \"transcript\" messages.\n\nIn V2, we send more: lifecycle event messages, acknowledgment messages, add-on messages, post-processing messages, etc.\n\nTo read a transcription message in V1, you verify that the `type` field is `\"final\"` and/or the `transcription` field is not empty.\n\nIn V2, you should confirm that the `type` field is `transcript` and that `data.is_final` is `true`.\n\n**V1:**\n```json\n{\n  \"event\": \"transcript\",\n  \"request_id\": \"G-3abade39\",\n  \"type\": \"final\",\n  \"transcription\": \" Hello world\",\n  \"time_begin\": 1.4376875,\n  \"time_end\": 2.4696875,\n  \"confidence\": 0.65,\n  \"language\": \"en\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"transcript\",\n  \"session_id\": \"de0a341d-c69f-4e15-a649-7b3f49e211f0\",\n  \"created_at\": \"2024-10-10T14:35:28.387Z\",\n  \"data\": {\n    \"id\": \"00_00000000\",\n    \"is_final\": true,\n    \"utterance\": {\n      \"text\": \" Hello world\",\n      \"start\": 0.188,\n      \"end\": 1.284,\n      \"language\": \"en\",\n      \"confidence\": 1,\n      \"channel\": 0,\n      \"words\": [\n        { \"word\": \" Hello\", \"start\": 0.188, \"end\": 0.735, \"confidence\": 1 },\n        { \"word\": \" world\", \"start\": 0.736, \"end\": 1.284, \"confidence\": 1 }\n      ]\n    }\n  }\n}\n```\n\nIf you're not interested in new messages and simply want the ones from the V1 API, you can configure what messages you receive:\n\n```json\n{\n  \"messages_config\": {\n    \"receive_partial_transcripts\": false,\n    \"receive_final_transcripts\": true,\n    \"receive_speech_events\": false,\n    \"receive_pre_processing_events\": false,\n    \"receive_realtime_processing_events\": false,\n    \"receive_post_processing_events\": false,\n    \"receive_acknowledgments\": false,\n    \"receive_lifecycle_events\": false\n  }\n}\n```\n\n## End the live session\n\n**V1:**\n```json\n{\n  \"event\": \"terminate\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"stop_recording\"\n}\n```"
  },
  {
    "title": "Audio Intelligence for Live Transcription",
    "path": "audio-intelligence",
    "url": "https://docs.gladia.io/chapters/live-stt/audio-intelligence",
    "keywords": [
      "audio intelligence",
      "translation",
      "summarization",
      "NER",
      "named entity recognition",
      "sentiment analysis",
      "emotion analysis",
      "chapterization"
    ],
    "use_cases": [
      "how to add translation to live transcription",
      "how to use summarization with streaming audio",
      "how to detect entities in real-time transcription",
      "when to use audio intelligence features"
    ],
    "tags": ["live", "realtime", "streaming", "stt", "audio-intelligence", "NLP"],
    "priority": 8,
    "content": "# Audio Intelligence for Live Transcription\n\nAudio intelligence turns raw speech into structured, useful data on top of transcription. Once the words are captured, these features help you understand, organize, and act on the content - from instant translation to key-point summaries, entity detection, chapter markers, or emotions analysis.\n\n## Available Features\n\n### Translation\nTranslate transcripts and subtitles into multiple languages in one request.\n\n### Summarization\nGenerate concise summaries or bullet points for quick understanding.\n\n### Named Entity Recognition\nDetect and categorize key entities like people, organizations, dates, and more.\n\n### Chapterization\nSegment long audio into chapters with headlines and summaries for easy navigation.\n\n### Sentiment & Emotion Analysis\nUnderstand the tone and emotions expressed across the transcript.\n\nFor detailed documentation on each feature, see the [Audio Intelligence](https://docs.gladia.io/chapters/audio-intelligence) chapter."
  },
  {
    "title": "Live Transcription Features",
    "path": "features",
    "url": "https://docs.gladia.io/chapters/live-stt/features",
    "keywords": [
      "partial transcripts",
      "custom vocabulary",
      "custom spelling",
      "multiple channels",
      "custom metadata",
      "real-time features",
      "live STT configuration"
    ],
    "use_cases": [
      "how to enable partial transcripts for low latency",
      "how to improve recognition with custom vocabulary",
      "how to handle multi-channel audio streams",
      "how to add custom metadata to transcription sessions",
      "when to use custom spelling"
    ],
    "tags": ["live", "realtime", "streaming", "stt", "features", "configuration"],
    "priority": 9,
    "content": "# Live Transcription Features\n\nAll the configuration properties described in the feature pages are defined in the [POST /v2/live endpoint](https://docs.gladia.io/api-reference/v2/live/init).\n\n## Available Features\n\n### Partial transcripts\nStream incremental hypotheses while audio is being processed. Useful for low-latency applications where you need to show text as it's being spoken.\n\n### Custom vocabulary\nImprove recognition for domain-specific terms and phrases. Helpful when your audio contains technical jargon, brand names, or specialized terminology.\n\n### Custom spelling\nControl how names and terms are rendered in output. Useful for ensuring proper capitalization and formatting of specific words.\n\n### Multiple channels\nHandle stereo or multi-channel inputs with channel-aware transcription. Enables speaker attribution when each speaker is on a separate channel.\n\n### Custom metadata\nAttach metadata to sessions for organization and auditability. Helps with tracking and organizing transcription sessions.\n\n## Additional Resources\n\nWant to know more about the audio intelligence features? Check out the [Audio Intelligence](https://docs.gladia.io/chapters/audio-intelligence) chapter."
  }
]
