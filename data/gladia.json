{
  "name": "gladia",
  "display_name": "Gladia",
  "version": "v1",
  "base_url": "https://docs.gladia.io",
  "sections": [
    {
      "title": "API Reference Introduction",
      "path": "api-reference",
      "url": "https://docs.gladia.io/api-reference",
      "keywords": [
        "gladia api",
        "api reference",
        "rest api",
        "transcription api",
        "speech to text",
        "webhooks",
        "audio intelligence",
        "api documentation",
        "json api"
      ],
      "use_cases": [
        "how to get started with Gladia API",
        "how to integrate speech-to-text in my application",
        "when to use webhooks vs polling for transcription results",
        "how to structure API calls to Gladia",
        "what endpoints are available in Gladia API"
      ],
      "tags": [
        "api-reference",
        "introduction",
        "getting-started",
        "rest-api"
      ],
      "priority": 10,
      "content": "# API Reference Introduction\n\n**Gladia API** aims to provide a complete set of resources alongside with its **state-of-the-art API** features to makes the integration as easy as possible, like **webhooks event**, **callback urls** and **audio intelligence** to transform your transcriptions from raw data to usable knowledge, allowing you to focus on your product.\n\nThe API is structured following the **REST** standard and JSON input/output whenever it's possible, with a user-friendly experience and ensure a **low effort** integration process.\n\n## Available Endpoints\n\n### Live Endpoints\n- POST - Initiate a session\n- WSS - Live WebSocket\n- GET - Get result\n- GET - List transcriptions\n- GET - Download audio file\n- DEL - Delete transcription\n- Webhooks\n- Callbacks\n\n### Pre-recorded Endpoints\n- POST - Upload a file\n- POST - Initiate a transcription\n- GET - Get result\n- GET - List transcriptions\n- GET - Download audio file\n- DEL - Delete transcription\n- Webhooks\n- Callbacks\n\n### Transcription (deprecated)\n- POST - Initiate a transcription\n- GET - Get result\n- GET - List transcriptions\n- GET - Download audio file\n- DEL - Delete transcription\n\nIf you're not looking for technical API reference documentation, you can check the [Quickstart guide](https://docs.gladia.io/chapters/introduction/getting-started).\n\nLet's get started with learning how to use [Authentication](https://docs.gladia.io/api-reference/authentication) with your API calls."
    },
    {
      "title": "Authentication",
      "path": "api-reference/authentication",
      "url": "https://docs.gladia.io/api-reference/authentication",
      "keywords": [
        "authentication",
        "api key",
        "x-gladia-key",
        "sign up",
        "authorization",
        "header authentication",
        "api credentials",
        "gladia account"
      ],
      "use_cases": [
        "how to authenticate Gladia API calls",
        "how to get a Gladia API key",
        "how to sign up for Gladia",
        "where to find my API key in Gladia dashboard",
        "how to pass API key in request headers"
      ],
      "tags": [
        "api-reference",
        "authentication",
        "security",
        "api-key"
      ],
      "priority": 10,
      "content": "# Authentication\n\nUse your API key to authenticate your calls.\n\n## Signing Up\n\nYou will first need to create your account. Sign-up to [app.gladia.io](https://app.gladia.io/). You can sign-up through Google and more sign-up methods will be available in the near future.\n\n## Get your API key\n\nNow that you signed up, login to app.gladia.io and go to the [API keys section](https://app.gladia.io/apikeys). We should have already created a default key for you. You can use this one or create your own.\n\n> Gladia offers 10 Hours of free audio transcription per month if you want to test the service!\n\nWith your API key, you're now ready to use Gladia APIs.\n\n## Authenticate your API calls\n\nOnce you got your API key, pass it in the header `x-gladia-key`, like this:\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/transcription \\\n  --header 'x-gladia-key: YOUR_GLADIA_API_KEY'\n```\n\nThe header authentication method ensures that you are able to use Gladia in any environment or any cloud-based third party service.\n\nThis is the **only** authentication method you'll need to use all the API endpoints.\n\nNext, we'll see what's the general pre-recorded flow and how to manage it on your side."
    },
    {
      "title": "Welcome to Gladia",
      "path": "introduction",
      "url": "https://docs.gladia.io/chapters/introduction",
      "keywords": [
        "gladia",
        "speech-to-text",
        "audio transcription",
        "real-time transcription",
        "audio intelligence",
        "API",
        "voice recognition",
        "SDK",
        "integrations"
      ],
      "use_cases": [
        "How to get started with Gladia API?",
        "What can Gladia do for audio transcription?",
        "When to use live vs pre-recorded transcription?",
        "How to integrate Gladia with third-party services?",
        "What audio intelligence features does Gladia offer?"
      ],
      "tags": [
        "introduction",
        "overview",
        "getting-started"
      ],
      "priority": 10,
      "content": "# Welcome to Gladia\n\nGladia is a state-of-the-art audio transcription and intelligence provider. We offer a simple API to get transcription from your audios and videos, in both Real-time and asynchronous ways, with audio intelligence tools to extract, analyze and understand data from your audios.\n\n## Quick Links\n\n- **Getting started**: Getting started with Gladia Speech-To-Text API.\n- **Transcribe live audio**: Quickstart with Real-Time transcription using Gladia.\n- **Transcribe pre-recorded audio**: Quickstart with Asynchronous transcription using Gladia.\n- **Apply audio intelligence**: Use the power of LLMs directly on your audios without a third party service.\n- **Test gladia playground**: Test gladia quickly on our playground.\n- **API Reference**: Visit our technical documentation for more details.\n\n## Our integration partners\n\nIf you want to build fast with Gladia, you can use one of our integration partners:\n\n- **SDK**: For built-in best practices\n- **Pipecat**: For vocal/AI agents\n- **Livekit**: For vocal/AI agents\n- **Vapi**: For vocal/AI agents\n- **Recall**: For meeting recorder / agents\n- **Meeting Baas**: For meeting recorder / agents\n- **Twilio**: To use Gladia with telephony"
    },
    {
      "title": "Getting Started",
      "path": "introduction/getting-started",
      "url": "https://docs.gladia.io/chapters/introduction/getting-started",
      "keywords": [
        "getting started",
        "API key",
        "sign up",
        "account setup",
        "authentication",
        "quickstart",
        "free tier",
        "SDK"
      ],
      "use_cases": [
        "How to create a Gladia account?",
        "How to get an API key for Gladia?",
        "How to start using Gladia for transcription?",
        "What is the free tier limit for Gladia?"
      ],
      "tags": [
        "getting-started",
        "authentication",
        "quickstart"
      ],
      "priority": 10,
      "content": "# Getting Started\n\nSetup your Gladia account and start using the most reliable state-of-the-art Speech-To-Text API.\n\n## Step 1: Setup account\n\n### Signing Up\n\nYou will first need to create your account. Sign-up to app.gladia.io. You can sign-up through Google and more sign-up methods will be available in the near feature.\n\n### Get your API key\n\nNow that you signed up, login to app.gladia.io and go to the API keys section. We should have already created a default key for you. You can use this one or create your own.\n\n> Gladia offers 10 Hours of free audio transcription per month if you want to test the service!\n\nWith your API key, you're now ready to use Gladia APIs.\n\n## Step 2: Start building with Gladia!\n\n### With our SDK / API\n\nWant to build your integration yourself? Use our SDK or our API directly:\n\n- **Transcribe live audio**: Quickstart with Real-Time transcription using Gladia.\n- **Transcribe pre-recorded audio**: Quickstart with Asynchronous transcription using Gladia.\n\n### With one of our partners\n\nIf you want to build fast with Gladia, you can use one of our integration partners:\n\n- **SDK**: For built-in best practices\n- **Pipecat**: For vocal/AI agents\n- **Livekit**: For vocal/AI agents\n- **Vapi**: For vocal/AI agents\n- **Recall**: For meeting recorder / agents\n- **Meeting Baas**: For meeting recorder / agents\n- **Twilio**: To use Gladia with telephony\n\n## Next steps\n\nNow that you tested Gladia's basic transcription, you might want to extract data, enhance, translate or format your audio transcriptions. On top of its Speech-to-text API, Gladia provides a whole set of tools that you might want to use for your particular use cases like:\n\n- Pre-recorded transcription features\n- Realtime transcription features\n- Code samples\n- API Reference"
    },
    {
      "title": "Pre-recorded STT Quickstart",
      "path": "pre-recorded-stt/quickstart",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/quickstart",
      "keywords": [
        "quickstart",
        "pre-recorded",
        "transcription",
        "upload",
        "audio file",
        "polling",
        "webhook",
        "callback",
        "API",
        "speech-to-text"
      ],
      "use_cases": [
        "how to transcribe a pre-recorded audio file",
        "how to upload audio files to Gladia API",
        "how to get transcription results via polling",
        "how to configure webhooks for transcription notifications",
        "when to use callback URLs instead of polling"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "quickstart",
        "transcription"
      ],
      "priority": 10,
      "content": "## Upload your file\n\nThis step is **optional** if you are already working with **audio URLs**.\n\nIf you're working with audio or video files, you'll need to upload it first using our `POST /v2/upload` endpoint with `multipart/form-data` content-type since the `POST /v2/pre-recorded` endpoint only accept audio URLs. If you are already using audio file URLs, proceed to the next step.\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/upload \\\n  --header 'Content-Type: multipart/form-data' \\\n  --header 'x-gladia-key: YOUR_GLADIA_API_KEY' \\\n  --form audio=@/path/to/your/audio/conversation.wav\n```\n\nExample response:\n\n```json\n{\n  \"audio_url\": \"https://api.gladia.io/file/636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n  \"audio_metadata\": {\n    \"id\": \"636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n    \"filename\": \"conversation.wav\",\n    \"extension\": \"wav\",\n    \"size\": 99515383,\n    \"audio_duration\": 4146.468542,\n    \"number_of_channels\": 2\n  }\n}\n```\n\nWe will now proceed to the next steps using the returned `audio_url`.\n\n## Transcribe\n\nWe'll now POST the transcription request to Gladia's API using the `POST /v2/pre-recorded` endpoint.\n\n`/v2/pre-recorded` only accept `application/json` as Content-Type.\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/pre-recorded \\\n  --header 'Content-Type: application/json' \\\n  --header 'x-gladia-key: YOUR_GLADIA_API_KEY' \\\n  --data '{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"language_config\": {\n    \"languages\": [],\n    \"code_switching\": false\n  },\n  \"diarization\": true,\n  \"diarization_config\": {\n    \"number_of_speakers\": 3,\n    \"min_speakers\": 1,\n    \"max_speakers\": 5\n  },\n  \"translation\": true,\n  \"translation_config\": {\n    \"model\": \"base\",\n    \"target_languages\": [\"fr\", \"en\"],\n    \"context_adaptation\": true,\n    \"context\": \"Business meeting discussing quarterly results\",\n    \"informal\": false\n  },\n  \"subtitles\": true,\n  \"subtitles_config\": {\n    \"formats\": [\"srt\", \"vtt\"]\n  }\n}'\n```\n\nYou'll get an instant response from the request with an `id` and a `result_url`. The `id` is your transcription ID that you will use to get your transcription result once it's done. `result_url` is returned for convenience. This is a pre-built url with your transcription id in it that you can use to get your result in the next step.\n\n## Get the transcription result\n\nYou can get your transcription results in **3 different ways**:\n\n### Polling\n\nOnce you post your transcription request, you get a transcription `id` and a pre-built `result_url` for convenience. To get the result with this method, you'll just have to GET continuously on the given `result_url` until the status of your transcription is `done`.\n\n### Webhook\n\nYou can configure webhooks at https://app.gladia.io/webhooks to be notified when your transcriptions are done. Once a transcription is done, a `POST` request will be made to the endpoint you configured. The request body is a JSON object containing the transcription `id` that you can use to retrieve your result with our API.\n\n### Callback URL\n\nCallback are HTTP calls that you can use to get notified when your transcripts are ready. Instead of polling and keeping your server busy and maintaining work, you can use the `callback` feature to receive the result to a specified endpoint:\n\n```json\n{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"callback\": true,\n  \"callback_config\": {\n    \"url\": \"https://yourserverurl.com/your/callback/endpoint/\",\n    \"method\": \"POST\"\n  }\n}\n```\n\nOnce the transcription is done, a request will be made to the url you provided in `callback_config.url` using the HTTP method you provided in `callback_config.method`. Allowed methods are `POST` and `PUT` with the default being `POST`.\n\nThe request body is a JSON object containing the transcription `id` and an `event` property that tells you if it's a success or an error.\n\n## Full code sample\n\nYou can find complete code samples in our Github repository:\n\n- [Typescript/Javascript](https://github.com/gladiaio/gladia-samples/tree/main/typescript)\n- [Python](https://github.com/gladiaio/gladia-samples/tree/main/python)\n- [Browser](https://github.com/gladiaio/gladia-samples/tree/main/javascript-browser)"
    },
    {
      "title": "Live Transcription Quickstart",
      "path": "quickstart",
      "url": "https://docs.gladia.io/chapters/live-stt/quickstart",
      "keywords": [
        "live transcription",
        "real-time STT",
        "websocket",
        "audio streaming",
        "SDK",
        "speech-to-text",
        "live session",
        "audio chunks",
        "multi-channel"
      ],
      "use_cases": [
        "how to set up live transcription with Gladia",
        "how to send audio chunks via WebSocket",
        "how to handle multiple audio channels in real-time",
        "when to use partial vs final transcripts",
        "how to reconnect after WebSocket disconnection"
      ],
      "tags": [
        "live",
        "realtime",
        "streaming",
        "stt",
        "websocket",
        "quickstart"
      ],
      "priority": 10,
      "content": "# Live Transcription Quickstart\n\nThe SDK simplifies real-time speech-to-text integration by abstracting the underlying API. Designed for developers, it offers:\n\n- Effortless implementation with minimal code to write.\n- Built-in resilience with automatic error handling (e.g., reconnection on network drops) ensures uninterrupted transcription. No need to manually manage retries or state recovery.\n\n## Install the SDK\n\n**JavaScript:**\n```bash\nnpm install @gladiaio/sdk\n```\n\n```javascript\nimport { GladiaClient } from \"@gladiaio/sdk\";\n```\n\n**Python:**\n```bash\npip install gladia\n```\n\n```python\nfrom gladia import GladiaClient\n```\n\n## Initiate your real-time session\n\nFirst, call the endpoint and pass your configuration. It's important to correctly define the properties `encoding`, `sample_rate`, `bit_depth` and `channels` as we need them to parse your audio chunks.\n\n**JavaScript:**\n```javascript\nconst gladiaClient = new GladiaClient({\n  apiKey: <YOUR_GLADIA_API_KEY>,\n});\n\nconst gladiaConfig = {\n  model: \"solaria-1\",\n  encoding: 'wav/pcm',\n  sample_rate: 16000,\n  bit_depth: 16,\n  channels: 1,\n  language_config: {\n    languages: [\"fr\"],\n    code_switching: false,\n  },\n};\n\nconst liveSession = gladiaClient.liveV2().startSession(gladiaConfig);\n```\n\n### Why initiate with POST instead of connecting directly to the WebSocket?\n\n- **Security**: Generate the WebSocket URL on your backend and keep your API key private. The init call returns a connectable URL and a session `id` that you can safely pass to web, iOS, or Android clients without exposing credentials in the app.\n- **Lower infrastructure load**: The secure URL is generated on your backend, the client can connect directly to Gladia's WebSocket server without a pass-through on your side, saving your own resources.\n- **Resilient reconnection and session continuity**: If the WebSocket disconnects (which can happen on unreliable networks), the session created by the init call lets the client reconnect without losing context.\n\n## Connect to the WebSocket\n\n**JavaScript:**\n```javascript\nliveSession.on(\"message\", (message) => {\n  // Handle messages from the API\n});\nliveSession.on(\"started\", (message) => {\n  // Handle start session message\n});\nliveSession.on(\"ended\", (message) => {\n  // Handle end session message\n});\nliveSession.on(\"error\", (message) => {\n  // Handle error message\n});\n```\n\n## Send audio chunks\n\n```javascript\nliveSession.sendAudio(audioChunk)\n```\n\n## Read messages\n\nDuring the whole session, we will send various messages through the WebSocket, the callback URL or webhooks. You can specify which kind of messages you want to receive in the initial configuration. See `messages_config` for WebSocket messages and `callback_config` for callback messages.\n\n```javascript\nliveSession.on(\"message\", (message) => {\n  if (message.type === 'transcript' && message.data.is_final) {\n    console.log(`${message.data.id}: ${message.data.utterance.text}`)\n});\n```\n\n**Need low-latency partial results?** Enable partial transcripts by setting `messages_config.receive_partial_transcripts: true`. Use the `is_final` property to distinguish between partial and final transcript messages.\n\n## Stop the recording\n\nOnce you're done, send us the `stop_recording` message. We will process remaining audio chunks and start the post-processing phase.\n\n```javascript\nliveSession.stopRecording()\n```\n\n## Get the final results\n\nIf you want to get the complete result, you can call the `GET /v2/live/:id` endpoint with the `id` you received from the initial request.\n\n```javascript\nconst response = await fetch(`https://api.gladia.io/v2/live/${sessionId}`, {\n  method: 'GET',\n  headers: {\n    'x-gladia-key': '<YOUR_GLADIA_API_KEY>',\n  },\n});\nconst result = await response.json();\nconsole.log(result)\n```\n\n## Using the API directly (without SDK)\n\n### Initiate your real-time session\n\n```javascript\nconst response = await fetch(\"https://api.gladia.io/v2/live\", {\n  method: \"POST\",\n  headers: {\n    \"Content-Type\": \"application/json\",\n    \"x-gladia-key\": \"<YOUR_GLADIA_API_KEY>\",\n  },\n  body: JSON.stringify({\n    encoding: \"wav/pcm\",\n    sample_rate: 16000,\n    bit_depth: 16,\n    channels: 1,\n  }),\n});\n\nconst { id, url } = await response.json();\n```\n\nResponse example:\n```json\n{\n  \"id\": \"636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n  \"url\": \"wss://api.gladia.io/v2/live?token=636c70f6-92c1-4026-a8b6-0dfe3ecf826f\"\n}\n```\n\n### Connect to the WebSocket\n\n```javascript\nimport WebSocket from \"ws\";\n\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function () {\n  // Connection is opened. You can start sending audio chunks.\n});\n\nsocket.addEventListener(\"error\", function (error) {\n  // An error occurred during the connection.\n});\n\nsocket.addEventListener(\"close\", function ({ code, reason }) {\n  // The connection has been closed\n  // If the \"code\" is equal to 1000, it means we closed intentionally the connection.\n  // Otherwise, you can reconnect to the same url.\n});\n\nsocket.addEventListener(\"message\", function (event) {\n  const message = JSON.parse(event.data.toString());\n  console.log(message);\n});\n```\n\n### Send audio chunks\n\n```javascript\n// as binary\nsocket.send(buffer);\n\n// as json\nsocket.send(\n  JSON.stringify({\n    type: \"audio_chunk\",\n    data: {\n      chunk: buffer.toString(\"base64\"),\n    },\n  })\n);\n```\n\n## Sending multiple audio tracks in real-time\n\nIf you have multiple audio sources (like different participants in a conversation) that you need to transcribe simultaneously, you can merge these separate audio tracks into a single multi-channel audio stream and send it over one WebSocket connection.\n\n### Merging multiple audio tracks into one multi-channel WebSocket\n\nBenefits:\n- Reduce the number of WebSocket connections from multiple to just one\n- Maintain speaker identity through channel mapping\n- Simplify synchronization of audio streams from multiple participants\n- Reduce network overhead and connection management complexity\n\n### Creating a multi-channel audio stream\n\n```typescript\nexport function interleaveAudio(channelsData: Buffer[], bitDepth = 16): Buffer {\n  const nbChannels = channelsData.length;\n  if (nbChannels === 1) {\n    return channelsData[0];\n  }\n\n  const bytesPerSample = bitDepth / 8;\n  const samplesPerChannel = channelsData[0].byteLength / bytesPerSample;\n  const audio = Buffer.alloc(nbChannels * samplesPerChannel * bytesPerSample);\n\n  for (let i = 0; i < samplesPerChannel; i++) {\n    for (let j = 0; j < nbChannels; j++) {\n      const sample = channelsData[j].subarray(\n        i * bytesPerSample,\n        (i + 1) * bytesPerSample\n      );\n      audio.set(sample, (i * nbChannels + j) * bytesPerSample);\n    }\n  }\n\n  return audio;\n}\n```\n\n### Example use case\n\nConsider a scenario with three participants in a room: Sami, Maxime, and Mark.\n\n```typescript\n// Collect audio buffers from each participant\nconst samiAudio = getSamiAudioBuffer();\nconst maximeAudio = getMaximeAudioBuffer();\nconst markAudio = getMarkAudioBuffer();\n\n// Merge into a multi-channel audio\n// Channel ordering: [0]=Sami, [1]=Maxime, [2]=Mark\nconst channelsData = [samiAudio, maximeAudio, markAudio];\nconst mergedAudio = interleaveAudio(channelsData, 16);\n\n// Initialize a single WebSocket session with multi-channel config\nconst response = await fetch(\"https://api.gladia.io/v2/live\", {\n  method: \"POST\",\n  headers: {\n    \"Content-Type\": \"application/json\",\n    \"x-gladia-key\": \"<YOUR_GLADIA_API_KEY>\",\n  },\n  body: JSON.stringify({\n    encoding: \"wav/pcm\",\n    sample_rate: 16000,\n    bit_depth: 16,\n    channels: 3, // Specify the number of channels\n  }),\n});\n\nconst { url } = await response.json();\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function () {\n  socket.send(mergedAudio);\n});\n```\n\n### Understanding the response\n\nEach transcription message will include a `channel` field that indicates which audio channel (and thus which participant) the transcription belongs to:\n\n```json\n{\n  \"type\": \"transcript\",\n  \"session_id\": \"de70f43f-3041-46e0-892c-8e7f53800a22\",\n  \"created_at\": \"2025-04-09T08:44:16.471Z\",\n  \"data\": {\n    \"id\": \"00_00000000\",\n    \"utterance\": {\n      \"text\": \"Hello, I'm Sami. I'm the first speaker\",\n      \"start\": 0.188,\n      \"end\": 2.852,\n      \"language\": \"en\",\n      \"channel\": 0\n    }\n  }\n}\n```\n\nThe channel numbers correspond to the order in which you added the audio tracks:\n- Channel 0 -> Sami (first in the array)\n- Channel 1 -> Maxime (second in the array)\n- Channel 2 -> Mark (third in the array)\n\n## Full code samples\n\nYou can find complete code samples in the Github repository:\n- [Typescript/Javascript](https://github.com/gladiaio/gladia-samples/tree/main/typescript)\n- [Python](https://github.com/gladiaio/gladia-samples/tree/main/python)\n- [Browser](https://github.com/gladiaio/gladia-samples/tree/main/javascript-browser)"
    },
    {
      "title": "Delete Transcription - Live",
      "path": "api-reference/v2/live/delete",
      "url": "https://docs.gladia.io/api-reference/v2/live/delete",
      "keywords": [
        "delete transcription",
        "DELETE /v2/live/:id",
        "remove live job",
        "cleanup",
        "data deletion",
        "audio file deletion",
        "GDPR compliance"
      ],
      "use_cases": [
        "How to delete a live transcription and its data?",
        "When to clean up completed transcription jobs?",
        "How to remove audio files after transcription?",
        "How to comply with data retention policies?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "DELETE",
        "cleanup"
      ],
      "priority": 9,
      "content": "# Delete Transcription - Live\n\n**DELETE /v2/live/{id}**\n\nDelete a live transcription and all its data (audio file, transcription results).\n\n## Example Request\n\n```bash\ncurl --request DELETE \\\n  --url https://api.gladia.io/v2/live/{id} \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response\n\n**202**: The live job has been successfully deleted.\n\n## Error Responses\n\n**401 Unauthorized**:\n```json\n{\n  \"timestamp\": \"2023-12-28T09:04:17.210Z\",\n  \"path\": \"/v2/live/45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"request_id\": \"G-821fe9df\",\n  \"statusCode\": 401,\n  \"message\": \"gladia key not found\"\n}\n```\n\n**403 Forbidden**: You don't have permission to delete this resource.\n\n**404 Not Found**: The transcription job was not found.\n\n## Authorization\n\n**x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| id | string | Yes | ID of the live job. Example: `45463597-20b7-4af7-b3b3-f5fb778203ab` |"
    },
    {
      "title": "Get Result - Live Transcription",
      "path": "api-reference/v2/live/get",
      "url": "https://docs.gladia.io/api-reference/v2/live/get",
      "keywords": [
        "get result",
        "live transcription result",
        "GET /v2/live/:id",
        "transcription status",
        "job metadata",
        "utterances",
        "full transcript",
        "audio duration",
        "billing time"
      ],
      "use_cases": [
        "How to retrieve live transcription results after session ends?",
        "How to check the status of a live transcription job?",
        "When to poll for transcription completion?",
        "How to get word-level timestamps from live transcription?",
        "How to access translation results from live session?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "GET",
        "result",
        "status"
      ],
      "priority": 9,
      "content": "# Get Result - Live Transcription\n\n**GET /v2/live/{id}**\n\nGet the live job's metadata, status, parameters, and results.\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/live/{id} \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response (200)\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"request_id\": \"G-45463597\",\n  \"version\": 2,\n  \"status\": \"done\",\n  \"created_at\": \"2023-12-28T09:04:17.210Z\",\n  \"completed_at\": \"2023-12-28T09:04:37.210Z\",\n  \"kind\": \"live\",\n  \"custom_metadata\": { \"user\": \"John Doe\" },\n  \"file\": {\n    \"audio_duration\": 3600,\n    \"number_of_channels\": 1\n  },\n  \"result\": {\n    \"metadata\": {\n      \"audio_duration\": 3600,\n      \"number_of_distinct_channels\": 1,\n      \"billing_time\": 3600,\n      \"transcription_time\": 20\n    },\n    \"transcription\": {\n      \"full_transcript\": \"...\",\n      \"languages\": [\"en\"],\n      \"utterances\": [...]\n    }\n  }\n}\n```\n\n## Authorization\n\n**x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| id | string | Yes | ID of the live job. Example: `45463597-20b7-4af7-b3b3-f5fb778203ab` |\n\n## Response Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | uuid | Job ID |\n| request_id | string | Debug ID |\n| version | integer | API version |\n| status | enum | Job status: `queued`, `processing`, `done`, `error` |\n| created_at | datetime | Creation date |\n| completed_at | datetime | Completion date (when status is done/error) |\n| kind | enum | Always `live` |\n| custom_metadata | object | Custom metadata from initial request |\n| error_code | integer | HTTP status code if error (400-599) |\n| file | object | File data (id, filename, source, audio_duration, number_of_channels) |\n| request_params | object | Parameters used for this transcription |\n| result | object | Transcription results when status is \"done\" |\n\n## Result Object\n\n| Field | Type | Description |\n|-------|------|-------------|\n| metadata | object | Audio duration, billing time, transcription time |\n| transcription | object | Full transcript, languages, utterances, sentences, subtitles |\n| translation | object | Translation results if enabled |\n| summarization | object | Summary if enabled |\n| named_entity_recognition | object | NER results if enabled |\n| sentiment_analysis | object | Sentiment results if enabled |\n| chapterization | object | Chapter results if enabled |"
    },
    {
      "title": "Download Audio File - Live",
      "path": "api-reference/v2/live/get-audio",
      "url": "https://docs.gladia.io/api-reference/v2/live/get-audio",
      "keywords": [
        "download audio",
        "GET /v2/live/:id/file",
        "audio file",
        "recorded audio",
        "live session audio",
        "binary download",
        "audio export"
      ],
      "use_cases": [
        "How to download the audio file from a live transcription session?",
        "How to retrieve recorded audio for archiving?",
        "When to download audio for quality review?",
        "How to export audio from completed live sessions?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "GET",
        "audio",
        "download"
      ],
      "priority": 9,
      "content": "# Download Audio File - Live\n\n**GET /v2/live/{id}/file**\n\nDownload the audio file recorded during a live transcription session.\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/live/{id}/file \\\n  --header 'x-gladia-key: <api-key>' \\\n  --output audio.wav\n```\n\n## Response\n\n**200 OK**: Returns the audio file as binary data (`application/octet-stream`).\n\n## Error Responses\n\n**401 Unauthorized**: Invalid or missing API key.\n\n**404 Not Found**: The transcription job or audio file was not found.\n\n## Authorization\n\n**x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n| Parameter | Type | Required | Description |\n|-----------|------|----------|-------------|\n| id | string | Yes | ID of the live job. Example: `45463597-20b7-4af7-b3b3-f5fb778203ab` |\n\n## Response\n\nThe response is a binary file (`application/octet-stream`) containing the audio recorded during the live transcription session."
    },
    {
      "title": "Initiate a Session - Live Transcription",
      "path": "api-reference/v2/live/init",
      "url": "https://docs.gladia.io/api-reference/v2/live/init",
      "keywords": [
        "live transcription",
        "websocket session",
        "POST /v2/live",
        "initiate session",
        "audio encoding",
        "sample rate",
        "bit depth",
        "solaria-1 model",
        "real-time transcription",
        "streaming audio"
      ],
      "use_cases": [
        "How to start a live transcription session with Gladia API?",
        "How to configure audio encoding for live streaming?",
        "When to use POST init before WebSocket connection?",
        "How to set up custom vocabulary for live transcription?",
        "How to enable real-time translation during live sessions?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "POST",
        "init",
        "websocket",
        "session"
      ],
      "priority": 9,
      "content": "# Initiate a Session - Live Transcription\n\n**POST /v2/live**\n\nInitiate a new live transcription WebSocket session.\n\n## Example Request\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/live \\\n  --header 'Content-Type: application/json' \\\n  --header 'x-gladia-key: <api-key>' \\\n  --data '{\n    \"encoding\": \"wav/pcm\",\n    \"bit_depth\": 16,\n    \"sample_rate\": 16000,\n    \"channels\": 1,\n    \"model\": \"solaria-1\"\n  }'\n```\n\n## Response (201)\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"created_at\": \"2023-12-28T09:04:17.210Z\",\n  \"url\": \"wss://api.gladia.io/v2/live?token=4a39145c-2844-4557-8f34-34883f7be7d9\"\n}\n```\n\nUse the returned WebSocket url to connect and start sending audio chunks. Use the returned `id` with GET /v2/live/:id to obtain status and results.\n\n## Why initiate with POST instead of connecting directly to the WebSocket?\n\n- **Security**: Generate the WebSocket URL on your backend and keep your API key private. The init call returns a connectable URL and a session `id` that you can safely pass to web, iOS, or Android clients without exposing credentials.\n- **Lower infrastructure load**: The secure URL is generated on your backend, the client can connect directly to Gladia's WebSocket server without a pass-through on your side.\n- **Resilient reconnection**: If the WebSocket disconnects, the session created by the init call lets the client reconnect without losing context.\n\n## Authorization\n\n**x-gladia-key** (header, required): Your personal Gladia API key\n\n## Query Parameters\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| region | enum | The region used to process the audio. Options: `us-west`, `eu-west` |\n\n## Body Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| encoding | enum | wav/pcm | Audio encoding format. Options: `wav/pcm`, `wav/alaw`, `wav/ulaw` |\n| bit_depth | enum | 16 | Bit depth. Options: `8`, `16`, `24`, `32` |\n| sample_rate | enum | 16000 | Sample rate. Options: `8000`, `16000`, `32000`, `44100`, `48000` |\n| channels | integer | 1 | Number of channels (1-8) |\n| custom_metadata | object | - | Custom metadata to attach |\n| model | enum | solaria-1 | The model used. Options: `solaria-1` |\n| endpointing | number | 0.05 | Silence duration to finish utterance (0.01-10 seconds) |\n| maximum_duration_without_endpointing | number | 5 | Max duration without endpointing (5-60 seconds) |\n| language_config | object | - | Language configuration |\n| pre_processing | object | - | Pre-processing config (audio_enhancer, speech_threshold) |\n| realtime_processing | object | - | Realtime processing (custom_vocabulary, translation, NER, sentiment) |\n| post_processing | object | - | Post-processing (summarization, chapterization) |\n| messages_config | object | - | WebSocket messages configuration |\n| callback | boolean | false | Enable callbacks |\n| callback_config | object | - | Callback configuration |\n\n## Response\n\n| Field | Type | Description |\n|-------|------|-------------|\n| id | uuid | Job ID |\n| created_at | datetime | Creation date |\n| url | uri | WebSocket URL to connect for sending audio |"
    },
    {
      "title": "List Transcriptions - Live",
      "path": "api-reference/v2/live/list",
      "url": "https://docs.gladia.io/api-reference/v2/live/list",
      "keywords": [
        "list transcriptions",
        "GET /v2/live",
        "pagination",
        "filter transcriptions",
        "transcription history",
        "status filter",
        "date filter",
        "custom metadata filter"
      ],
      "use_cases": [
        "How to list all live transcriptions?",
        "How to paginate through transcription results?",
        "How to filter transcriptions by status?",
        "How to filter transcriptions by date range?",
        "How to find transcriptions by custom metadata?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "GET",
        "list",
        "pagination"
      ],
      "priority": 9,
      "content": "# List Transcriptions - Live\n\n**GET /v2/live**\n\nList all live transcriptions matching the specified parameters with pagination support.\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url 'https://api.gladia.io/v2/live?limit=20&status=done' \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response (200)\n\n```json\n{\n  \"first\": \"https://api.gladia.io/v2/live?status=done&offset=0&limit=20\",\n  \"current\": \"https://api.gladia.io/v2/live?status=done&offset=0&limit=20\",\n  \"next\": \"https://api.gladia.io/v2/live?status=done&offset=20&limit=20\",\n  \"items\": [\n    {\n      \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n      \"status\": \"done\",\n      \"created_at\": \"2023-12-28T09:04:17.210Z\",\n      \"kind\": \"live\",\n      ...\n    }\n  ]\n}\n```\n\n## Authorization\n\n**x-gladia-key** (header, required): Your personal Gladia API key\n\n## Query Parameters\n\n| Parameter | Type | Default | Description |\n|-----------|------|---------|-------------|\n| offset | integer | 0 | Starting point for pagination (0 = first item) |\n| limit | integer | 20 | Maximum items to return (min: 1) |\n| date | datetime | - | Filter by specific date (ISO format: YYYY-MM-DD) |\n| before_date | datetime | - | Include items before this date (ISO format) |\n| after_date | datetime | - | Include items after this date (ISO format) |\n| status | enum[] | - | Filter by status: `queued`, `processing`, `done`, `error` |\n| custom_metadata | object | - | Filter by custom metadata. Example: `{\"user\": \"John Doe\"}` |\n\n## Response Fields\n\n| Field | Type | Description |\n|-------|------|-------------|\n| first | uri | URL to fetch the first page |\n| current | uri | URL to fetch the current page |\n| next | uri | URL to fetch the next page (null if last page) |\n| items | array | List of live transcription objects |"
    },
    {
      "title": "Live WebSocket - Real-time Transcription",
      "path": "api-reference/v2/live/websocket",
      "url": "https://docs.gladia.io/api-reference/v2/live/websocket",
      "keywords": [
        "websocket",
        "WSS",
        "real-time streaming",
        "audio chunks",
        "transcript events",
        "speech events",
        "translation events",
        "sentiment analysis",
        "named entity recognition",
        "live audio"
      ],
      "use_cases": [
        "How to connect to the live transcription WebSocket?",
        "How to send audio chunks via WebSocket?",
        "How to handle transcript events in real-time?",
        "How to receive translation results via WebSocket?",
        "How to stop recording and end a session?",
        "How to handle speech start and end events?"
      ],
      "tags": [
        "api-reference",
        "live",
        "endpoints",
        "websocket",
        "streaming",
        "real-time"
      ],
      "priority": 9,
      "content": "# Live WebSocket - Real-time Transcription\n\n**WSS wss://api.gladia.io/v2/live?token={SESSION}**\n\nWebSocket endpoint for sending audio chunks and receiving real-time transcription events.\n\n## Connection\n\nConnect to the WebSocket URL returned from POST /v2/live init endpoint:\n```\nwss://api.gladia.io/v2/live?token=4a39145c-2844-4557-8f34-34883f7be7d9\n```\n\n## Messages You Can Send\n\n### Audio Chunk (JSON)\nSend audio bytes as base64-encoded chunk:\n```json\n{\n  \"type\": \"audio_chunk\",\n  \"data\": {\n    \"chunk\": \"UklGRiQAAABXQVZFZm10IBIAAAABAAEAESsAACJWAAACABAAZGF0YQAAAA==\"\n  }\n}\n```\n\n### Audio Chunk (Binary Frame)\nSend audio bytes as a binary WebSocket frame (more efficient).\n\n### Stop Recording\nInform Gladia that recording is over:\n```json\n{\n  \"type\": \"stop_recording\"\n}\n```\n\n## Messages You Receive\n\n### Transcript\nContains transcription information:\n```json\n{\n  \"session_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"created_at\": \"2025-09-19T12:34:10Z\",\n  \"type\": \"transcript\",\n  \"data\": {\n    \"id\": \"00-00000011\",\n    \"is_final\": true,\n    \"utterance\": {\n      \"start\": 0,\n      \"end\": 0.48,\n      \"confidence\": 0.91,\n      \"channel\": 0,\n      \"words\": [\n        { \"word\": \"Hello\", \"start\": 0, \"end\": 0.35, \"confidence\": 0.91 },\n        { \"word\": \"world\", \"start\": 0.36, \"end\": 0.48, \"confidence\": 0.91 }\n      ],\n      \"text\": \"Hello world.\",\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n### Speech Start\nIndicates start of detected speech:\n```json\n{\n  \"session_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"type\": \"speech_start\",\n  \"data\": { \"time\": 1.24, \"channel\": 0 }\n}\n```\n\n### Speech End\nIndicates end of detected speech:\n```json\n{\n  \"session_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"type\": \"speech_end\",\n  \"data\": { \"time\": 3.1, \"channel\": 0 }\n}\n```\n\n### Translation\nTranslation of utterances (if enabled):\n```json\n{\n  \"type\": \"translation\",\n  \"data\": {\n    \"utterance_id\": \"utt_001\",\n    \"original_language\": \"es\",\n    \"target_language\": \"en\",\n    \"translated_utterance\": {\n      \"text\": \"good morning\",\n      \"language\": \"en\"\n    }\n  }\n}\n```\n\n### Named Entity Recognition\nNER results (if enabled):\n```json\n{\n  \"type\": \"named_entity_recognition\",\n  \"data\": {\n    \"utterance_id\": \"utt_002\",\n    \"results\": [\n      { \"entity_type\": \"PERSON\", \"text\": \"Alice\" },\n      { \"entity_type\": \"TIME\", \"text\": \"3pm\" }\n    ]\n  }\n}\n```\n\n### Sentiment Analysis\nSentiment results (if enabled):\n```json\n{\n  \"type\": \"sentiment_analysis\",\n  \"data\": {\n    \"utterance_id\": \"utt_003\",\n    \"results\": [\n      { \"sentiment\": \"positive\", \"emotion\": \"joy\", \"text\": \"love this product\" }\n    ]\n  }\n}\n```\n\n### Post-Processing Events\n\n**Post Transcript**: Full transcript after session\n```json\n{ \"type\": \"post_transcript\", \"data\": { \"full_transcript\": \"hello world\" } }\n```\n\n**Final Transcript (Aggregated)**: Complete metadata and transcription\n```json\n{\n  \"type\": \"post_final_transcript\",\n  \"data\": {\n    \"metadata\": { \"audio_duration\": 123.45, \"billing_time\": 120 },\n    \"transcription\": { \"full_transcript\": \"hello world\" }\n  }\n}\n```\n\n**Chapterization**: Chapter breakdown (if enabled)\n```json\n{\n  \"type\": \"post_chapterization\",\n  \"data\": {\n    \"results\": [{ \"headline\": \"Project kickoff\", \"start\": 0, \"end\": 60 }]\n  }\n}\n```\n\n**Summarization**: Summary (if enabled)\n```json\n{\n  \"type\": \"post_summarization\",\n  \"data\": { \"results\": \"The team aligned on goals and next steps.\" }\n}\n```\n\n### Lifecycle Events\n\n**Start Session**: Session has started\n```json\n{ \"type\": \"start_session\", \"session_id\": \"...\" }\n```\n\n**Start Recording**: Recording has begun\n```json\n{ \"type\": \"start_recording\", \"session_id\": \"...\" }\n```\n\n**End Recording**: Recording has ended\n```json\n{ \"type\": \"end_recording\", \"data\": { \"reason\": \"user_request\", \"received_total_bytes\": 1048576 } }\n```\n\n**End Session**: Session has ended\n```json\n{ \"type\": \"end_session\", \"session_id\": \"...\" }\n```\n\n### Acknowledgments\n\n**Audio Chunk Acknowledgment**:\n```json\n{\n  \"type\": \"audio_chunk\",\n  \"acknowledged\": true,\n  \"data\": { \"byte_range\": [0, 4095], \"time_range\": [0, 0.256] }\n}\n```\n\n**Stop Recording Acknowledgment**:\n```json\n{ \"type\": \"stop_recording\", \"acknowledged\": true }\n```"
    },
    {
      "title": "Audio Intelligence",
      "path": "audio-intelligence",
      "url": "https://docs.gladia.io/chapters/audio-intelligence",
      "keywords": [
        "audio intelligence",
        "speech insights",
        "transcription features",
        "translation",
        "summarization",
        "named entity recognition",
        "sentiment analysis",
        "chapterization",
        "audio to llm"
      ],
      "use_cases": [
        "how to add AI features to transcription",
        "when to use audio intelligence for meeting notes",
        "how to automate voice agent workflows",
        "when to use compliance redaction with NER",
        "how to enable multilingual publishing"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "overview"
      ],
      "priority": 9,
      "content": "# Audio Intelligence\n\nAudio intelligence turns raw speech into structured, useful data on top of transcription. Once the words are captured, these features help you understand, organize, and act on the content - from instant translation to key-point summaries, entity detection, chapter markers, emotions, and even custom LLM prompts.\n\nUse these capabilities alongside Live or Pre-recorded STT to automate workflows like meeting notes, voice agents, compliance redaction, knowledge extraction, and multilingual publishing.\n\n## Available Features\n\n- **Translation** - Translate transcripts and subtitles into multiple languages in one request.\n- **Summarization** - Generate concise summaries or bullet points for quick understanding.\n- **Named Entity Recognition** - Detect and categorize key entities like people, organizations, dates, and more.\n- **Chapterization** - Segment long audio into chapters with headlines and summaries for easy navigation.\n- **Sentiment & Emotion Analysis** - Understand the tone and emotions expressed across the transcript.\n- **Audio to LLM** - Ask custom questions and run prompts on your audio like an assistant would."
    },
    {
      "title": "Named Entity Recognition",
      "path": "audio-intelligence/named-entity-recognition",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/named-entity-recognition",
      "keywords": [
        "named entity recognition",
        "NER",
        "entity detection",
        "PII detection",
        "GDPR compliance",
        "HIPAA compliance",
        "data redaction",
        "personal information",
        "PHI",
        "regulatory compliance"
      ],
      "use_cases": [
        "how to detect personal information in audio",
        "how to comply with GDPR using NER",
        "when to use entity detection for compliance",
        "how to identify names and organizations in transcription",
        "how to redact sensitive data from audio"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "ner",
        "compliance",
        "pii"
      ],
      "priority": 9,
      "content": "# Named Entity Recognition\n\n> This feature is in **Alpha** state. Breaking changes may still be introduced to this API, but an advanced notice will be sent. We're looking for feedback to improve this feature.\n\n**Named Entity Recognition** (also known as **Entity Detection**) detects and categorizes key information in the audio.\n\n## Usage\n\nTo enable named entity recognition simply set the appropriate flag:\n\n### Live transcription\n\n```json\n{\n  \"realtime_processing\": {\n    \"named_entity_recognition\": true\n  },\n  \"messages_config\": {\n    \"receive_realtime_processing_events\": true\n  }\n}\n```\n\n### Pre-recorded\n\n```json\n{\n  \"named_entity_recognition\": true\n}\n```\n\n## Result\n\nThe transcription result will contain a `\"named_entity_recognition\"` key with the output of the model:\n\n```json\n{\n  \"transcription\": {...},\n  \"named_entity_recognition\": {\n    \"success\": true,\n    \"is_empty\": false,\n    \"results\": [\n      {\n        \"entity_type\": \"EMAIL_ADDRESS\",\n        \"text\": \"privacy@gladia.io\",\n        \"start\": 123.4,\n        \"end\": 124.5\n      },\n      {\n        \"entity_type\": \"AGE\",\n        \"text\": \"27 years old\",\n        \"start\": 234.7,\n        \"end\": 235.6\n      }\n    ],\n    \"exec_time\": 1.7726809978485107,\n    \"error\": null\n  }\n}\n```\n\n## Supported Regulations\n\nGladia.io helps you cover the following regulations for your business:\n\n- EU General Data Protection Regulation (GDPR)\n- California Privacy Rights Act (CPRA)\n- USA Health Insurance Portability and Accountability Act (HIPAA Safe Harbor)\n- Quebec Privacy Act (law 25)\n- Japan Act on the Protection of Personal Information (APPI)\n\n## Supported Entities\n\n### PII Entities\n\n- ACCOUNT_NUMBER - Customer account or membership identification number\n- AGE - Numbers associated with an individual's age\n- DATE - Specific calendar dates\n- DATE_INTERVAL - Broader time periods, including date ranges\n- DOB - Dates of birth\n- DRIVER_LICENSE - Driver's permit numbers\n- DURATION - Periods of time\n- EMAIL_ADDRESS - Email addresses\n- EVENT - Names of events or holidays\n- FILENAME - Names of computer files\n- GENDER_SEXUALITY - Terms indicating gender identity or sexual orientation\n- HEALTHCARE_NUMBER - Healthcare numbers and health plan beneficiary numbers\n- IP_ADDRESS - Internet IP address (IPv4 and IPv6)\n- LANGUAGE - Names of natural languages\n- LOCATION - Any named location reference\n- LOCATION_ADDRESS - Full or partial physical mailing addresses\n- LOCATION_CITY - Municipality names\n- LOCATION_COORDINATE - Geographic positions\n- LOCATION_COUNTRY - Country names\n- LOCATION_STATE - State, province, territory names\n- LOCATION_ZIP - Zip codes, postcodes, or postal codes\n- MARITAL_STATUS - Terms indicating marital status\n- MONEY - Names and/or amounts of currency\n- NAME - Names of individuals\n- NAME_FAMILY - Family names\n- NAME_GIVEN - Given names\n- NAME_MEDICAL_PROFESSIONAL - Full names of medical professionals\n- NUMERICAL_PII - Numerical PII that doesn't fall under other categories\n- OCCUPATION - Job titles or professions\n- ORGANIZATION - Names of organizations\n- ORGANIZATION_MEDICAL_FACILITY - Names of medical facilities\n- ORIGIN - Terms indicating nationality, ethnicity, or provenance\n- PASSPORT_NUMBER - Passport numbers\n- PASSWORD - Account passwords, PINs, access keys\n- PHONE_NUMBER - Telephone or fax numbers\n- PHYSICAL_ATTRIBUTE - Distinctive bodily attributes\n- POLITICAL_AFFILIATION - Terms referring to political party or ideology\n- RELIGION - Terms indicating religious affiliation\n- SSN - Social Security Numbers or equivalent\n- TIME - Expressions indicating clock times\n- URL - Internet addresses\n- USERNAME - Usernames, login names, or handles\n- VEHICLE_ID - Vehicle identification numbers and license plates\n- ZODIAC_SIGN - Names of Zodiac signs\n\n### PHI Entities (Protected Health Information)\n\n- BLOOD_TYPE - Blood types\n- CONDITION - Names of medical conditions, diseases, syndromes\n- DOSE - Medically prescribed quantity of medication\n- DRUG - Medications, vitamins, and supplements\n- INJURY - Bodily injuries\n- MEDICAL_PROCESS - Medical processes, treatments, procedures\n- STATISTICS - Medical statistics\n\n### Financial Entities\n\n- BANK_ACCOUNT - Bank account numbers and IBAN\n- CREDIT_CARD - Credit card numbers\n- CREDIT_CARD_EXPIRATION - Expiration date of credit cards\n- CVV - Card verification codes\n- ROUTING_NUMBER - Bank routing numbers"
    },
    {
      "title": "Translation",
      "path": "audio-intelligence/translation",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/translation",
      "keywords": [
        "translation",
        "multilingual",
        "subtitles translation",
        "transcript translation",
        "target languages",
        "lip sync",
        "dubbing",
        "context aware translation",
        "multiple languages"
      ],
      "use_cases": [
        "how to translate transcriptions to multiple languages",
        "how to generate translated subtitles",
        "when to use enhanced vs base translation model",
        "how to enable lip sync for dubbing",
        "how to provide context for better translation"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "translation",
        "multilingual"
      ],
      "priority": 9,
      "content": "# Translation\n\nThe **Translation** model generates translations of your transcriptions to one or more targeted languages. If subtitles and/or sentences are enabled, the translations will also include translated results for them.\n\nYou can translate your transcription to **multiple languages** in a single API call. The list of the languages covered by the Translation feature are listed in Supported Languages.\n\n## Translation Models\n\n**2 translation models** are available:\n\n- `base` - Fast, covers most use cases\n- `enhanced` - Slower, but higher quality and with context awareness\n\n## Quickstart\n\nTo enable translation, set `translation` to `true` on your request, and add a `translation_config` object:\n\n### Live transcription\n\n```json\n{\n  \"realtime_processing\": {\n    \"translation\": true,\n    \"translation_config\": {\n      \"target_languages\": [\"fr\"],\n      \"model\": \"base\",\n      \"match_original_utterances\": true,\n      \"lipsync\": true,\n      \"context_adaptation\": true,\n      \"context\": \"<string>\",\n      \"informal\": false\n    }\n  },\n  \"messages_config\": {\n    \"receive_realtime_processing_events\": true\n  }\n}\n```\n\n### Pre-recorded\n\n```json\n{\n  \"translation\": true,\n  \"translation_config\": {\n    \"target_languages\": [\"fr\", \"es\"],\n    \"model\": \"enhanced\"\n  }\n}\n```\n\n## Translation Configuration Fields\n\n| Field | Type | Default | Description |\n|-------|------|---------|-------------|\n| target_languages | string[] | required | Target language codes for translation output |\n| model | enum[\"base\", \"enhanced\"] | \"base\" | Specifies the translation model to be used |\n| match_original_utterances | boolean | true | Keep translated segments aligned with source segmentation. Use `true` for subtitles/dubbing; set `false` for more natural flow |\n| lipsync | boolean | true | Controls alignment with visual cues, specifically lip movements. Enhances viewing experience for dubbed content |\n| context_adaptation | boolean | true | Enable context-aware translation. Leverages extra context and style preferences for better accuracy |\n| context | string | - | Additional context to improve terminology, proper nouns, or disambiguation. Effective with `context_adaptation: true` |\n| informal | boolean | false | Prefer informal register when available; useful for chatty UX or youth audiences |\n\n## Result\n\nThe transcription result will contain a `\"translation\"` key with the output of the model:\n\n```json\n{\n  \"transcription\": {...},\n  \"translation\": {\n    \"success\": true,\n    \"is_empty\": false,\n    \"results\": [\n      {\n        \"words\": [\n          {\n            \"word\": \"Diviser\",\n            \"start\": 0.20043,\n            \"end\": 0.70080,\n            \"confidence\": 1\n          },\n          {\n            \"word\": \"l'infini\",\n            \"start\": 0.90095,\n            \"end\": 1.56144,\n            \"confidence\": 1\n          }\n        ],\n        \"languages\": [\"fr\"],\n        \"full_transcript\": \"Diviser l'infini dans un temps o moins est plus...\",\n        \"utterances\": [...],\n        \"error\": null\n      }\n    ],\n    \"exec_time\": 0.6475496292114258,\n    \"error\": null\n  }\n}\n```\n\nIf you enabled the `subtitles` generation, those will also benefit from the translation model.\n\n## Best Practices\n\n- Set `target_languages` to only the languages you need.\n- Use `enhanced` with `context_adaptation` for high-accuracy, domain-heavy content.\n- Provide a meaningful `context` to improve terminology and named entities.\n- Keep `match_original_utterances: true` for subtitles; set to `false` for a more natural flow.\n- Pair with language detection and code switching when source language may vary."
    },
    {
      "title": "Live Transcription Features",
      "path": "features",
      "url": "https://docs.gladia.io/chapters/live-stt/features",
      "keywords": [
        "partial transcripts",
        "custom vocabulary",
        "custom spelling",
        "multiple channels",
        "custom metadata",
        "real-time features",
        "live STT configuration"
      ],
      "use_cases": [
        "how to enable partial transcripts for low latency",
        "how to improve recognition with custom vocabulary",
        "how to handle multi-channel audio streams",
        "how to add custom metadata to transcription sessions",
        "when to use custom spelling"
      ],
      "tags": [
        "live",
        "realtime",
        "streaming",
        "stt",
        "features",
        "configuration"
      ],
      "priority": 9,
      "content": "# Live Transcription Features\n\nAll the configuration properties described in the feature pages are defined in the [POST /v2/live endpoint](https://docs.gladia.io/api-reference/v2/live/init).\n\n## Available Features\n\n### Partial transcripts\nStream incremental hypotheses while audio is being processed. Useful for low-latency applications where you need to show text as it's being spoken.\n\n### Custom vocabulary\nImprove recognition for domain-specific terms and phrases. Helpful when your audio contains technical jargon, brand names, or specialized terminology.\n\n### Custom spelling\nControl how names and terms are rendered in output. Useful for ensuring proper capitalization and formatting of specific words.\n\n### Multiple channels\nHandle stereo or multi-channel inputs with channel-aware transcription. Enables speaker attribution when each speaker is on a separate channel.\n\n### Custom metadata\nAttach metadata to sessions for organization and auditability. Helps with tracking and organizing transcription sessions.\n\n## Additional Resources\n\nWant to know more about the audio intelligence features? Check out the [Audio Intelligence](https://docs.gladia.io/chapters/audio-intelligence) chapter."
    },
    {
      "title": "Pre-recorded Audio Intelligence",
      "path": "pre-recorded-stt/audio-intelligence",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/audio-intelligence",
      "keywords": [
        "audio intelligence",
        "translation",
        "summarization",
        "named entity recognition",
        "NER",
        "sentiment analysis",
        "emotion analysis",
        "chapterization",
        "audio to LLM",
        "pre-recorded"
      ],
      "use_cases": [
        "how to add translation to transcriptions",
        "how to generate summaries from audio",
        "how to detect named entities in speech",
        "how to analyze sentiment in transcriptions",
        "when to use chapterization for long audio"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "audio-intelligence",
        "AI features"
      ],
      "priority": 9,
      "content": "Audio intelligence turns raw speech into structured, useful data on top of transcription. Once the words are captured, these features help you understand, organize, and act on the content - from instant translation to key-point summaries, entity detection, chapter markers, emotions, and even custom LLM prompts.\n\n## Available Features\n\n### Translation\nTranslate transcripts and subtitles into multiple languages in one request.\n\n### Summarization\nGenerate concise summaries or bullet points for quick understanding.\n\n### Named Entity Recognition\nDetect and categorize key entities like people, organizations, dates, and more.\n\n### Chapterization\nSegment long audio into chapters with headlines and summaries for easy navigation.\n\n### Sentiment & Emotion Analysis\nUnderstand the tone and emotions expressed across the transcript.\n\n### Audio to LLM\nAsk custom questions and run prompts on your audio like an assistant would."
    },
    {
      "title": "Speaker Diarization",
      "path": "pre-recorded-stt/features/speaker-diarization",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/speaker-diarization",
      "keywords": [
        "speaker diarization",
        "multiple speakers",
        "speaker detection",
        "who said what",
        "speaker identification",
        "diarization config",
        "min speakers",
        "max speakers",
        "number of speakers"
      ],
      "use_cases": [
        "how to enable speaker diarization in transcription",
        "how to detect multiple speakers in audio",
        "how to improve diarization accuracy",
        "how to specify the number of speakers",
        "when to use min/max speaker hints"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "diarization",
        "speakers"
      ],
      "priority": 9,
      "content": "Speaker diarization is the process of detecting multiple speakers in an audio, and understanding which parts of the transcription each speaker said.\n\n## Enabling diarization\n\nDiarization is enabled by sending the `diarization` parameter in the transcription request:\n\n```json\n{\n  \"audio_url\": \"<your audio URL>\",\n  \"diarization\": true\n}\n```\n\n## Response\n\nWhen diarization is enabled, each utterance will contain a `speaker` field, whose value is an index representing the speaker. Speakers will be assigned indexes by **order of appearance** (i.e. the 1st speaker will be speaker 0, the 2nd speaker 1, etc).\n\n```json\n{\n  \"transcription\": {\n    \"utterances\": [\n      {\n        \"words\": [...],\n        \"text\": \"it says you are trained in technology.\",\n        \"language\": \"en\",\n        \"start\": 0.7334100000000001,\n        \"end\": 2.364,\n        \"confidence\": 0.8914285714285715,\n        \"channel\": 0,\n        \"speaker\": 0\n      },\n      ...\n    ]\n  }\n}\n```\n\n## Improving diarization accuracy\n\nYou can improve the accuracy of the diarization by providing the model with hints regarding the expected number or lower/upper bounds of speakers using the `diarization_config.num_of_speakers`, `diarization_config.min_speakers` and `diarization_config.max_speakers` parameters respectively.\n\n**Important:** These parameters are hints, not hard constraints. The actual number of speakers detected by the model may not comply with the provided parameters.\n\n| Key | Type | Description |\n| --- | --- | --- |\n| `diarization_config.number_of_speakers` | number | Guiding number of speakers - instructs the model to detect an exact number of speakers in the audio. |\n| `diarization_config.min_speakers` | number | Instructs the model to detect no less than this number of speakers in the audio. |\n| `diarization_config.max_speakers` | number | Causes the model to detect no more than this number of speakers in the audio. |"
    },
    {
      "title": "Callback Error Event",
      "path": "api-reference/v2/pre-recorded/callback/error",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/callback/error",
      "keywords": [
        "callback",
        "error",
        "transcription.error",
        "webhook",
        "notification",
        "failure",
        "pre-recorded"
      ],
      "use_cases": [
        "What is the callback error payload format?",
        "How to handle transcription errors via callback?",
        "What information is included in error callbacks?",
        "How to identify which transcription failed?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "callbacks",
        "error"
      ],
      "priority": 8,
      "content": "# Callback Error Event\n\nPayload definition for the callback event `transcription.error`.\n\nThis callback is sent to your configured callback URL when a transcription job fails.\n\n## Schema\n\n### Fields\n\n- **id** (string, uuid, required): Id of the job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n- **event** (enum, required): Type of event\n  - Value: `transcription.error`\n  - Example: `\"transcription.error\"`\n\n- **error** (object, required): The error that occurred during the transcription\n  - **code** (integer): HTTP error code\n  - **message** (string): Error message\n\n- **custom_metadata** (object, optional): Custom metadata given in the initial request\n  - Example: `{\"user\": \"John Doe\"}`\n\n## Example Payload\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"transcription.error\",\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Bad Request\"\n  },\n  \"custom_metadata\": {\n    \"user\": \"John Doe\"\n  }\n}\n```\n\n## Notes\n\n- This callback is sent when the transcription fails for any reason\n- Use the `id` to correlate with your original request\n- The `custom_metadata` helps identify the context of the failed job\n- Check the `error.code` and `error.message` for debugging"
    },
    {
      "title": "Callback Success Event",
      "path": "api-reference/v2/pre-recorded/callback/success",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/callback/success",
      "keywords": [
        "callback",
        "success",
        "transcription.success",
        "webhook",
        "notification",
        "result",
        "pre-recorded"
      ],
      "use_cases": [
        "What is the callback success payload format?",
        "How to receive transcription results via callback?",
        "What information is included in success callbacks?",
        "How to process callback transcription results?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "callbacks",
        "success"
      ],
      "priority": 8,
      "content": "# Callback Success Event\n\nPayload definition for the callback event `transcription.success`.\n\nThis callback is sent to your configured callback URL when a transcription job completes successfully.\n\n## Schema\n\n### Fields\n\n- **id** (string, uuid, required): Id of the job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n- **event** (enum, required): Type of event\n  - Value: `transcription.success`\n  - Example: `\"transcription.success\"`\n\n- **payload** (object, required): Result of the transcription\n  - Contains the full transcription result including:\n    - **metadata**: Audio duration, billing time, transcription time\n    - **transcription**: Full transcript, languages, utterances, sentences, subtitles\n    - **translation**: Translation results (if enabled)\n    - **summarization**: Summary results (if enabled)\n    - **moderation**: Moderation results (if enabled)\n    - **named_entity_recognition**: NER results (if enabled)\n    - **name_consistency**: Name consistency results (if enabled)\n    - **speaker_reidentification**: Speaker re-identification results\n    - **structured_data_extraction**: Structured data results (if enabled)\n    - **sentiment_analysis**: Sentiment results (if enabled)\n    - **audio_to_llm**: Audio to LLM results (if enabled)\n    - **sentences**: Sentence parsing results (if enabled)\n    - **display_mode**: Display mode results (if enabled)\n    - **chapterization**: Chapterization results (if enabled)\n    - **diarization**: Speaker diarization results (if enabled)\n\n- **custom_metadata** (object, optional): Custom metadata given in the initial request\n  - Example: `{\"user\": \"John Doe\"}`\n\n## Example Payload\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"transcription.success\",\n  \"payload\": {\n    \"metadata\": {\n      \"audio_duration\": 3600,\n      \"number_of_distinct_channels\": 1,\n      \"billing_time\": 3600,\n      \"transcription_time\": 20\n    },\n    \"transcription\": {\n      \"full_transcript\": \"...\",\n      \"languages\": [\"en\"],\n      \"utterances\": [...],\n      \"sentences\": [...],\n      \"subtitles\": [...]\n    }\n  },\n  \"custom_metadata\": {\n    \"user\": \"John Doe\"\n  }\n}\n```\n\n## Notes\n\n- The payload contains the complete transcription result\n- Same structure as the result from GET /v2/pre-recorded/{id}\n- Use callbacks to avoid polling for results\n- The `custom_metadata` helps identify and route the callback"
    },
    {
      "title": "Delete Transcription",
      "path": "api-reference/v2/pre-recorded/delete",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/delete",
      "keywords": [
        "transcription",
        "pre-recorded",
        "DELETE",
        "remove",
        "cleanup",
        "data",
        "privacy"
      ],
      "use_cases": [
        "How to delete a pre-recorded transcription?",
        "How to remove transcription data for privacy?",
        "When should I delete old transcriptions?",
        "What data is removed when deleting a transcription?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "endpoints",
        "DELETE"
      ],
      "priority": 8,
      "content": "# Delete Transcription\n\n**DELETE /v2/pre-recorded/{id}**\n\nDelete a pre-recorded transcription and all its data (audio file, transcription).\n\n## Authorization\n\n- **x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the pre recorded job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Response (202)\n\nThe pre recorded job has been successfully deleted.\n\nNo response body is returned.\n\n## Error Responses\n\n- **401**: Unauthorized (invalid API key)\n  ```json\n  {\n    \"timestamp\": \"2023-12-28T09:04:17.210Z\",\n    \"path\": \"/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab\",\n    \"request_id\": \"G-821fe9df\",\n    \"statusCode\": 401,\n    \"message\": \"gladia key not found\"\n  }\n  ```\n- **403**: Forbidden (not authorized to delete this transcription)\n- **404**: Not Found (transcription not found)\n\n## Example Request\n\n```bash\ncurl --request DELETE \\\n  --url https://api.gladia.io/v2/pre-recorded/45463597-20b7-4af7-b3b3-f5fb778203ab \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Notes\n\n- This operation is irreversible\n- Deletes both the audio file and all transcription data\n- Use for data privacy compliance or cleanup purposes"
    },
    {
      "title": "Get Transcription Result",
      "path": "api-reference/v2/pre-recorded/get",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/get",
      "keywords": [
        "transcription",
        "pre-recorded",
        "GET",
        "result",
        "status",
        "polling",
        "utterances",
        "metadata",
        "full_transcript"
      ],
      "use_cases": [
        "How to get the result of a pre-recorded transcription?",
        "How to check the status of a transcription job?",
        "When is a transcription complete and ready?",
        "How to poll for transcription results?",
        "What information is included in the transcription response?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "endpoints",
        "GET",
        "result"
      ],
      "priority": 8,
      "content": "# Get Transcription Result\n\n**GET /v2/pre-recorded/{id}**\n\nGet the pre recorded job's metadata, status, and result.\n\n## Authorization\n\n- **x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the pre recorded job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Response (200)\n\nThe pre recorded job's metadata.\n\n### Core Fields\n\n- **id** (string, uuid, required): Id of the job\n- **request_id** (string, required): Debug id (e.g., \"G-45463597\")\n- **version** (integer, required): API version (2)\n- **kind** (enum, required): Always \"pre-recorded\"\n- **status** (enum, required): Job status\n  - `queued`: The job has been queued\n  - `processing`: The job is being processed\n  - `done`: The job is complete and result is available\n  - `error`: An error occurred during processing\n\n### Timestamps\n\n- **created_at** (string, date-time, required): Creation date\n- **completed_at** (string, date-time | null): Completion date (when status is \"done\" or \"error\")\n\n### Metadata\n\n- **custom_metadata** (object): Custom metadata from the initial request\n- **post_session_metadata** (object, required): Debug metadata\n- **error_code** (integer | null): HTTP status code if status is \"error\" (400-599)\n\n### File Information\n\n- **file** (object): The uploaded file data (null if status is \"error\")\n  - **id**: File ID\n  - **filename**: Original filename\n  - **source**: File source\n  - **audio_duration**: Duration in seconds (3600)\n  - **number_of_channels**: Number of audio channels (1)\n\n### Request Parameters\n\n- **request_params** (object): Parameters used for this transcription (null if status is \"error\")\n  - Contains all parameters from the initial request (audio_url, diarization, translation, etc.)\n\n### Result (when status is \"done\")\n\n- **result** (object): Pre-recorded transcription result\n  \n  #### Metadata\n  - **metadata.audio_duration**: Audio duration in seconds\n  - **metadata.number_of_distinct_channels**: Number of distinct channels\n  - **metadata.billing_time**: Billable time in seconds\n  - **metadata.transcription_time**: Processing time in seconds\n\n  #### Transcription\n  - **transcription.full_transcript**: Complete transcript text\n  - **transcription.languages**: Array of detected languages\n  - **transcription.utterances**: Array of utterance objects\n    - **start**: Start time\n    - **end**: End time\n    - **confidence**: Confidence score\n    - **channel**: Audio channel\n    - **words**: Array of word objects (word, start, end, confidence)\n    - **text**: Utterance text\n    - **language**: Language code\n    - **speaker**: Speaker number (if diarization enabled)\n  - **transcription.sentences**: Array of sentence results\n  - **transcription.subtitles**: Array of subtitle objects (format, subtitles)\n\n  #### Audio Intelligence Results\n  - **translation**: Translation results\n  - **summarization**: Summarization results\n  - **moderation**: Moderation results\n  - **named_entity_recognition**: NER results\n  - **name_consistency**: Name consistency results\n  - **speaker_reidentification**: Speaker re-identification results\n  - **structured_data_extraction**: Structured data results\n  - **sentiment_analysis**: Sentiment analysis results\n  - **audio_to_llm**: Audio to LLM results\n  - **sentences**: Sentence parsing results\n  - **display_mode**: Display mode results\n  - **chapterization**: Chapterization results\n  - **diarization**: Speaker diarization results\n\n## Error Responses\n\n- **401**: Unauthorized (invalid API key)\n- **404**: Not Found (transcription not found)\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/pre-recorded/45463597-20b7-4af7-b3b3-f5fb778203ab \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Example Response\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"request_id\": \"G-45463597\",\n  \"version\": 2,\n  \"status\": \"done\",\n  \"created_at\": \"2023-12-28T09:04:17.210Z\",\n  \"completed_at\": \"2023-12-28T09:04:37.210Z\",\n  \"kind\": \"pre-recorded\",\n  \"custom_metadata\": {\n    \"user\": \"John Doe\"\n  },\n  \"file\": {\n    \"id\": \"...\",\n    \"filename\": \"audio.wav\",\n    \"audio_duration\": 3600,\n    \"number_of_channels\": 1\n  },\n  \"result\": {\n    \"metadata\": {\n      \"audio_duration\": 3600,\n      \"number_of_distinct_channels\": 1,\n      \"billing_time\": 3600,\n      \"transcription_time\": 20\n    },\n    \"transcription\": {\n      \"full_transcript\": \"Hello world...\",\n      \"languages\": [\"en\"],\n      \"utterances\": [...]\n    }\n  }\n}\n```"
    },
    {
      "title": "Download Audio File",
      "path": "api-reference/v2/pre-recorded/get-audio",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/get-audio",
      "keywords": [
        "transcription",
        "pre-recorded",
        "GET",
        "audio",
        "download",
        "file",
        "binary"
      ],
      "use_cases": [
        "How to download the original audio file from a transcription?",
        "How to retrieve the audio used for transcription?",
        "When would I need to download the audio file?",
        "What format is the downloaded audio file?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "endpoints",
        "GET",
        "audio"
      ],
      "priority": 8,
      "content": "# Download Audio File\n\n**GET /v2/pre-recorded/{id}/file**\n\nDownload the audio file used for this pre recorded job.\n\n## Authorization\n\n- **x-gladia-key** (header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the pre recorded job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Response (200)\n\nThe audio file used for this pre recorded job.\n\n- **Content-Type**: application/octet-stream\n- **Response Type**: file (binary)\n\n## Error Responses\n\n- **401**: Unauthorized (invalid API key)\n- **404**: Not Found (transcription or file not found)\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/pre-recorded/45463597-20b7-4af7-b3b3-f5fb778203ab/file \\\n  --header 'x-gladia-key: <api-key>' \\\n  --output audio_file.wav\n```\n\n## Notes\n\n- The file is returned as binary data\n- Use the `--output` flag with curl to save to a file\n- The original audio format is preserved"
    },
    {
      "title": "Initiate a Transcription",
      "path": "api-reference/v2/pre-recorded/init",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/init",
      "keywords": [
        "transcription",
        "pre-recorded",
        "POST",
        "initiate",
        "audio_url",
        "diarization",
        "translation",
        "summarization",
        "callback",
        "custom_vocabulary"
      ],
      "use_cases": [
        "How to start a new pre-recorded transcription job?",
        "How to transcribe an audio file from URL?",
        "How to enable speaker diarization in transcription?",
        "When to use callbacks vs polling for results?",
        "How to configure custom vocabulary for better accuracy?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "endpoints",
        "POST",
        "transcription"
      ],
      "priority": 8,
      "content": "# Initiate a Transcription\n\n**POST /v2/pre-recorded**\n\nInitiate a new pre recorded job.\n\n## Authorization\n\n- **x-gladia-key** (header, required): Your personal Gladia API key\n\n## Request Body (application/json)\n\n### Required Parameters\n\n- **audio_url** (string, uri, required): URL to a Gladia file or to an external audio or video file\n  - Example: `\"http://files.gladia.io/example/audio-transcription/split_infinity.wav\"`\n\n### Language Configuration\n\n- **language_config** (object): Specify the language configuration\n  - **languages**: Array of language codes\n  - **code_switching**: Enable multi-language detection\n\n- **detect_language** (boolean, default: true, deprecated): Use `language_config` instead\n- **enable_code_switching** (boolean, default: false, deprecated): Use `language_config` instead\n- **language** (enum, deprecated): Set the spoken language (ISO 639 standard)\n  - Options: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, zh\n\n### Custom Vocabulary (Beta)\n\n- **custom_vocabulary** (boolean, default: false): Enable custom vocabulary\n- **custom_vocabulary_config** (object): Custom vocabulary configuration\n  - **vocabulary**: Array of vocabulary items (strings or objects with value, pronunciations, intensity, language)\n  - **default_intensity**: Default intensity (0.5)\n\n### Callback Configuration\n\n- **callback** (boolean, default: false): Enable callback for this transcription\n- **callback_config** (object): Customize the callback behaviour\n  - **url**: Callback URL\n  - **method**: HTTP method (POST)\n- **callback_url** (string, uri, deprecated): Use `callback_config` instead\n\n### Subtitles Configuration\n\n- **subtitles** (boolean, default: false): Enable subtitles generation\n- **subtitles_config** (object): Configuration for subtitles\n  - **formats**: Array of formats (e.g., \"srt\")\n  - **minimum_duration**: Minimum duration (1)\n  - **maximum_duration**: Maximum duration (15.5)\n  - **maximum_characters_per_row**: Max chars per row (2)\n  - **maximum_rows_per_caption**: Max rows per caption (3)\n  - **style**: Style (\"default\")\n\n### Speaker Recognition (Diarization)\n\n- **diarization** (boolean, default: false): Enable speaker recognition\n- **diarization_config** (object): Speaker recognition configuration\n  - **number_of_speakers**: Exact number of speakers (3)\n  - **min_speakers**: Minimum speakers (1)\n  - **max_speakers**: Maximum speakers (2)\n\n### Translation (Beta)\n\n- **translation** (boolean, default: false): Enable translation\n- **translation_config** (object): Translation configuration\n  - **target_languages**: Array of target language codes\n  - **model**: Model type (\"base\")\n  - **match_original_utterances**: Match original utterances (true)\n  - **lipsync**: Enable lipsync (true)\n  - **context_adaptation**: Enable context adaptation (true)\n  - **context**: Context string\n  - **informal**: Use informal language (false)\n\n### Summarization (Beta)\n\n- **summarization** (boolean, default: false): Enable summarization\n- **summarization_config** (object): Summarization configuration\n  - **type**: Type (\"general\")\n\n### Audio Intelligence Features\n\n- **moderation** (boolean, default: false, Alpha): Enable content moderation\n- **named_entity_recognition** (boolean, default: false, Alpha): Enable NER\n- **chapterization** (boolean, default: false, Alpha): Enable chapterization\n- **name_consistency** (boolean, default: false, Alpha): Enable name consistency\n- **sentiment_analysis** (boolean, default: false): Enable sentiment analysis\n\n### Custom Spelling (Alpha)\n\n- **custom_spelling** (boolean, default: false): Enable custom spelling\n- **custom_spelling_config** (object): Custom spelling configuration\n  - **spelling_dictionary**: Dictionary mapping (e.g., {\"Gettleman\": [\"gettleman\"], \"SQL\": [\"Sequel\"]})\n\n### Structured Data Extraction (Alpha)\n\n- **structured_data_extraction** (boolean, default: false): Enable structured data extraction\n- **structured_data_extraction_config** (object): Configuration\n  - **classes**: Array of classes (e.g., [\"Persons\", \"Organizations\"])\n\n### Audio to LLM (Alpha)\n\n- **audio_to_llm** (boolean, default: false): Enable audio to LLM processing\n- **audio_to_llm_config** (object): Configuration\n  - **prompts**: Array of prompts (e.g., [\"Extract the key points from the transcription\"])\n\n### Other Options\n\n- **custom_metadata** (object): Custom metadata to attach (e.g., {\"user\": \"John Doe\"})\n- **sentences** (boolean, default: false): Enable sentences output\n- **display_mode** (boolean, default: false, Alpha): Change output display mode\n- **punctuation_enhanced** (boolean, default: false, Alpha): Use enhanced punctuation\n- **context_prompt** (string, deprecated): Context to feed the model\n\n## Response (201)\n\nThe pre recorded job has been initiated.\n\n- **id** (string, uuid, required): Id of the job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n- **result_url** (string, uri, required): Prebuilt URL to fetch the result\n  - Example: `\"https://api.gladia.io/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Error Responses\n\n- **400**: Bad Request\n- **401**: Unauthorized (invalid API key)\n- **422**: Unprocessable Entity\n\n## Example Request\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/pre-recorded \\\n  --header 'Content-Type: application/json' \\\n  --header 'x-gladia-key: <api-key>' \\\n  --data '{\n    \"audio_url\": \"http://files.gladia.io/example/audio-transcription/split_infinity.wav\",\n    \"diarization\": true,\n    \"diarization_config\": {\n      \"min_speakers\": 1,\n      \"max_speakers\": 4\n    }\n  }'\n```\n\n## Example Response\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"result_url\": \"https://api.gladia.io/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab\"\n}\n```"
    },
    {
      "title": "List Transcriptions",
      "path": "api-reference/v2/pre-recorded/list",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/list",
      "keywords": [
        "transcription",
        "pre-recorded",
        "GET",
        "list",
        "pagination",
        "filter",
        "status",
        "date"
      ],
      "use_cases": [
        "How to list all my pre-recorded transcriptions?",
        "How to filter transcriptions by status?",
        "How to paginate through transcription results?",
        "How to find transcriptions by date range?",
        "How to filter by custom metadata?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "endpoints",
        "GET",
        "list"
      ],
      "priority": 8,
      "content": "# List Transcriptions\n\n**GET /v2/pre-recorded**\n\nGet pre recorded jobs based on query parameters.\n\n## Authorization\n\n- **x-gladia-key** (header, required): Your personal Gladia API key\n\n## Query Parameters\n\n### Pagination\n\n- **offset** (integer, default: 0): The starting point for pagination (>= 0)\n- **limit** (integer, default: 20): Maximum number of items to return (>= 1)\n\n### Date Filters\n\n- **date** (string, date-time): Filter by specific date (ISO format YYYY-MM-DD)\n  - Example: `\"2026-01-12\"`\n- **before_date** (string, date-time): Include items before this date (ISO format)\n  - Example: `\"2026-01-12T00:00:09.657Z\"`\n- **after_date** (string, date-time): Include items after this date (ISO format)\n  - Example: `\"2026-01-12T00:00:09.657Z\"`\n\n### Status Filter\n\n- **status** (enum[], optional): Filter by item status\n  - Options: `queued`, `processing`, `done`, `error`\n  - Example: `[\"done\"]`\n\n### Custom Metadata Filter\n\n- **custom_metadata** (object): Filter by custom metadata\n  - Example: `{\"user\": \"John Doe\"}`\n\n## Response (200)\n\nA list of pre recorded jobs matching the parameters.\n\n### Pagination Links\n\n- **first** (string, uri, required): URL to fetch the first page\n- **current** (string, uri, required): URL to fetch the current page\n- **next** (string, uri | null, required): URL to fetch the next page (null if last page)\n\n### Items Array\n\n- **items** (object[], required): List of pre-recorded transcriptions\n  - Each item contains the same structure as the GET /v2/pre-recorded/{id} response\n\n## Error Responses\n\n- **401**: Unauthorized (invalid API key)\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url 'https://api.gladia.io/v2/pre-recorded?limit=20&status=done' \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Example Response\n\n```json\n{\n  \"first\": \"https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20\",\n  \"current\": \"https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20\",\n  \"next\": \"https://api.gladia.io/v2/transcription?status=done&offset=20&limit=20\",\n  \"items\": [\n    {\n      \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n      \"request_id\": \"G-45463597\",\n      \"version\": 2,\n      \"status\": \"done\",\n      \"created_at\": \"2023-12-28T09:04:17.210Z\",\n      \"completed_at\": \"2023-12-28T09:04:37.210Z\",\n      \"kind\": \"pre-recorded\",\n      ...\n    }\n  ]\n}\n```\n\n## Notes\n\n- Use pagination to avoid large response payloads\n- Combine date filters with status for precise queries\n- Custom metadata filtering enables user-specific queries"
    },
    {
      "title": "Webhook Created Event",
      "path": "api-reference/v2/pre-recorded/webhook/created",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/webhook/created",
      "keywords": [
        "webhook",
        "created",
        "transcription.created",
        "notification",
        "job",
        "pre-recorded"
      ],
      "use_cases": [
        "What is the webhook created event format?",
        "How to know when a transcription job is created?",
        "What is the difference between callback and webhook?",
        "How to track transcription job lifecycle?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "webhooks",
        "created"
      ],
      "priority": 8,
      "content": "# Webhook Created Event\n\nPayload definition for the webhook event `transcription.created`.\n\nThis webhook is triggered when a new pre-recorded transcription job is created.\n\n## Schema\n\n### Fields\n\n- **event** (enum, required): Type of event\n  - Value: `transcription.created`\n  - Example: `\"transcription.created\"`\n\n- **payload** (object, required): Event payload\n  - **id** (string, uuid): Id of the created job\n    - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Payload\n\n```json\n{\n  \"event\": \"transcription.created\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Notes\n\n- This webhook notifies when a job enters the queue\n- Use the `id` to track the job through its lifecycle\n- Webhooks are account-level notifications (configured separately from per-request callbacks)\n- This event is sent before processing begins"
    },
    {
      "title": "Webhook Error Event",
      "path": "api-reference/v2/pre-recorded/webhook/error",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/webhook/error",
      "keywords": [
        "webhook",
        "error",
        "transcription.error",
        "notification",
        "failure",
        "pre-recorded"
      ],
      "use_cases": [
        "What is the webhook error event format?",
        "How to receive error notifications via webhook?",
        "What is the difference between callback error and webhook error?",
        "How to monitor transcription failures at account level?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "webhooks",
        "error"
      ],
      "priority": 8,
      "content": "# Webhook Error Event\n\nPayload definition for the webhook event `transcription.error`.\n\nThis webhook is triggered when a pre-recorded transcription job fails.\n\n## Schema\n\n### Fields\n\n- **event** (enum, required): Type of event\n  - Value: `transcription.error`\n  - Example: `\"transcription.error\"`\n\n- **payload** (object, required): Event payload\n  - **id** (string, uuid): Id of the failed job\n    - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Payload\n\n```json\n{\n  \"event\": \"transcription.error\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Notes\n\n- This webhook provides a lightweight notification of failure\n- Use the `id` to fetch full error details via GET /v2/pre-recorded/{id}\n- Webhooks are account-level notifications (configured separately from per-request callbacks)\n- The callback error event contains more detail (error code and message)"
    },
    {
      "title": "Webhook Success Event",
      "path": "api-reference/v2/pre-recorded/webhook/success",
      "url": "https://docs.gladia.io/api-reference/v2/pre-recorded/webhook/success",
      "keywords": [
        "webhook",
        "success",
        "transcription.success",
        "notification",
        "complete",
        "pre-recorded"
      ],
      "use_cases": [
        "What is the webhook success event format?",
        "How to receive success notifications via webhook?",
        "What is the difference between callback success and webhook success?",
        "How to monitor transcription completions at account level?"
      ],
      "tags": [
        "api-reference",
        "pre-recorded",
        "webhooks",
        "success"
      ],
      "priority": 8,
      "content": "# Webhook Success Event\n\nPayload definition for the webhook event `transcription.success`.\n\nThis webhook is triggered when a pre-recorded transcription job completes successfully.\n\n## Schema\n\n### Fields\n\n- **event** (enum, required): Type of event\n  - Value: `transcription.success`\n  - Example: `\"transcription.success\"`\n\n- **payload** (object, required): Event payload\n  - **id** (string, uuid): Id of the completed job\n    - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Payload\n\n```json\n{\n  \"event\": \"transcription.success\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Notes\n\n- This webhook provides a lightweight notification of completion\n- Use the `id` to fetch full results via GET /v2/pre-recorded/{id}\n- Webhooks are account-level notifications (configured separately from per-request callbacks)\n- The callback success event contains the full result payload"
    },
    {
      "title": "Audio Intelligence for Live Transcription",
      "path": "audio-intelligence",
      "url": "https://docs.gladia.io/chapters/live-stt/audio-intelligence",
      "keywords": [
        "audio intelligence",
        "translation",
        "summarization",
        "NER",
        "named entity recognition",
        "sentiment analysis",
        "emotion analysis",
        "chapterization"
      ],
      "use_cases": [
        "how to add translation to live transcription",
        "how to use summarization with streaming audio",
        "how to detect entities in real-time transcription",
        "when to use audio intelligence features"
      ],
      "tags": [
        "live",
        "realtime",
        "streaming",
        "stt",
        "audio-intelligence",
        "NLP"
      ],
      "priority": 8,
      "content": "# Audio Intelligence for Live Transcription\n\nAudio intelligence turns raw speech into structured, useful data on top of transcription. Once the words are captured, these features help you understand, organize, and act on the content - from instant translation to key-point summaries, entity detection, chapter markers, or emotions analysis.\n\n## Available Features\n\n### Translation\nTranslate transcripts and subtitles into multiple languages in one request.\n\n### Summarization\nGenerate concise summaries or bullet points for quick understanding.\n\n### Named Entity Recognition\nDetect and categorize key entities like people, organizations, dates, and more.\n\n### Chapterization\nSegment long audio into chapters with headlines and summaries for easy navigation.\n\n### Sentiment & Emotion Analysis\nUnderstand the tone and emotions expressed across the transcript.\n\nFor detailed documentation on each feature, see the [Audio Intelligence](https://docs.gladia.io/chapters/audio-intelligence) chapter."
    },
    {
      "title": "Audio to LLM",
      "path": "audio-intelligence/audio-to-llm",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/audio-to-llm",
      "keywords": [
        "audio to llm",
        "custom prompts",
        "llm integration",
        "transcription analysis",
        "key points extraction",
        "title generation",
        "audio assistant",
        "ai prompts"
      ],
      "use_cases": [
        "how to extract key points from audio transcription",
        "how to generate titles from transcription",
        "when to use custom LLM prompts on audio",
        "how to ask questions about transcribed content",
        "how to run custom analysis on audio files"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "llm",
        "prompts"
      ],
      "priority": 8,
      "content": "# Audio to LLM\n\n> This feature is in **Alpha** state. We're looking for feedback to improve this feature.\n\nThe **Audio to LLM** feature applies your own prompts to the audio transcription. For now, it is only available for **pre-recorded** audio.\n\n## Usage\n\nEnable Audio to LLM by setting the appropriate flag and providing your prompts:\n\n### Pre-recorded\n\n```json\n{\n  \"audio_to_llm\": true,\n  \"audio_to_llm_config\": {\n    \"prompts\": [\n      \"Extract the key points from the transcription as bullet points\",\n      \"Generate a title from this transcription\"\n    ]\n  }\n}\n```\n\n## Response\n\nWith this code, your output will look like this:\n\n```json\n{\n  \"success\": true,\n  \"is_empty\": false,\n  \"results\": [\n    {\n      \"success\": true,\n      \"is_empty\": false,\n      \"results\": {\n        \"prompt\": \"Extract the key points from the transcription as bullet points\",\n        \"response\": \"The main entities key points from the transcription are:\\n- ...\"\n      },\n      \"exec_time\": 1.7726809978485107,\n      \"error\": null\n    },\n    {\n      \"success\": true,\n      \"is_empty\": false,\n      \"results\": {\n        \"prompt\": \"Generate a title from this transcription\",\n        \"response\": \"The Great Title\"\n      },\n      \"exec_time\": 1.7832809978258485,\n      \"error\": null\n    }\n  ],\n  \"exec_time\": 6.127103805541992,\n  \"error\": null\n}\n```\n\nYou'll find the results for each prompt under the `results` key."
    },
    {
      "title": "Chapterization",
      "path": "audio-intelligence/chapterization",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/chapterization",
      "keywords": [
        "chapterization",
        "audio chapters",
        "content segmentation",
        "meeting navigation",
        "chapter summaries",
        "headlines",
        "keywords extraction",
        "long audio"
      ],
      "use_cases": [
        "how to segment long audio into chapters",
        "how to add chapter navigation to meeting recordings",
        "when to use chapterization for podcasts",
        "how to get summaries and headlines for audio sections",
        "how to extract keywords from audio chapters"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "chapterization",
        "segmentation"
      ],
      "priority": 8,
      "content": "# Chapterization\n\n> This feature is in **Alpha** state. Breaking changes may still be introduced to this API, but advance notice will be sent. We're looking for feedback to improve this feature.\n\nThe chapterization model segments the audio into logical chapters based on the audio and content, and makes it easier to navigate long audios such as meeting recordings. Each chapter will contain its start and end time, as well as a summary, headline, bottom line \"gist\" and keywords.\n\n## Usage\n\nEnable chapterization by setting the appropriate flag:\n\n### Live transcription\n\n```json\n{\n  \"post_processing\": {\n    \"chapterization\": true\n  },\n  \"messages_config\": {\n    \"receive_post_processing_events\": true\n  }\n}\n```\n\n### Pre-recorded\n\n```json\n{\n  \"chapterization\": true\n}\n```\n\n## Result\n\nThe transcription result will contain a `\"chapterization\"` key with the output of the model:\n\n```json\n{\n  \"transcription\": {...},\n  \"chapterization\": {\n    \"success\": true,\n    \"is_empty\": false,\n    \"results\": [\n      {\n        \"summary\": \"In a world where minimalism is valued, yet excess is desired, hope for the future remains. The past predicts the present, which is yet to be determined.\",\n        \"headline\": \"Headline: \\\"Embracing Hope: The Interconnectedness of Past, Present, and Future\\\"\",\n        \"gist\": \"Embracing Hope: Past, Present, Future Interconnected\",\n        \"keywords\": [\n          \"Split infinity\",\n          \"less is more\",\n          \"too much\",\n          \"hope\",\n          \"present\"\n        ],\n        \"start\": 0.0,\n        \"end\": 19.83977\n      }\n    ],\n    \"exec_time\": 5.078396797180176,\n    \"error\": null\n  }\n}\n```"
    },
    {
      "title": "Sentiment and Emotion Analysis",
      "path": "audio-intelligence/sentiment-analysis",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/sentiment-analysis",
      "keywords": [
        "sentiment analysis",
        "emotion detection",
        "tone analysis",
        "positive negative neutral",
        "speaker emotions",
        "diarization",
        "conversation analysis",
        "customer sentiment"
      ],
      "use_cases": [
        "how to detect emotions in audio transcription",
        "how to analyze customer sentiment in calls",
        "when to use sentiment analysis for feedback",
        "how to combine sentiment with speaker diarization",
        "how to identify positive and negative moments in conversations"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "sentiment",
        "emotion"
      ],
      "priority": 8,
      "content": "# Sentiment and Emotion Analysis\n\nThe sentiment and emotion analysis model analyzes the transcript, detecting the general sentiment which is conveyed in each sentence (positive, neutral or negative) as well as any emotion that is being expressed.\n\n## Usage\n\nTo enable sentiment and emotion analysis simply set the appropriate flag:\n\n### Live transcription\n\n```json\n{\n  \"realtime_processing\": {\n    \"sentiment_analysis\": true\n  },\n  \"messages_config\": {\n    \"receive_realtime_processing_events\": true\n  }\n}\n```\n\n### Pre-recorded\n\n```json\n{\n  \"sentiment_analysis\": true\n}\n```\n\n## Result\n\nYour transcription result will contain a `sentiment_analysis` key which will contain an array of all sentences in the audio, and for each one the conveyed sentiment and expressed emotion.\n\nWhen `diarization` is enabled, the sentiment analysis output will contain the `speaker` as well, allowing you to analyze each speaker separately.\n\n```json\n{\n  \"transcription\": {...},\n  \"sentiment_analysis\": {\n    \"success\": true,\n    \"is_empty\": false,\n    \"results\": [\n      {\n        \"text\": \"Jonathan, it says you are trained in technology.\",\n        \"sentiment\": \"neutral\",\n        \"emotion\": \"neutral\",\n        \"start\": 0.45158,\n        \"end\": 2.364,\n        \"channel\": 0,\n        \"speaker\": 0\n      },\n      {\n        \"text\": \"That's very good.\",\n        \"sentiment\": \"positive\",\n        \"emotion\": \"positive_surprise\",\n        \"start\": 2.54438,\n        \"end\": 3.54323,\n        \"channel\": 0,\n        \"speaker\": 0\n      }\n    ],\n    \"exec_time\": 1.127103805541992,\n    \"error\": null\n  }\n}\n```\n\n## Possible Values\n\n### Sentiments\n\n- positive\n- negative\n- neutral\n- mixed\n- unknown\n\n### Emotions\n\n- adoration\n- amusement\n- anger\n- awe\n- confusion\n- contempt\n- contentment\n- desire\n- disappointment\n- disgust\n- distress\n- ecstatic\n- elation\n- embarrassment\n- fear\n- interest\n- pain\n- realization\n- relief\n- sadness\n- negative_surprise\n- positive_surprise\n- sympathy\n- triumph\n- neutral"
    },
    {
      "title": "Summarization",
      "path": "audio-intelligence/summarization",
      "url": "https://docs.gladia.io/chapters/audio-intelligence/summarization",
      "keywords": [
        "summarization",
        "audio summary",
        "transcript summary",
        "bullet points",
        "concise summary",
        "key takeaways",
        "meeting summary",
        "content overview"
      ],
      "use_cases": [
        "how to generate summaries from audio",
        "how to get bullet points from transcription",
        "when to use concise vs general summary",
        "how to create meeting notes automatically",
        "how to extract key information from long audio"
      ],
      "tags": [
        "audio-intelligence",
        "ai-features",
        "summarization"
      ],
      "priority": 8,
      "content": "# Summarization\n\nThe **Summarization** model generates a summary of your transcript. You can choose one of our summary types to customize the summarization based on your preference.\n\n## Summarization Types\n\n**3 summarization types** are available:\n\n- `general` - A regular summary of the transcription\n- `concise` - A shorter summary for quick overview\n- `bullet_points` - Retrieve the key points in a list\n\nIf no `summarization_config` is provided, `general` type will be used by default.\n\n### Notes on Options\n\n- **general**: Balanced summary for most use cases; good readability and coverage.\n- **concise**: Shorter output for quick overviews or previews; fewer details.\n- **bullet_points**: Lists key takeaways; ideal for action items, meeting notes, or highlights.\n\n## Usage\n\nTo enable summarization simply set the `\"summarization\"` parameter to true:\n\n### Live transcription\n\n```json\n{\n  \"post_processing\": {\n    \"summarization\": true,\n    \"summarization_config\": {\n      \"type\": \"concise\"\n    }\n  },\n  \"messages_config\": {\n    \"receive_post_processing_events\": true\n  }\n}\n```\n\n### Pre-recorded\n\n```json\n{\n  \"summarization\": true,\n  \"summarization_config\": {\n    \"type\": \"bullet_points\"\n  }\n}\n```\n\n## Result\n\nThe transcription result will contain a `\"summarization\"` key with the output of the model:\n\n```json\n{\n  \"transcription\": {...},\n  \"summarization\": {\n    \"success\": true,\n    \"is_empty\": false,\n    \"results\": \"This transcription suggests that...\",\n    \"exec_time\": 1.5126123428344727,\n    \"error\": null\n  }\n}\n```\n\nYou'll find the summarization of your audio under the `results` key."
    },
    {
      "title": "Code Switching",
      "path": "code-switching",
      "url": "https://docs.gladia.io/chapters/language/code-switching",
      "keywords": [
        "code switching",
        "multilingual",
        "language switching",
        "mixed languages",
        "language detection",
        "real-time transcription",
        "language_config",
        "bilingual"
      ],
      "use_cases": [
        "How to transcribe conversations with multiple languages",
        "How to enable code switching in Gladia API",
        "When speakers switch languages mid-conversation",
        "How to get detected language per utterance",
        "How to configure language_config for multilingual audio"
      ],
      "tags": [
        "language",
        "localization",
        "i18n",
        "multilingual",
        "transcription"
      ],
      "priority": 8,
      "content": "# Code Switching\n\nCode switching handles conversations where speakers switch languages mid-utterance or across turns. Gladia can detect and transcribe the active language dynamically, and annotate results with the detected languages code.\n\nFor best accuracy and latency, provide a small set of expected languages. Avoid listing dozens of similar languages.\n\n## How to use code switching?\n\nEnable code switching in the session configuration:\n\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"sample_rate\": 16000,\n  \"bit_depth\": 16,\n  \"channels\": 1,\n  \"language_config\": {\n    \"languages\": [\"en\", \"de\", \"fr\"],\n    \"code_switching\": true\n  }\n}\n```\n\nWhen enabled, transcript messages include the detected `language` per utterance and per word. Use the [`transcript` message](https://docs.gladia.io/api-reference/v2/live/message/transcript) schema for details.\n\nYou can also omit `languages` to allow detection across all supported languages, but constraining the set usually improves accuracy.\n\n## Recommendations\n\n- Limit the number of languages to those you expect.\n- Combine with [custom vocabulary](https://docs.gladia.io/chapters/pre-recorded-stt/features#custom-vocabulary) per language for domain terms."
    },
    {
      "title": "Custom Vocabulary",
      "path": "custom-vocabulary",
      "url": "https://docs.gladia.io/chapters/live-stt/features/custom-vocabulary",
      "keywords": [
        "custom vocabulary",
        "vocabulary boost",
        "word recognition",
        "custom_vocabulary_config",
        "intensity",
        "pronunciations",
        "domain-specific",
        "proper nouns",
        "technical terms"
      ],
      "use_cases": [
        "How to boost recognition of specific words?",
        "How to improve accuracy for domain-specific terms?",
        "How to configure custom vocabulary with pronunciations?",
        "How to set intensity for vocabulary boosting?"
      ],
      "tags": [
        "live",
        "realtime",
        "features",
        "vocabulary",
        "accuracy"
      ],
      "priority": 8,
      "content": "# Custom Vocabulary\n\nTo enhance the precision of words you know will recur often in your transcription, use the `custom_vocabulary` feature.\n\n## Configuration Example\n\n```json\n{\n  \"realtime_processing\": {\n    \"custom_vocabulary\": true,\n    \"custom_vocabulary_config\": {\n      \"vocabulary\": [\n        \"Westeros\",\n        {\"value\": \"Stark\"},\n        {\n          \"value\": \"Night's Watch\",\n          \"pronunciations\": [\"Nightz Watch\"],\n          \"intensity\": 0.4,\n          \"language\": \"en\"\n        }\n      ],\n      \"default_intensity\": 0.6\n    }\n  }\n}\n```\n\n## Parameters\n\n### default_intensity\n- **Type:** number\n- **Description:** The global intensity of the feature (minimum 0, maximum 1, default 0.5).\n\n### vocabulary\n- **Type:** object array\n- **Properties:**\n\n#### value (required)\n- **Type:** string\n- **Description:** The text used to replace in the transcription\n\n#### pronunciations\n- **Type:** string array\n- **Description:** The pronunciations used in the transcription language, or `vocabulary.language` if present.\n\n#### intensity\n- **Type:** number\n- **Description:** The intensity of the feature for this particular word (minimum 0, maximum 1, default 0.5).\n\n#### language\n- **Type:** string\n- **Description:** Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language."
    },
    {
      "title": "Custom Vocabulary",
      "path": "custom-vocabulary",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/custom-vocabulary",
      "keywords": [
        "custom vocabulary",
        "vocabulary boost",
        "custom_vocabulary_config",
        "word recognition",
        "pronunciations",
        "intensity",
        "transcription accuracy",
        "domain-specific terms"
      ],
      "use_cases": [
        "How to improve recognition of specific words?",
        "How to configure custom vocabulary with pronunciations?",
        "When to use vocabulary boost for domain terms?",
        "How to set intensity for custom vocabulary?",
        "How to specify pronunciation variations for words?"
      ],
      "tags": [
        "pre-recorded",
        "features",
        "customization"
      ],
      "priority": 8,
      "content": "# Custom Vocabulary\n\nTo enhance the precision of transcription, especially for recurring words or phrases, use `custom_vocabulary`.\n\n## Request Example\n\n```json\n{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"custom_vocabulary\": true,\n  \"custom_vocabulary_config\": {\n    \"vocabulary\": [\n      \"Westeros\",\n      {\"value\": \"Stark\"},\n      {\n        \"value\": \"Night's Watch\",\n        \"pronunciations\": [\"Nightz Vatch\"],\n        \"intensity\": 0.4,\n        \"language\": \"de\"\n      }\n    ],\n    \"default_intensity\": 0.6\n  }\n}\n```\n\n## Parameters\n\n### default_intensity\n`number` - The global intensity of the feature (minimum 0, maximum 1, default 0.5).\n\n### vocabulary\n`object` - Array of vocabulary items with the following properties:\n\n#### value\n`string` (required) - The text used to replace in the transcription.\n\n#### pronunciations\n`string` - The pronunciations used in the transcription language, or `vocabulary.language` if present.\n\n#### intensity\n`number` - The intensity of the feature for this particular word (minimum 0, maximum 1, default 0.5).\n\n#### language\n`string` - Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language."
    },
    {
      "title": "LiveKit",
      "path": "integrations/livekit",
      "url": "https://docs.gladia.io/chapters/integrations/livekit",
      "keywords": [
        "LiveKit",
        "LiveKit Agents",
        "real-time",
        "voice agents",
        "AI agents",
        "STT",
        "TTS",
        "LLM",
        "rooms",
        "webinars"
      ],
      "use_cases": [
        "How to integrate Gladia with LiveKit Agents?",
        "How to build real-time voice assistants with LiveKit?",
        "How to add live captions to LiveKit rooms?",
        "When to use LiveKit Agents for voice applications?",
        "How to build customer-support bots with LiveKit and Gladia?"
      ],
      "tags": [
        "integration",
        "livekit",
        "voice-agents",
        "ai-agents",
        "third-party"
      ],
      "priority": 8,
      "content": "# LiveKit Integration\n\n## What is LiveKit Agents?\n\nLiveKit Agents is a framework for building real-time, voice-enabled AI applications that connect to LiveKit rooms. It lets you compose speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) to create interactive agents that can hear, think, and speak in low-latency pipelines. With the Gladia STT integration, your agents benefit from accurate, fast transcription.\n\nLooking for the full API and options? See the official LiveKit Gladia STT docs: [docs.livekit.io/agents/integrations/stt/gladia](https://docs.livekit.io/agents/integrations/stt/gladia/).\n\n## What you can build\n\n- Real-time voice assistants in LiveKit rooms\n- Live captions for webinars and meetings\n- Customer-support and IVR voice bots\n- In-app guidance with speech interfaces\n\n## Quickstart using the sample repo\n\nUse our end-to-end sample that wires LiveKit Agents with Gladia STT: [gladia-samples/integrations-examples/livekit-agent](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/livekit-agent)\n\n## Next steps\n\n- [Get your API key](https://app.gladia.io/apikeys) - Get your API key on the Gladia dashboard\n- [LiveKit agent with Gladia](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/livekit-agent) - Browse and run the full sample\n- [LiveKit Gladia STT docs](https://docs.livekit.io/agents/integrations/stt/gladia/) - Explore service options and parameters"
    },
    {
      "title": "Pipecat",
      "path": "integrations/pipecat",
      "url": "https://docs.gladia.io/chapters/integrations/pipecat",
      "keywords": [
        "Pipecat",
        "voice agents",
        "AI agents",
        "real-time",
        "STT",
        "TTS",
        "LLM",
        "pipeline",
        "open-source",
        "framework"
      ],
      "use_cases": [
        "How to integrate Gladia with Pipecat?",
        "How to build real-time voice agents with Pipecat and Gladia?",
        "How to create live captions using Pipecat?",
        "When to use Pipecat for voice-first AI applications?",
        "How to build contact-center bots with Pipecat and Gladia?"
      ],
      "tags": [
        "integration",
        "pipecat",
        "voice-agents",
        "ai-agents",
        "third-party"
      ],
      "priority": 8,
      "content": "# Pipecat Integration\n\n## What is Pipecat?\n\nPipecat is an open-source framework for building realtime, voice-first AI agents. It orchestrates audio input/output, speech-to-text (STT), large language models (LLMs), and text-to-speech (TTS) into one low-latency pipeline you can run on the server and connect to from the browser or native apps. With Pipecat's Gladia STT service, you can plug Gladia's accurate, low-latency transcription directly into your agent.\n\nLooking for the full API and options? See the official Pipecat Gladia STT docs: [docs.pipecat.ai/server/services/stt/gladia](https://docs.pipecat.ai/server/services/stt/gladia).\n\n## What you can build\n\n- Voice assistants that understand users in real time\n- Live captions and meeting companions\n- Contact-center bots and IVRs\n- In-app help widgets with speech interfaces\n\n## Quickstart using the sample repo\n\nUse our end-to-end sample that wires Pipecat with Gladia STT: [gladia-samples/integrations-examples/pipecat-bot](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/pipecat-bot)\n\n## Next steps\n\n- [Get your API key](https://app.gladia.io/apikeys) - Get your API key on the Gladia dashboard\n- [Pipecat bot with Gladia](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/pipecat-bot) - Browse and run the full sample\n- [Pipecat Gladia STT docs](https://docs.pipecat.ai/server/services/stt/gladia) - Explore service options and parameters"
    },
    {
      "title": "SDK",
      "path": "integrations/sdk",
      "url": "https://docs.gladia.io/chapters/integrations/sdk",
      "keywords": [
        "SDK",
        "JavaScript",
        "TypeScript",
        "Python",
        "integration",
        "npm",
        "pypi",
        "gladiaio-sdk",
        "library",
        "package"
      ],
      "use_cases": [
        "How to integrate Gladia using the official SDK?",
        "How to use Gladia with JavaScript or TypeScript?",
        "How to use Gladia with Python?",
        "When to use the SDK instead of direct API calls?",
        "How to quickly start building with Gladia?"
      ],
      "tags": [
        "integration",
        "sdk",
        "javascript",
        "python",
        "library"
      ],
      "priority": 8,
      "content": "# SDK\n\nTo integrate Gladia into your project faster, and use all of our best practices for transcribing audio, you can use one of our SDKs.\n\nWant to start building realtime transcription applications with Gladia? Check out our [live transcription guide](https://docs.gladia.io/chapters/live-stt/quickstart).\n\n## Available SDKs\n\n### Typescript/Javascript\n\nUse the JavaScript SDK to integrate Gladia into your project.\n\n- **Package**: [@gladiaio/sdk on npm](https://www.npmjs.com/package/@gladiaio/sdk)\n\n### Python\n\nUse the Python SDK to integrate Gladia into your project.\n\n- **Package**: [gladiaio-sdk on PyPI](https://pypi.org/project/gladiaio-sdk/)"
    },
    {
      "title": "Twilio",
      "path": "integrations/twilio",
      "url": "https://docs.gladia.io/chapters/integrations/twilio",
      "keywords": [
        "Twilio",
        "Media Streams",
        "WebSocket",
        "phone calls",
        "voice",
        "telephony",
        "IVR",
        "call center",
        "real-time",
        "live captions"
      ],
      "use_cases": [
        "How to integrate Gladia with Twilio Media Streams?",
        "How to transcribe phone calls in real-time?",
        "How to build IVR and call center bots with Gladia?",
        "How to add live captions to phone calls?",
        "When to use Twilio with Gladia for voice applications?"
      ],
      "tags": [
        "integration",
        "twilio",
        "telephony",
        "voice",
        "third-party"
      ],
      "priority": 8,
      "content": "# Twilio Integration\n\n## What is Twilio\n\nTwilio is a cloud platform that lets you add voice, messaging, and video to your apps with simple APIs. Instead of worrying about telecom infrastructure, you can focus on your product. In this guide, we'll use Twilio for voice with Gladia Speech-to-Text.\n\n## Twilio Media Streams\n\nTwilio Programmable Voice lets you make and receive calls. Media Streams can send real-time audio from a live call to your server over WebSocket. When connected to Gladia's low-latency Speech-to-Text (STT), you get instant transcription. This powers real-time voice agents, call analytics, compliance tools, and live captions.\n\nWant the full API reference? Check the official Twilio Media Streams docs: [www.twilio.com/docs/voice/twiml/stream](https://www.twilio.com/docs/voice/twiml/stream).\n\n## What you can build\n\n- Real-time transcription and analytics\n- IVR and call center bots\n- Compliance and QA monitoring\n- Live captions on phone calls\n\n## Quickstart\n\nTry our sample project that connects Twilio Media Streams with Gladia STT:\n\n[gladia-samples/integrations-examples/twilio](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/twilio)\n\n## Next steps\n\n- [Get your API key](https://app.gladia.io/apikeys) - Get your API key on the Gladia dashboard\n- [Twilio with Gladia](https://github.com/gladiaio/gladia-samples/tree/main/integrations-examples/twilio) - See and run the complete sample\n- [Twilio Media Streams docs](https://www.twilio.com/docs/voice/twiml/stream) - Learn about streaming audio from Twilio"
    },
    {
      "title": "Playground",
      "path": "introduction/playground",
      "url": "https://docs.gladia.io/chapters/introduction/playground",
      "keywords": [
        "playground",
        "testing",
        "demo",
        "web interface",
        "upload audio",
        "microphone",
        "live transcription",
        "named entity recognition"
      ],
      "use_cases": [
        "How to test Gladia without writing code?",
        "How to use the Gladia playground for transcription?",
        "How to transcribe audio from my microphone?",
        "How to upload and transcribe local audio files?"
      ],
      "tags": [
        "introduction",
        "playground",
        "testing",
        "demo"
      ],
      "priority": 8,
      "content": "# Playground\n\nUse the Gladia app to transcribe your audio.\n\nGladia's playground is a convenient way to test our Speech-To-Text API. On the playground you are able to transcribe remote audio files through URL, and also upload your local audio files, alongside with live audio transcription.\n\n## Step 1: Select your audio source\n\nChoose your audio source (stream from your microphone, or upload a local file). Then proceed to the next step.\n\n## Step 2: Select features\n\nYou'll be able to select some of the options Gladia API provide for your transcription.\n\nFor this example, we want to detect the named entity (like email addresses, phone numbers, etc.), so we turned on **named entity recognition**.\n\n> Only a few features of Gladia API are available on the playground. For more advanced testing, check our API documentation instead.\n\n## Step 3: Talk to Gladia\n\nYou can talk to Gladia by clicking on the \"Start transcribing\" button, and you'll be able to see the transcription of your voice in the \"Transcription\" tab.\n\n> Text in italic in the transcription represents partial transcripts.\n\n## Step 4: Transcribe\n\nYou can see an already formatted and readable results in the default \"Transcription\" tab, and you'll also find the result in JSON format (the one you'd get with an API call)."
    },
    {
      "title": "Automatic Language Detection",
      "path": "language-detection",
      "url": "https://docs.gladia.io/chapters/language/language-detection",
      "keywords": [
        "language detection",
        "automatic detection",
        "spoken language",
        "language identification",
        "language_config",
        "ISO 639-1",
        "real-time detection",
        "speech recognition"
      ],
      "use_cases": [
        "How to automatically detect the spoken language",
        "How to configure language detection in Gladia API",
        "When to constrain language detection to specific languages",
        "How to improve language detection accuracy",
        "How to get detected language in transcription results"
      ],
      "tags": [
        "language",
        "localization",
        "i18n",
        "detection",
        "transcription"
      ],
      "priority": 8,
      "content": "# Automatic Language Detection\n\nGladia can automatically detect the spoken language from audio, either within a candidate set you provide or across all supported languages.\n\nProvide a small list of likely languages when possible. It improves accuracy and reduces latency.\n\n## How to use language detection?\n\nYou can either omit `language_config.languages` to search across all supported languages, or pass a constrained list:\n\n### Detect across all languages\n\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"sample_rate\": 16000,\n  \"bit_depth\": 16,\n  \"channels\": 1,\n  \"language_config\": {\n    \"languages\": [],\n    \"code_switching\": false\n  }\n}\n```\n\n### Detect within a set of languages\n\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"sample_rate\": 16000,\n  \"bit_depth\": 16,\n  \"channels\": 1,\n  \"language_config\": {\n    \"languages\": [\"en\", \"fr\", \"es\"],\n    \"code_switching\": false\n  }\n}\n```\n\nThe detected `language` is returned on each utterance and can vary over time if you also enable [code switching](https://docs.gladia.io/chapters/language/code-switching).\n\n## Tips\n\n- Use [supported languages](https://docs.gladia.io/chapters/language/supported-languages) to pick valid ISO 639-1 codes.\n- Combine with `messages_config.receive_partial_transcripts` in real time to get immediate feedback.\n- For mixed-language conversations, enable [code switching](https://docs.gladia.io/chapters/language/code-switching)."
    },
    {
      "title": "Transcript message (Live)",
      "path": "live-message-transcript",
      "url": "https://docs.gladia.io/api-reference/v2/live/message/transcript",
      "keywords": [
        "live",
        "transcript",
        "message",
        "websocket",
        "real-time",
        "utterance",
        "is_final",
        "streaming"
      ],
      "use_cases": [
        "How to receive live transcription results?",
        "What is the transcript message format in WebSocket?",
        "How to check if a transcript is final?",
        "How to parse live transcription utterances?",
        "When does Gladia send transcript messages?"
      ],
      "tags": [
        "api-reference",
        "live",
        "websocket",
        "transcript",
        "message"
      ],
      "priority": 8,
      "content": "# Transcript message (Live)\n\nPayload definition for the message `transcript`.\n\nThis message contains the information about the transcript of the audio chunk.\n\n## Message Schema\n\n```json\n{\n  \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n  \"created_at\": \"2021-09-01T12:00:00.123Z\",\n  \"type\": \"transcript\",\n  \"data\": {\n    \"id\": \"00-00000011\",\n    \"is_final\": true,\n    \"utterance\": {\n      \"start\": 123,\n      \"end\": 123,\n      \"confidence\": 123,\n      \"channel\": 1,\n      \"words\": [\n        {\n          \"word\": \"<string>\",\n          \"start\": 123,\n          \"end\": 123,\n          \"confidence\": 123\n        }\n      ],\n      \"text\": \"<string>\",\n      \"language\": \"en\",\n      \"speaker\": 1\n    }\n  }\n}\n```\n\n## Fields\n\n### Root Level\n\n- **session_id** (string, required): Id of the live session\n  - Example: `\"4a39145c-2844-4557-8f34-34883f7be7d9\"`\n- **created_at** (string, required): Date of creation of the message. The date is formatted as an ISO 8601 string\n  - Example: `\"2021-09-01T12:00:00.123Z\"`\n- **type** (enum, default: transcript, required): Message type\n  - Available options: `transcript`\n- **data** (object, required): The message data\n\n### Data Object\n\n- **id** (string): Unique identifier for this transcript segment\n- **is_final** (boolean): Whether this transcript is final or interim\n  - `true`: Final transcript, will not be updated\n  - `false`: Interim transcript, may be updated with more accurate text\n- **utterance** (object): The transcribed utterance\n  - **start** (number): Start time in milliseconds\n  - **end** (number): End time in milliseconds\n  - **confidence** (number): Confidence score\n  - **channel** (integer): Audio channel number (default: 1)\n  - **words** (array): Array of word objects\n    - **word** (string): The transcribed word\n    - **start** (number): Word start time in milliseconds\n    - **end** (number): Word end time in milliseconds\n    - **confidence** (number): Word confidence score\n  - **text** (string): Full text of the utterance\n  - **language** (string): Detected language code (ISO 639)\n  - **speaker** (integer): Speaker identifier for diarization\n\n## Usage\n\nThis message is received via WebSocket when using the [Live WebSocket](/api-reference/v2/live/websocket) endpoint. Monitor the `is_final` field to determine when a transcript segment is complete and stable."
    },
    {
      "title": "Partial Transcripts",
      "path": "partial-transcripts",
      "url": "https://docs.gladia.io/chapters/live-stt/features/partial-transcripts",
      "keywords": [
        "partial transcripts",
        "streaming transcription",
        "low latency",
        "real-time",
        "is_final",
        "receive_partial_transcripts",
        "messages_config",
        "intermediate results",
        "utterance"
      ],
      "use_cases": [
        "How to enable partial transcripts for low-latency streaming?",
        "How to distinguish between partial and final transcripts?",
        "When to use partial vs final transcripts?",
        "How to configure messages_config for partial results?"
      ],
      "tags": [
        "live",
        "realtime",
        "features",
        "streaming",
        "latency"
      ],
      "priority": 8,
      "content": "# Partial Transcripts\n\nPartial transcripts provide a low-latency streaming transcription as words are spoken, offering immediate insights before the final, high-accuracy transcript is ready.\n\n## Enabling Partial Transcripts\n\nTo enable partial transcripts, add the `receive_partial_transcripts` property to the `messages_config` object:\n\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"sample_rate\": 16000,\n  \"bit_depth\": 16,\n  \"channels\": 1,\n  \"language_config\": {\n    \"languages\": [\"en\"],\n    \"code_switching\": false\n  },\n  \"messages_config\": {\n    \"receive_partial_transcripts\": true,\n    \"receive_final_transcripts\": true\n  }\n}\n```\n\nWith this configuration, you will receive both partial transcripts as they are generated and the final, most accurate version of each utterance.\n\n## How Partial Transcripts Work\n\nTo reduce the total response time and create a more fluid user experience, partial transcripts use a faster, smaller model than the one used for final transcripts, trading a small amount of accuracy for a large gain in latency.\n\n**Important:** Partial transcripts accuracy deteriorates when multiple languages and/or code switching are enabled. For best results, limit the number of languages.\n\n## Distinguishing Partial from Final Transcripts\n\nWhen `receive_partial_transcripts` is `true`, the real-time API will send transcript messages for both intermediate and final results. To distinguish between them, the message payload includes the `is_final` boolean field:\n\n- `\"is_final\": false`: The message contains a partial transcript, which is subject to change.\n- `\"is_final\": true`: The message contains the final, most accurate transcript for an utterance. This transcript will not change.\n\nIn the same utterance, the partial and final transcripts share the same `data.id`."
    },
    {
      "title": "Pre-recorded Features Overview",
      "path": "pre-recorded-stt/features",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features",
      "keywords": [
        "features",
        "speaker diarization",
        "subtitles",
        "SRT",
        "VTT",
        "custom vocabulary",
        "custom spelling",
        "punctuation",
        "sentences",
        "channels"
      ],
      "use_cases": [
        "what features are available for pre-recorded transcription",
        "how to enable speaker diarization",
        "how to export subtitles in SRT or VTT format",
        "how to improve transcription accuracy with custom vocabulary",
        "when to use multi-channel transcription"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "features",
        "overview"
      ],
      "priority": 8,
      "content": "The core functionality of the Gladia API is its Speech Recognition model, designed to convert spoken language into written text. Additional capabilities like diarization, summarization, translation, custom prompts and more can be enabled by adding parameters to your request.\n\n## Available Features\n\n### Speaker Diarization\nDetect speakers and understand who said what, and when.\n\n### Export subtitles (SRT/VTT)\nGenerate ready-to-use subtitle files in SRT or VTT formats.\n\n### Custom vocabulary\nBoost recognition accuracy for brand, product, and domain terms.\n\n### Custom spelling\nNormalize how specific words, brands, and names are spelled.\n\n### Enhanced punctuation\nImproved punctuation and casing for cleaner, easier-to-read transcripts.\n\n### Sentences\nGroup words into sentences with timing for better readability and parsing.\n\n### Name consistency\nEnforce consistent rendering of speaker and entity names.\n\n### Dual or multiple channels\nTranscribe stereo or multi-channel audio with channel-aware processing.\n\n### Custom metadata\nAttach and propagate metadata to organize and trace your jobs.\n\nWant to know more about the audio intelligence features? Check out our Audio Intelligence chapter."
    },
    {
      "title": "Custom Metadata",
      "path": "pre-recorded-stt/features/custom-metadata",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/custom-metadata",
      "keywords": [
        "custom metadata",
        "metadata",
        "filtering",
        "job tracking",
        "internal user ID",
        "transcription organization",
        "query parameters",
        "job identification"
      ],
      "use_cases": [
        "how to add metadata to transcription jobs",
        "how to filter transcriptions by metadata",
        "how to track transcriptions by internal user ID",
        "when to use custom metadata for organization",
        "how to query transcriptions with specific metadata"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "metadata",
        "filtering"
      ],
      "priority": 8,
      "content": "You can add metadata to your transcription using the `custom_metadata` input during your POST request to `/v2/pre-recorded`. This helps recognize your transcription when fetching via GET `/v2/pre-recorded/:id` and enables filtering in GET `/v2/pre-recorded`.\n\n## Adding custom metadata\n\nFor example:\n\n```json\n\"custom_metadata\": {\n  \"internalUserId\": 2348739875894375,\n  \"paymentMethod\": { \"last4Digits\": 4576 },\n  \"internalUserName\": \"Spencer\"\n}\n```\n\n## Filtering by metadata\n\nFilter results:\n\n```\nhttps://api.gladia.io/v2/pre-recorded?custom_metadata={\"internalUserId\": \"2348739875894375\"}\n```\n\nor\n\n```\nhttps://api.gladia.io/v2/pre-recorded?custom_metadata={\"paymentMethod\": {\"last4Digits\": 4576}, \"internalUserName\": \"Spencer\"}\n```\n\n## Limitations\n\n`custom_metadata` cannot be longer than 2000 characters when stringified."
    },
    {
      "title": "Recommended Parameters by Use Case",
      "path": "recommended-parameters",
      "url": "https://docs.gladia.io/chapters/recommended-parameters",
      "keywords": [
        "parameters",
        "configuration",
        "voice agents",
        "meeting recorders",
        "subtitles",
        "captioning",
        "endpointing",
        "latency",
        "best practices"
      ],
      "use_cases": [
        "what parameters should I use for voice agents",
        "how to configure Gladia for meeting recording",
        "what is the best endpointing value for subtitles",
        "how to optimize latency for voice bots",
        "how to configure realtime transcription for my use case"
      ],
      "tags": [
        "parameters",
        "configuration",
        "best-practices",
        "voice-agents",
        "subtitles"
      ],
      "priority": 8,
      "content": "# Recommended Parameters by Use Case\n\nWhen building realtime transcription applications, the **right configuration** of parameters can make a big difference regarding the transcription quality and latency. This guide provides **recommended values** for common use cases so you can get started quickly with settings optimized for **latency, readability, or accuracy**.\n\nThese recommendations apply to the **Realtime API** and can be passed during session initialization. They are starting points - feel free to fine-tune them to your needs.\n\n---\n\n## Voice Agents\n\nFor callbots, customer service assistants, or voice-driven chatbots, the top priority is **low latency**. The agent should respond quickly, even if sentence boundaries are not perfect.\n\n**Recommended parameters:**\n\n- **`endpointing`: 0.05 - 0.1**\n  Keeps conversations snappy by closing utterances quickly.\n\n- **`maximum_duration_without_endpointing`: 15s**\n  Prevents very long utterances from staying open, without cutting off the conversation.\n\n- **`messages_config.partial_transcripts`: true**\n  Enables interim results for early reactions. For that, you can use the speech_stop event to know when the user has stopped speaking.\n\n- **`language_config.language`: fixed if known**\n  Skips auto-detection for faster response.\n\nThis setup is best when **fast turn-taking** is essential!\n\n---\n\n## Meeting Recorders\n\nFor meetings, lectures, and conferences, the focus shifts to **readability and completeness**. Latency is less critical than producing well-punctuated, accurate transcripts.\n\n**Recommended parameters:**\n\n- **`endpointing`: 0.3 - 0.5**\n  Captures sentences fully before closing.\n\n- **`maximum_duration_without_endpointing`: 60s**\n  Allows for longer interventions or presentations.\n\n- **`messages_config.partial_transcripts`: true**\n  Shows live transcripts while waiting for finals.\n\n- **`realtime_processing.custom_vocabulary`: add company-specific terms**\n  Ensures correct spelling of jargon, acronyms, or product names.\n\n---\n\n## Subtitles / Captioning\n\nWhen providing live subtitles, the goal is to **sync text with the speaker**. For post-production subtitles, readability and sentence integrity matter more.\n\n**Recommended parameters:**\n\n- **`endpointing`:**\n  - `0.3` -> for **live captions** (minimal lag)\n  - `0.8` -> for **clean subtitles** (post-production or recordings)\n\n- **`maximum_duration_without_endpointing`: 5s**\n  Prevents excessively long subtitle blocks.\n\n- **`messages_config.partial_transcripts`: true**\n  Shows words as they're spoken, then refines them."
    },
    {
      "title": "Sentences",
      "path": "sentences",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/sentences",
      "keywords": [
        "sentences",
        "sentence segmentation",
        "semantic segmentation",
        "readable transcription",
        "sentence output",
        "translated sentences",
        "utterances",
        "sentence structure"
      ],
      "use_cases": [
        "How to get sentence-level segmentation in transcripts?",
        "How to combine sentences with translation?",
        "What is the difference between sentences and utterances?",
        "How to get more readable transcription results?",
        "How to access sentence data in the API response?"
      ],
      "tags": [
        "pre-recorded",
        "features"
      ],
      "priority": 8,
      "content": "# Sentences\n\nIn addition to getting the transcription split by utterances, you can request semantic sentence segmentation for more readable results.\n\n**Tip:** You can get translated sentences by enabling both `sentences` and `translation`! You'll receive sentences output for the original transcript, and each `translation` result will also contain sentences in the translated language.\n\n## Usage\n\n```json\n{\n  \"sentences\": true\n}\n```\n\n## Response\n\nThe result will contain a `sentences` key (in addition to `utterances`):\n\n```json\n\"sentences\": {\n  \"success\": true,\n  \"is_empty\": false,\n  \"results\": [\n    {\n      \"sentence\": \"Amy, it says you are trained in technology.\",\n      \"start\": 0.4681999999999999,\n      \"end\": 2.45525,\n      \"words\": [...],\n      \"confidence\": 0.95,\n      \"language\": \"en\",\n      \"speaker\": 0,\n      \"channel\": 0\n    }\n  ]\n}\n```"
    },
    {
      "title": "Export Subtitles (SRT/VTT)",
      "path": "subtitles",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/subtitles",
      "keywords": [
        "subtitles",
        "SRT",
        "VTT",
        "captions",
        "subtitles_config",
        "caption export",
        "video subtitles",
        "subtitle formats"
      ],
      "use_cases": [
        "How to export transcripts as SRT or VTT subtitles?",
        "How to configure subtitle duration and formatting?",
        "How to generate subtitles in multiple languages?",
        "What subtitle configuration options are available?",
        "How to control characters per row in subtitles?"
      ],
      "tags": [
        "pre-recorded",
        "features",
        "export"
      ],
      "priority": 8,
      "content": "# Export Subtitles (SRT/VTT)\n\nYou can export completed transcripts in both SRT and VTT formats for subtitles and captions.\n\n**Tip:** You can use the `subtitles` feature alongside `translation`. You'll have subtitles in the original language, and in each targeted translation language.\n\n## Usage\n\n```json\n{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"subtitles\": true,\n  \"subtitles_config\": {\n    \"formats\": [\"srt\", \"vtt\"],\n    \"minimum_duration\": 1,\n    \"maximum_duration\": 5,\n    \"maximum_characters_per_row\": 42,\n    \"maximum_rows_per_caption\": 2,\n    \"style\": \"compliance\"\n  }\n}\n```\n\n## Configuration Options\n\n### formats\n`enum[\"srt\", \"vtt\"]` - The format of the subtitles.\n\n### minimum_duration\n`number` - The minimum duration of the subtitles in seconds (min 0).\n\n### maximum_duration\n`number` - The maximum duration of the subtitles in seconds (min 1, max 30).\n\n### maximum_characters_per_row\n`number` - The maximum number of characters per row (min 1).\n\n### maximum_rows_per_caption\n`number` - The maximum number of rows per caption (min 1, max 5).\n\n### style\n`enum[\"default\", \"compliance\"]` - The style of the subtitles (\"default\" or \"compliance\").\n\n## Response\n\nThe JSON response adds a `subtitles` array with items like:\n\n```json\n{\n  \"format\": \"srt\",\n  \"subtitles\": \"1\\n00:00:00,210 --> 00:00:04,711.....\"\n}\n```"
    },
    {
      "title": "Supported Languages",
      "path": "supported-languages",
      "url": "https://docs.gladia.io/chapters/language/supported-languages",
      "keywords": [
        "supported languages",
        "ISO 639-1",
        "language codes",
        "auto-discovery",
        "translation support",
        "multilingual transcription",
        "language list",
        "Gladia languages"
      ],
      "use_cases": [
        "How to find the language code for transcription",
        "How to check if a language is supported by Gladia",
        "When to use ISO 639-1 vs ISO 639-3 codes",
        "How to configure language_config.languages parameter",
        "What languages support auto-discovery and translation"
      ],
      "tags": [
        "language",
        "localization",
        "i18n",
        "ISO-639",
        "transcription"
      ],
      "priority": 8,
      "content": "# Supported Languages\n\nNot sure which language is spoken? See [Automatic language detection](https://docs.gladia.io/chapters/language/language-detection). For conversations mixing languages, see [Code switching](https://docs.gladia.io/chapters/language/code-switching).\n\nIn our APIs and results, languages use 2-letter ISO 639-1 codes. When no 639-1 code exists, we use the corresponding ISO 639-3 three-letter code (e.g. `haw` for Hawaiian).\n\nUse the 2-letter code in `language_config.languages` (e.g. `[\"en\", \"fr\"]`).\n\n## Supported Languages Table\n\n| Language | Code | Auto-discovery and code-switch | Translation |\n| --- | --- | --- | --- |\n| Afrikaans | af | Yes | Yes |\n| Albanian | sq | Yes | Yes |\n| Amharic | am | Yes | Yes |\n| Arabic | ar | Yes | Yes |\n| Armenian | hy | Yes | Yes |\n| Assamese | as | Yes | Yes |\n| Azerbaijani | az | Yes | Yes |\n| Bashkir | ba | Yes | Yes |\n| Basque | eu | Yes | Yes |\n| Belarusian | be | Yes | Yes |\n| Bengali | bn | Yes | Yes |\n| Bosnian | bs | Yes | Yes |\n| Breton | br | Yes | Yes |\n| Bulgarian | bg | Yes | Yes |\n| Catalan | ca | Yes | Yes |\n| Chinese | zh | Yes | Yes |\n| Croatian | hr | Yes | Yes |\n| Czech | cs | Yes | Yes |\n| Danish | da | Yes | Yes |\n| Dutch | nl | Yes | Yes |\n| English | en | Yes | Yes |\n| Estonian | et | Yes | Yes |\n| Faroese | fo | Yes | Yes |\n| Finnish | fi | Yes | Yes |\n| French | fr | Yes | Yes |\n| Galician | gl | Yes | Yes |\n| Georgian | ka | Yes | Yes |\n| German | de | Yes | Yes |\n| Greek | el | Yes | Yes |\n| Gujarati | gu | Yes | Yes |\n| Haitian Creole | ht | Yes | Yes |\n| Hausa | ha | Yes | Yes |\n| Hawaiian | haw | Yes | Yes |\n| Hebrew | he | Yes | Yes |\n| Hindi | hi | Yes | Yes |\n| Hungarian | hu | Yes | Yes |\n| Icelandic | is | Yes | Yes |\n| Indonesian | id | Yes | Yes |\n| Italian | it | Yes | Yes |\n| Japanese | ja | Yes | Yes |\n| Javanese | jw | Yes | Yes |\n| Kannada | kn | Yes | Yes |\n| Kazakh | kk | Yes | Yes |\n| Khmer | km | Yes | Yes |\n| Korean | ko | Yes | Yes |\n| Lao | lo | Yes | Yes |\n| Latin | la | Yes | Yes |\n| Latvian | lv | Yes | Yes |\n| Lingala | ln | Yes | Yes |\n| Lithuanian | lt | Yes | Yes |\n| Luxembourgish | lb | Yes | Yes |\n| Macedonian | mk | Yes | Yes |\n| Malagasy | mg | Yes | Yes |\n| Malay | ms | Yes | Yes |\n| Malayalam | ml | Yes | Yes |\n| Maltese | mt | Yes | Yes |\n| Maori | mi | Yes | Yes |\n| Marathi | mr | Yes | Yes |\n| Mongolian | mn | Yes | Yes |\n| Myanmar | my | Yes | Yes |\n| Nepali | ne | Yes | Yes |\n| Norwegian | no | Yes | Yes |\n| Nynorsk | nn | Yes | Yes |\n| Occitan | oc | Yes | Yes |\n| Pashto | ps | Yes | Yes |\n| Persian | fa | Yes | Yes |\n| Polish | pl | Yes | Yes |\n| Portuguese | pt | Yes | Yes |\n| Punjabi | pa | Yes | Yes |\n| Romanian | ro | Yes | Yes |\n| Russian | ru | Yes | Yes |\n| Sanskrit | sa | Yes | Yes |\n| Serbian | sr | Yes | Yes |\n| Shona | sn | Yes | Yes |\n| Sindhi | sd | Yes | Yes |\n| Sinhala | si | Yes | Yes |\n| Slovak | sk | Yes | Yes |\n| Slovenian | sl | Yes | Yes |\n| Somali | so | Yes | Yes |\n| Spanish | es | Yes | Yes |\n| Sundanese | su | Yes | Yes |\n| Swahili | sw | Yes | Yes |\n| Swedish | sv | Yes | Yes |\n| Tagalog | tl | Yes | Yes |\n| Tajik | tg | Yes | Yes |\n| Tamil | ta | Yes | Yes |\n| Tatar | tt | Yes | Yes |\n| Telugu | te | Yes | Yes |\n| Thai | th | Yes | Yes |\n| Tibetan | bo | Yes | Yes |\n| Turkish | tr | Yes | Yes |\n| Turkmen | tk | Yes | Yes |\n| Ukrainian | uk | Yes | Yes |\n| Urdu | ur | Yes | Yes |\n| Uzbek | uz | Yes | Yes |\n| Vietnamese | vi | Yes | Yes |\n| Welsh | cy | Yes | Yes |\n| Wolof | wo | No | Yes |\n| Yiddish | yi | Yes | Yes |\n| Yoruba | yo | Yes | Yes |"
    },
    {
      "title": "Upload a file",
      "path": "upload-audio-file",
      "url": "https://docs.gladia.io/api-reference/v2/upload/audio-file",
      "keywords": [
        "upload",
        "audio",
        "file",
        "POST",
        "multipart",
        "form-data",
        "pre-recorded",
        "video"
      ],
      "use_cases": [
        "How to upload an audio file to Gladia?",
        "How to upload a video file for transcription?",
        "How to get a Gladia URL for my audio file?",
        "When to use file upload vs audio URL?",
        "How to check audio file metadata after upload?"
      ],
      "tags": [
        "api-reference",
        "upload",
        "POST",
        "pre-recorded",
        "audio"
      ],
      "priority": 8,
      "content": "# Upload a file\n\nUpload a file for use in a pre-recorded job.\n\n## Endpoint\n\n```\nPOST /v2/upload\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Request Body\n\nSupports both `multipart/form-data` and `application/json`.\n\n### multipart/form-data\n\n- **audio** (file): The file to be uploaded. This should be an audio or video file.\n\n### application/json\n\nYou can also provide an audio URL in JSON format.\n\n## Example Request\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/upload \\\n  --header 'Content-Type: multipart/form-data' \\\n  --header 'x-gladia-key: <api-key>' \\\n  --form audio='@example-file'\n```\n\n## Response (200 - OK)\n\n```json\n{\n  \"audio_url\": \"https://api.gladia.io/file/6c09400e-23d2-4bd2-be55-96a5ececfa3b\",\n  \"audio_metadata\": {\n    \"id\": \"6c09400e-23d2-4bd2-be55-96a5ececfa3b\",\n    \"filename\": \"short-audio-en-16000.wav\",\n    \"extension\": \"wav\",\n    \"size\": 365702,\n    \"audio_duration\": 4.145782,\n    \"number_of_channels\": 1,\n    \"source\": \"http://files.gladia.io/example/audio-transcription/split_infinity.wav\"\n  }\n}\n```\n\n### Response Fields\n\n- **audio_url** (string, uri, required): Uploaded audio file Gladia URL\n  - Example: `\"https://api.gladia.io/file/6c09400e-23d2-4bd2-be55-96a5ececfa3b\"`\n- **audio_metadata** (object, required): Uploaded audio file detected metadata\n  - **id** (string): File ID\n  - **filename** (string): Original filename\n  - **extension** (string): File extension\n  - **size** (integer): File size in bytes\n  - **audio_duration** (number): Duration in seconds\n  - **number_of_channels** (integer): Number of audio channels\n  - **source** (string): Original source URL if provided\n\n## Usage\n\nAfter uploading, use the returned `audio_url` in the [Initiate a transcription](/api-reference/v2/pre-recorded/init) endpoint."
    },
    {
      "title": "Custom Metadata",
      "path": "custom-metadata",
      "url": "https://docs.gladia.io/chapters/live-stt/features/custom-metadata",
      "keywords": [
        "custom metadata",
        "session metadata",
        "filtering",
        "tagging",
        "custom_metadata",
        "session tracking",
        "user identification",
        "API filtering"
      ],
      "use_cases": [
        "How to attach metadata to a live transcription session?",
        "How to filter transcription sessions by custom metadata?",
        "How to track sessions by internal user ID?",
        "What are the limits for custom_metadata?"
      ],
      "tags": [
        "live",
        "realtime",
        "features",
        "metadata",
        "filtering"
      ],
      "priority": 7,
      "content": "# Custom Metadata\n\nYou can attach metadata to your real-time transcription session using the `custom_metadata` property. This makes it easy to recognize your transcription when you receive data from the GET `/v2/live/:id` endpoint, and enables filtering in the GET `/v2/live` list endpoint.\n\n## Adding Custom Metadata\n\nAdd the following to your configuration:\n\n```json\n\"custom_metadata\": {\n  \"internalUserId\": 2348739875894375,\n  \"paymentMethod\": {\n    \"last4Digits\": 4576\n  },\n  \"internalUserName\": \"Spencer\"\n}\n```\n\n## Filtering by Custom Metadata\n\nThen filter results with a GET request, like:\n\n```\nhttps://api.gladia.io/v2/live?custom_metadata={\"internalUserId\": \"2348739875894375\"}\n```\n\nor:\n\n```\nhttps://api.gladia.io/v2/live?custom_metadata={\"paymentMethod\": {\"last4Digits\": 4576}, \"internalUserName\": \"Spencer\"}\n```\n\n## Limitations\n\n`custom_metadata` cannot be longer than 2000 characters when stringified."
    },
    {
      "title": "Custom Spelling",
      "path": "custom-spelling",
      "url": "https://docs.gladia.io/chapters/live-stt/features/custom-spelling",
      "keywords": [
        "custom spelling",
        "spelling dictionary",
        "spelling_dictionary",
        "custom_spelling_config",
        "word normalization",
        "spelling correction",
        "text replacement",
        "realtime_processing"
      ],
      "use_cases": [
        "How to customize spelling for specific words?",
        "How to normalize spelling variants in transcripts?",
        "How to handle names with multiple pronunciations?",
        "How to replace spoken words with specific spellings?"
      ],
      "tags": [
        "live",
        "realtime",
        "features",
        "spelling",
        "customization"
      ],
      "priority": 7,
      "content": "# Custom Spelling\n\nYou can customize how certain words, names or phrases are spelled in the final transcript.\n\n## Using Custom Spelling\n\nTo use custom spelling, provide a dictionary through the `custom_spelling_config` parameter. This dictionary should contain the correct spelling as the key and a list of one or more possible variations as the value.\n\nCustom spelling is useful in scenarios where consistent spelling of specific words is crucial (e.g., technical terms in industry-specific recordings).\n\n**Note:** The keys in the dictionary are case sensitive, while the values aren't. Values can contain multiple words.\n\n## Configuration Example\n\n```json\n{\n  \"realtime_processing\": {\n    \"custom_spelling\": true,\n    \"custom_spelling_config\": {\n      \"spelling_dictionary\": {\n        \"Gorish\": [\"ghorish\", \"gaurish\", \"gaureish\"],\n        \"Data Science\": [\"data-science\", \"data science\"],\n        \".\": [\"period\", \"full stop\"],\n        \"SQL\": [\"sequel\"]\n      }\n    }\n  }\n}\n```\n\nIn this example, the model will ensure that \"Gorish\" is spelled correctly throughout the transcript, even if it is pronounced in various ways such as \"ghorish,\" \"gaurish,\" or \"gaureish.\""
    },
    {
      "title": "Custom Spelling",
      "path": "custom-spelling",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/custom-spelling",
      "keywords": [
        "custom spelling",
        "spelling dictionary",
        "custom_spelling_config",
        "spelling variants",
        "normalize spelling",
        "word replacement",
        "transcription customization",
        "spelling correction"
      ],
      "use_cases": [
        "How to normalize spelling variants in transcripts?",
        "How to ensure consistent spelling of names and terms?",
        "When to use custom spelling for technical terminology?",
        "How to configure spelling dictionary for transcription?",
        "How to replace spoken words with specific spellings?"
      ],
      "tags": [
        "pre-recorded",
        "features",
        "customization"
      ],
      "priority": 7,
      "content": "# Custom Spelling\n\nYou can customize how certain words, names or phrases are spelled in the final transcript.\n\nTo use custom spelling, provide a dictionary through the `custom_spelling_config` parameter. This dictionary should contain the correct spelling as the key and a list of one or more possible variations as the value. Custom spelling is useful in scenarios where consistent spelling of specific words is crucial (e.g., technical terms in industry-specific recordings).\n\nThe keys in the dictionary are case sensitive, while the values aren't. Values can contain multiple words.\n\n## Request Example\n\n```json\n{\n  \"custom_spelling\": true,\n  \"custom_spelling_config\": {\n    \"spelling_dictionary\": {\n      \"Gorish\": [\"ghorish\", \"gaurish\", \"gaureish\"],\n      \"Data Science\": [\"data-science\", \"data science\"],\n      \".\": [\"period\", \"full stop\"],\n      \"SQL\": [\"sequel\"]\n    }\n  }\n}\n```\n\nIn this example, the model will ensure that \"Gorish\" is spelled correctly throughout the transcript, even if it is pronounced in various ways such as \"ghorish,\" \"gaurish,\" or \"gaureish.\""
    },
    {
      "title": "Enhanced Punctuation",
      "path": "enhanced-punctuation",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/enhanced-punctuation",
      "keywords": [
        "enhanced punctuation",
        "punctuation_enhanced",
        "comma placement",
        "sentence breaks",
        "quotation marks",
        "readability",
        "natural flow",
        "punctuation accuracy"
      ],
      "use_cases": [
        "How to improve punctuation in transcripts?",
        "When to enable enhanced punctuation?",
        "How to get natural sentence breaks in transcription?",
        "How to improve transcript readability with punctuation?",
        "What is the punctuation_enhanced parameter?"
      ],
      "tags": [
        "pre-recorded",
        "features",
        "alpha"
      ],
      "priority": 7,
      "content": "# Enhanced Punctuation\n\n**Note:** This feature is in **Alpha**.\n- It may have restricted access in the future.\n- Breaking changes could still be introduced; however, advanced notice will be provided.\n- Results may vary as we are updating the feature.\n\nEnhanced punctuation improves both the accuracy and natural flow of punctuation in transcriptions. It ensures precise comma placement, natural sentence breaks, and better handling of quotation marks.\n\n## Example\n\n**Standard:** \"hello how are you today I am doing fine thanks\"\n\n**Enhanced:** \"Hello, how are you today? I am doing fine, thanks!\"\n\n## Usage\n\nEnable it by sending the `punctuation_enhanced` parameter:\n\n```json\n{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"punctuation_enhanced\": true\n}\n```"
    },
    {
      "title": "Meeting BaaS",
      "path": "integrations/meeting-baas",
      "url": "https://docs.gladia.io/chapters/integrations/meeting-baas",
      "keywords": [
        "Meeting BaaS",
        "Google Meet",
        "Zoom",
        "Microsoft Teams",
        "meeting recording",
        "meeting transcription",
        "unified API",
        "webhook",
        "GDPR",
        "retranscribe"
      ],
      "use_cases": [
        "How to integrate Gladia with Meeting BaaS?",
        "How to record and transcribe meetings with Meeting BaaS?",
        "How to create a bot with Gladia as STT provider?",
        "How to retranscribe existing recordings with Gladia?",
        "When to use Meeting BaaS for meeting transcription?"
      ],
      "tags": [
        "integration",
        "meeting-baas",
        "meetings",
        "recording",
        "third-party"
      ],
      "priority": 7,
      "content": "# Meeting BaaS Integration\n\n## What is Meeting BaaS?\n\nMeeting BaaS provides a unified API for Google Meet, Zoom, and Microsoft Teams so you can record and transcribe meetings through one service. By connecting Gladia STT, you leverage Gladia's accurate, feature-rich transcription for your meeting recordings without building the capture and RTC plumbing yourself.\n\nLooking for API details and options? See the Meeting BaaS docs: [docs.meetingbaas.com](https://docs.meetingbaas.com/) and the Gladia docs [docs.gladia.io](https://docs.gladia.io/).\n\n## Benefits of integration\n\n- **Single API for Multiple Platforms**: Record and transcribe meetings from Google Meet, Zoom, and Microsoft Teams through one consistent API\n- **Advanced Transcription**: Utilize Gladia's accurate and feature-rich transcription service\n- **Instant Availability**: Get video recordings and transcriptions immediately after meetings\n- **Custom Branding**: Display your user's branding with custom names and chat messages\n- **GDPR Compliance**: Meeting BaaS provides a DPA and focuses on data minimization\n\n## What you can build\n\n- Use the generated transcripts in your applications\n- Set up webhook notifications for transcription events\n- Explore LLM summaries and metadata extraction features\n- Build custom interfaces using the provided transcription data\n\n## Quickstart via API\n\nThere are two main ways to use Gladia with Meeting BaaS.\n\n### Create a bot with Gladia as the STT provider\n\nWhen creating a bot to join a meeting, specify Gladia as the speech-to-text provider:\n\n```bash\ncurl -X POST \"https://api.meetingbaas.com/bots\" \\\n  -H \"x-meeting-baas-api-key: <MEETING_BAAS_API_KEY>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n          \"meeting_url\": \"https://meet.google.com/xyz-abc\",\n          \"speech_to_text\": \"Gladia\",\n          \"webhook_url\": \"https://yourapp.com/webhook\"\n       }'\n```\n\n### Retranscribe an existing recording with Gladia\n\nFor existing recordings, use the Retranscribe Bot API to process audio with Gladia:\n\n```bash\ncurl -X POST \"https://api.meetingbaas.com/bots/retranscribe\" \\\n  -H \"x-meeting-baas-api-key: <MEETING_BAAS_API_KEY>\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"bot_uuid\": \"string\",\n    \"speech_to_text\": {\n      \"api_key\": \"<GLADIA_API_KEY>\",\n      \"provider\": \"Gladia\"\n    },\n    \"webhook_url\": \"string\"\n  }'\n```\n\n## Setup instructions\n\n### Prerequisites\n\n1. A Meeting BaaS account with an API key\n2. A Gladia account with an API key\n\n### Integration steps\n\n1. **Get your Gladia API key**\n   - [Sign up for a Gladia account](https://app.gladia.io/auth/signin) if you don't have one\n   - Navigate to your API keys section and copy your key\n\n2. **Create a Meeting BaaS bot with Gladia**\n   - Use the API endpoint `/bots` with your Meeting BaaS API key\n   - Set `\"speech_to_text\": \"Gladia\"` in your request body as shown in the example above\n\n3. **Retranscribe with Gladia**\n   - Use the `/bots/retranscribe` endpoint\n   - Provide your Gladia API key in the `speech_to_text.api_key` field\n   - Set `\"provider\": \"Gladia\"` to specify Gladia as the transcription service\n\n## Response handling\n\nUpon successful integration, Meeting BaaS will return a response with status code 200 or 202, containing details about the transcription job.\n\n## What's next?\n\nAfter integrating with Meeting BaaS, you can:\n\n- Use the generated transcripts in your applications\n- Set up webhook notifications for transcription events\n- Explore additional Meeting BaaS features like LLM summaries and metadata extraction\n- Build custom interfaces using the provided transcription data\n\n## Next steps\n\n- [Get your API key](https://app.gladia.io/apikeys) - Get your API key on the Gladia dashboard\n- [Meeting BaaS docs](https://docs.meetingbaas.com/) - Explore service options and API details\n- [Gladia docs](https://docs.gladia.io/) - Learn about STT options and parameters"
    },
    {
      "title": "Recall",
      "path": "integrations/recall",
      "url": "https://docs.gladia.io/chapters/integrations/recall",
      "keywords": [
        "Recall.ai",
        "meeting bot",
        "Zoom",
        "Google Meet",
        "Microsoft Teams",
        "Webex",
        "Slack",
        "meeting transcription",
        "real-time",
        "unified API"
      ],
      "use_cases": [
        "How to integrate Gladia with Recall.ai?",
        "How to transcribe Zoom, Google Meet, and Teams meetings?",
        "How to build meeting assistants with Gladia and Recall.ai?",
        "How to add live captions to meetings using Recall.ai?",
        "When to use Recall.ai Meeting Bot with Gladia?"
      ],
      "tags": [
        "integration",
        "recall",
        "meetings",
        "meeting-bot",
        "third-party"
      ],
      "priority": 7,
      "content": "# Recall Integration\n\n## What is Recall?\n\nRecall.ai provides a unified Meeting Bot API that can join Zoom, Google Meet, Microsoft Teams, Cisco Webex, and Slack calls, stream audio in real time, and route it to providers for transcription and intelligence. With Gladia STT connected in Recall.ai, you get accurate, low-latency transcription for your calls without building any of the bot or RTC plumbing yourself.\n\nLooking to connect Gladia in Recall.ai now? Open the Gladia page in the Recall.ai dashboard: [us-east-1.recall.ai/dashboard/transcription/gladia](https://us-east-1.recall.ai/dashboard/transcription/gladia). If you use a different region, switch the region in the URL.\n\n## What you can build\n\n- Meeting assistants that take notes, summaries, and action items\n- Live captions for meetings, webinars, and trainings\n- Contact-center analytics and QA from customer calls\n- LLM agents that join calls and understand participants in real time\n\n## Quick setup via the dashboard\n\n1. Go to the Recall.ai Gladia page: [https://us-east-1.recall.ai/dashboard/transcription/gladia](https://us-east-1.recall.ai/dashboard/transcription/gladia)\n2. Paste your Gladia API key from the Gladia dashboard: [https://app.gladia.io/apikeys](https://app.gladia.io/apikeys)\n3. Click \"Save credentials.\" Recall.ai will use your Gladia key for transcription.\n\n## Next steps\n\n- [Get your API key](https://app.gladia.io/apikeys) - Get your API key on the Gladia dashboard\n- [Connect Gladia in Recall.ai](https://us-east-1.recall.ai/dashboard/transcription/gladia) - Open the dashboard page and save your key\n- [Recall.ai website](https://www.recall.ai/partners/gladia) - Learn more about the platform and features"
    },
    {
      "title": "Gladia over vanilla Whisper?",
      "path": "introduction/why-use-gladia",
      "url": "https://docs.gladia.io/chapters/introduction/why-use-gladia",
      "keywords": [
        "whisper",
        "comparison",
        "self-hosted",
        "cloud API",
        "benefits",
        "cost",
        "security",
        "scalability",
        "maintenance"
      ],
      "use_cases": [
        "Why should I use Gladia instead of self-hosted Whisper?",
        "What are the benefits of using Gladia API?",
        "How does Gladia compare to running my own Whisper instance?",
        "When is cloud-based transcription better than self-hosted?"
      ],
      "tags": [
        "introduction",
        "comparison",
        "whisper"
      ],
      "priority": 7,
      "content": "# Running your own Whisper instance\n\nIn the dynamic world of technology, leveraging AI for voice recognition has become a key asset for many applications. Whisper presents a cutting-edge solution. However, setting up and maintaining your own Whisper instance can be fraught with challenges. In this article, we dive into why managing your own Whisper instance might not be the best approach and how Gladia API seamlessly fills this gap.\n\n## The Downside of Running Your Own Whisper Instance\n\n### Resource-Heavy Requirements\n\n- **Intensive Computing Needs**: Whisper models are like power-hungry giants, requiring substantial CPU and GPU power.\n- **Enormous Storage Space**: Prepare to allocate extensive storage for datasets and model weights, akin to building large warehouses for data.\n\n### Navigating the Maze of Technical Complexity\n\n- **Daunting Setup and Maintenance**: The complexity of configuration and ongoing maintenance can be overwhelming.\n- **Staying Ahead of the Curve**: Keeping up with the latest advancements in AI technology requires constant vigilance and learning.\n\n### Security Concerns: A Thorny Issue\n\n- **Data Privacy Risks**: Storing sensitive voice data on your own servers can be like opening a Pandora's box of privacy issues.\n- **Attack Vulnerability**: Self-hosted solutions could be more susceptible to cyber-attacks, making them a potential Achilles' heel.\n\n### Cost: The Hidden Giant\n\n- **Ongoing Operational Expenses**: The high computational demands translate into increased electricity and cooling costs.\n- **Initial Investment in Hardware**: Get ready to invest in high-end hardware, akin to buying a sports car for a daily commute.\n\n## Gladia API: The Knight in Shining Armor\n\nGladia API emerges as a cloud-based savior, addressing these challenges and offering a more streamlined, secure, and efficient alternative.\n\n### User-Friendly Integration\n\n- **Plug and Play**: With Gladia API, integrating voice recognition is as easy as plugging in a USB drive - simple and straightforward.\n- **Maintenance-Free**: Say goodbye to the hassles of updates and maintenance, as Gladia API takes care of it all.\n\n### Scalability and Cost-Effectiveness\n\n- **Pay-As-You-Go Model**: Only pay for the processing you need, avoiding the financial burden of maintaining expensive hardware.\n- **Elastic Scalability**: Whether your demand is a trickle or a flood, Gladia API adapts effortlessly, scaling to your needs.\n\n### Enhanced Security: Your Digital Fortress\n\n- **Fort-Knox-Level Data Security**: Gladia API ensures top-notch security for your data, adhering to stringent data privacy standards.\n- **Continuous Security Updates**: Stay protected against emerging threats with regular security updates.\n\n### Comprehensive Support and Documentation\n\n- **Extensive Documentation**: Dive into detailed documentation to get the most out of Gladia API.\n- **Community and Expertise at Your Fingertips**: Leverage the collective wisdom of a vast user community and expert support for any queries or challenges.\n\nIn conclusion, while running your own Whisper instance might seem appealing, the journey is fraught with hurdles. Gladia API emerges as a robust, hassle-free alternative, letting you focus on innovation rather than infrastructure."
    },
    {
      "title": "Concurrency and Rate Limits",
      "path": "limits-and-specifications/concurrency",
      "url": "https://docs.gladia.io/chapters/limits-and-specifications/concurrency",
      "keywords": [
        "concurrency",
        "rate limits",
        "API limits",
        "transcription limits",
        "usage limits",
        "free tier",
        "paid tier",
        "enterprise",
        "queue"
      ],
      "use_cases": [
        "how to check my concurrency limits",
        "what are the rate limits for Gladia API",
        "how many concurrent transcriptions can I run",
        "when to upgrade from free to paid tier",
        "how to request higher concurrency limits"
      ],
      "tags": [
        "limits",
        "specifications",
        "rate-limiting",
        "concurrency",
        "pricing"
      ],
      "priority": 7,
      "content": "# Concurrency and Rate Limits\n\nIn order to provide a smooth experience and optimal performance for all users, Gladia accounts can have up to **3** different limits for **Free**, **Paid** & **Enterprise** users:\n\n> **Need a higher concurrency limit?** The values listed below are default values. We can provide high concurrencies based on your needs, so feel free to [contact us](https://www.gladia.io/contact).\n\n| Plan type | Usage limit (per month) | Max Transcriptions in concurrency (pre-recorded) | Max Transcriptions in concurrency (Live) |\n| --- | --- | --- | --- |\n| **Enterprise** | Unlimited | On demand | On demand |\n| **Paid** | Unlimited | 25 | 30 |\n| **Free** | 10 Hours | 3 | 1 |\n\n## Limit Types\n\n- **Usage limit** (free-tier users only): This determines the number of hours a user can transcribe a given month. This is a limitation for free tier users only.\n\n- **Concurrency** (depending on free/paid tier): This refers to the maximum number of transcriptions (pre-recorded or real-time) that a user can process at the same time. For asynchronous transcriptions, Paid plan users can queue up to 300 requests, but only will still have 25 max processed concurrently.\n\n- **API level rate limit** (same for every user): This is the number of API calls that a user can make within a particular time frame. This is to ensure that a single user or malicious actor doesn't affect the performance of the API for all the other users."
    },
    {
      "title": "Data Retention",
      "path": "limits-and-specifications/data-retention",
      "url": "https://docs.gladia.io/chapters/limits-and-specifications/data-retention",
      "keywords": [
        "data retention",
        "zero data retention",
        "privacy",
        "GDPR",
        "data storage",
        "enterprise",
        "audio files",
        "transcripts",
        "metadata"
      ],
      "use_cases": [
        "how long does Gladia store my data",
        "how to enable zero data retention",
        "what happens to my audio files after transcription",
        "how to comply with GDPR using Gladia",
        "when to use zero data retention mode"
      ],
      "tags": [
        "limits",
        "specifications",
        "privacy",
        "data-retention",
        "enterprise"
      ],
      "priority": 7,
      "content": "# Data Retention\n\nTo provide transcription services, Gladia processes several types of data:\n\n- **Audio input**: Audio files or audio streams provided for transcription\n- **Transcription output**: Text, timestamps, words, utterances\n- **API Metadata**: Request IDs, timestamps, processing status\n- **Logs**: Operational logs for system reliability\n\nThe duration for which your data is stored depends on your plan type. You have two main options for data retention:\n\n- **Standard data retention**: Your data (such as audio files, transcripts, and metadata) remains accessible for a set number of days, up to a maximum allowed by your plan. The minimum retention value is 0, which means your data is deleted within 24 hours. The maximum and default value is 12 months.\n\n- **Zero data retention**: Data storage is minimized at all stages, avoiding temporary storage whenever possible. All data is deleted immediately after processing.\n\n> Only Enterprise users are eligible for custom data retention and the zero data retention option.\n\nTo enable usage tracking, Gladia retains essential API metadata: request ID, timestamp, processing status and audio duration. Immutable logs are also maintained, for a limited period, to ensure service quality and reliability.\n\n## Zero Data Retention Behavior\n\nWhen Zero Data Retention is enabled, Gladia processes data ephemerally; no data is stored at rest.\n\n- **No audio files are stored**: Files cannot be retrieved through the API or in the Gladia's playground. File upload is also disabled; the asynchronous API must use an external audio file url, such as S3 presigned url.\n\n- **No transcripts are stored**: Transcription results cannot be retrieved through the API and are not visible in the Gladia's playground.\n\n- **No metadata retrieval**: Transcription API calls, audio duration, and other metadata cannot be retrieved through the API or in the Gladia's playground.\n\n- **Transcription results delivered only via callbacks**: The only way to receive transcription results is through callbacks; they cannot be retrieved by any other means.\n\nOnce the result is delivered, the audio, transcript, and metadata cannot be accessed."
    },
    {
      "title": "Supported Files and Duration",
      "path": "limits-and-specifications/supported-formats",
      "url": "https://docs.gladia.io/chapters/limits-and-specifications/supported-formats",
      "keywords": [
        "supported formats",
        "audio formats",
        "video formats",
        "file size",
        "duration limit",
        "mp3",
        "wav",
        "youtube",
        "conversion"
      ],
      "use_cases": [
        "what audio formats does Gladia support",
        "what is the maximum file size for transcription",
        "how to transcribe YouTube videos with Gladia",
        "how to split large audio files for transcription",
        "what video formats can Gladia transcribe"
      ],
      "tags": [
        "limits",
        "specifications",
        "formats",
        "audio",
        "video"
      ],
      "priority": 7,
      "content": "# Supported Files and Duration\n\nWe support almost all types of audio or video files with a tradeoff to be taken into account between the transfer time of specific formats that can generate big files and the time to convert the original format to the target one (WAV pcm 16KHz little-endian). You can find an estimate of the conversion times in the table below.\n\n## Gladia API Current Limitations\n\nThose limits will be gradually lifted to ensure the full stability and performance of the service for everyone.\n\n- **Audio length**: The maximum length of audio that can be transcribed in a single request is currently 135 minutes. Attempts to transcribe longer audio files will result in an error. Direct YouTube links are limited to 120 minutes instead of 135 minutes.\n\n> We support up to 4h15 audio length for enterprise plans.\n\n- **File size**: Audio files must not exceed 1000 MB in size. Larger files will not be accepted by the API.\n\n### Splitting Oversize Audio Files\n\nFor audio files that are near or exceed the limitations on length and size, it is recommended to split them into smaller chunks of ~60 minutes each. This approach not only adheres to the API constraints but also generally yields better transcription results.\n\nTools for Splitting Audio Files:\n- **FFMPEG**: FFMPEG is a versatile command-line tool that can be used to manipulate audio and video files. It is a popular choice for splitting long audio files.\n- **ffmpeg-python**: For Python users, ffmpeg-python is a wrapper around FFMPEG that provides a more Pythonic interface for interacting with FFMPEG.\n- **prism-media** for Node.js: Node.js users can use prism-media for manipulating media files, including splitting audio files.\n- **fluent-ffmpeg** for Node.js: Another option for Node.js users is fluent-ffmpeg, which offers a simpler and more fluent API for handling media files.\n\n## Supported Audio Formats\n\n| Source Format | Mime Type | Audio/Video |\n| --- | --- | --- |\n| aac | audio/aac | Audio |\n| ac3 | audio/ac3 | Audio |\n| eac3 | audio/eac3 | Audio |\n| flac | audio/flac | Audio |\n| m4a | audio/mp4 | Audio |\n| mp2 | audio/mpeg | Audio |\n| mp3 | audio/mpeg | Audio |\n| ogg | application/ogg | Audio |\n| opus | audio/opus | Audio |\n| wav | audio/wav | Audio |\n\n## Supported Video Formats\n\n| Source Format | Mime Type | Audio/Video |\n| --- | --- | --- |\n| 3g2 | video/3gpp2 | Video |\n| 3gp | video/3gpp | Video |\n| avi | video/x-msvideo | Video |\n| flv | video/x-flv | Video |\n| m4v | video/x-m4v | Video |\n| matroska | video/x-matroska | Audio/Video |\n| mov | video/quicktime | Video |\n| mp4 | video/mp4 | Audio/Video |\n| wmv | video/x-ms-wmv | Video |\n\n## Supported Online Video Services\n\n| Platform | Audio/Video Support | Stage |\n| --- | --- | --- |\n| YouTube | Video | Released |\n| TikTok | Video | Released |\n| Instagram | Video | Released |\n| Facebook | Video | Released |\n| Vimeo | Video | Released |\n| Dailymotion | Video | Released |\n| LinkedIn | Video | Released |\n| Sharechat | Video | Released |\n| Likee | Video | Released |\n| TikTok (Beta) | Video | Beta |\n| Twitter (Beta) | Video | Beta |\n\n## Conversion Time\n\n| Source Format | Mime Type | Audio/Video | Estimated File Size (1 Hour) | Estimated Conversion Time (1 Hour) |\n| --- | --- | --- | --- | --- |\n| 3g2 | video/3gpp2 | Video | ~300 MB | ~30 seconds |\n| 3gp | video/3gpp | Video | ~300 MB | ~40 seconds |\n| aac | audio/aac | Audio | ~60 MB | ~36 seconds |\n| ac3 | audio/ac3 | Audio | ~215 MB | ~42 seconds |\n| avi | video/x-msvideo | Video | ~800 MB | ~1 minute |\n| eac3 | audio/eac3 | Audio | ~215 MB | ~32 seconds |\n| flac | audio/flac | Audio | ~260 MB | ~46 seconds |\n| flv | video/x-flv | Video | ~400 MB | ~40 seconds |\n| m4a | audio/m4a | Audio | ~60 MB | ~26 seconds |\n| x-m4a | audio/x-m4a | Audio | ~60 MB | ~26 seconds |\n| m4v | video/x-m4v | Video | ~800 MB | ~1 minute |\n| matroska | video/x-matroska | Audio/Video | ~800 MB | ~1 minute |\n| mov | video/quicktime | Video | ~800 MB | ~1 minute |\n| mp2 | audio/mpeg | Audio | ~120 MB | ~42 seconds |\n| mp3 | audio/mpeg | Audio | ~120 MB | ~37 seconds |\n| mp4 | video/mp4 | Audio/Video | ~800 MB | ~1 minute |\n| ogg | application/ogg | Audio | ~60 MB | ~1 minute |\n| opus | audio/opus | Audio | ~30 MB | ~1 minute |\n| wav | audio/wav | Audio | ~510 MB | N/A |\n| wmv | video/x-ms-wmv | Video | ~800 MB | ~1 minute |"
    },
    {
      "title": "Live Transcription - Migration from V1 to V2",
      "path": "migration-from-v1",
      "url": "https://docs.gladia.io/chapters/live-stt/migration-from-v1",
      "keywords": [
        "migration",
        "V1 to V2",
        "upgrade",
        "API changes",
        "configuration",
        "breaking changes",
        "websocket",
        "live transcription"
      ],
      "use_cases": [
        "how to migrate from Gladia V1 to V2 live API",
        "what changed between V1 and V2 configuration",
        "how to update WebSocket connection for V2",
        "when to upgrade to V2 live transcription"
      ],
      "tags": [
        "live",
        "realtime",
        "streaming",
        "stt",
        "migration",
        "v2"
      ],
      "priority": 7,
      "content": "# Live Transcription - Migration Guide from V1 to V2\n\nLive transcription V2 is the latest real-time speech-to-text API from Gladia. It offers more features and has significant improvements in latency compared to V1. Please migrate sooner rather than later as support for V1 will be removed in the future.\n\n## Initiating the connection to the WebSocket\n\nIn V1, you always connect to the same WebSocket URL (`wss://api.gladia.io/audio/text/audio-transcription`) and send your configuration through the WebSocket connection.\n\nIn V2, you first generate a unique WebSocket URL with a call to `POST /v2/live` endpoint, and then connect to it. This URL contains a token that is unique to your live session. You'll be able to resume your session in case of a lost connection, or give the URL to a web client without exposing your Gladia API key.\n\n### V1\n```javascript\nimport WebSocket from 'ws';\n\nconst socket = new WebSocket('wss://api.gladia.io/audio/text/audio-transcription');\n\nsocket.addEventListener(\"open\", function() {\n  // Send configuration\n  socket.send(JSON.stringify({\n    'x_gladia_key': 'YOUR_GLADIA_API_KEY',\n    // ...config properties\n  }))\n\n  // Start sending audio chunks\n});\n```\n\n### V2\n```javascript\nimport WebSocket from 'ws';\n\nconst response = await fetch('https://api.gladia.io/v2/live', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'x-gladia-key': '<YOUR_GLADIA_API_KEY>',\n  },\n  body: JSON.stringify({\n    // ...config properties\n  }),\n});\n\nconst {url} = await response.json();\nconst socket = new WebSocket(url);\n\nsocket.addEventListener(\"open\", function() {\n  // Start sending audio chunks\n});\n```\n\n## Configuration\n\nWith V2 offering more features, the configuration comes with some changes.\n\n### Audio encoding\n\n`encoding`, `bit_depth` and `sample_rate` are still present in V2, but with less options for now.\n\n- `wav` is the same as `wav/pcm`, V2 defaults to `wav/pcm`\n- `amb`, `mp3`, `flac`, `ogg/vorbis`, `opus`, `sphere` and `amr-nb` are no longer supported\n- `bit_depth` option `64` is no longer supported\n\nIf you're using an unsupported `encoding` or `bit_depth`, please contact Gladia.\n\n### Model\n\nOnly one model is supported in V2 for now, so omit the property `model`.\n\n### End-pointing and maximum audio duration\n\n`endpointing` is now declared in seconds instead of milliseconds.\n`maximum_audio_duration` has been renamed to `maximum_duration_without_endpointing`.\n\n**V1:**\n```json\n{\n  \"endpointing\": 800,\n  \"maximum_audio_duration\": 10\n}\n```\n\n**V2:**\n```json\n{\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 10\n}\n```\n\n### Language\n\n#### Automatic single language\n\nAutomatic single language behavior is the default in both V1 and V2, so you can just omit those parameters.\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"automatic single language\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [],\n    \"code_switching\": false\n  }\n}\n```\n\n#### Automatic multiple languages\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"automatic multiple languages\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [], // You can now specify the expected languages in V2 as guidance\n    \"code_switching\": true\n  }\n}\n```\n\n#### Manual\n\nLanguages are now specified with a 2-letter code.\n\n**V1:**\n```json\n{\n  \"language_behaviour\": \"manual\",\n  \"language\": \"english\"\n}\n```\n\n**V2:**\n```json\n{\n  \"language_config\": {\n    \"languages\": [\"en\"],\n    \"code_switching\": false\n  }\n}\n```\n\n### Frames format\n\nYou can send audio chunk as bytes or base64 and we'll detect the format automatically. The parameter `frames_format` is no longer present.\n\n### Audio enhancer\n\n`audio_enhancer` has been moved into the `pre_processing` object.\n\n**V1:**\n```json\n{\n  \"audio_enhancer\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"pre_processing\": {\n    \"audio_enhancer\": true\n  }\n}\n```\n\n### Word timestamps\n\n`word_timestamps` has been renamed to `words_accurate_timestamps` and moved into the `realtime_processing` object.\n\n**V1:**\n```json\n{\n  \"word_timestamps\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"realtime_processing\": {\n    \"words_accurate_timestamps\": true\n  }\n}\n```\n\n### Other properties\n\n`prosody`, `reinject_context` and `transcription_hint` are not supported for now. They may return in another form in the future.\n\n### Full config migration sample\n\n**V1:**\n```json\n{\n  \"encoding\": \"wav\",\n  \"bit_depth\": 8,\n  \"sample_rate\": 48000,\n  \"model\": \"accurate\",\n  \"endpointing\": 800,\n  \"maximum_audio_duration\": 10,\n  \"language_behaviour\": \"manual\",\n  \"language\": \"english\",\n  \"audio_enhancer\": true,\n  \"word_timestamps\": true\n}\n```\n\n**V2:**\n```json\n{\n  \"encoding\": \"wav/pcm\",\n  \"bit_depth\": 8,\n  \"sample_rate\": 48000,\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 10,\n  \"language_config\": {\n    \"languages\": [\"en\"]\n  },\n  \"pre_processing\": {\n    \"audio_enhancer\": true\n  },\n  \"realtime_processing\": {\n    \"words_accurate_timestamps\": true\n  }\n}\n```\n\n## Send audio chunks\n\nIf you were sending chunks as bytes, nothing has changed. If you were sending them as base64, the format of the JSON messages changed in V2.\n\n**V1:**\n```json\n{\n  \"frames\": \"<base64 encoded>\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"audio_chunk\",\n  \"data\": {\n    \"chunk\": \"<base64 encoded>\"\n  }\n}\n```\n\n## Transcription message\n\nIn V1, we only send two kinds of messages through WebSocket: the \"connected\" message and the \"transcript\" messages.\n\nIn V2, we send more: lifecycle event messages, acknowledgment messages, add-on messages, post-processing messages, etc.\n\nTo read a transcription message in V1, you verify that the `type` field is `\"final\"` and/or the `transcription` field is not empty.\n\nIn V2, you should confirm that the `type` field is `transcript` and that `data.is_final` is `true`.\n\n**V1:**\n```json\n{\n  \"event\": \"transcript\",\n  \"request_id\": \"G-3abade39\",\n  \"type\": \"final\",\n  \"transcription\": \" Hello world\",\n  \"time_begin\": 1.4376875,\n  \"time_end\": 2.4696875,\n  \"confidence\": 0.65,\n  \"language\": \"en\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"transcript\",\n  \"session_id\": \"de0a341d-c69f-4e15-a649-7b3f49e211f0\",\n  \"created_at\": \"2024-10-10T14:35:28.387Z\",\n  \"data\": {\n    \"id\": \"00_00000000\",\n    \"is_final\": true,\n    \"utterance\": {\n      \"text\": \" Hello world\",\n      \"start\": 0.188,\n      \"end\": 1.284,\n      \"language\": \"en\",\n      \"confidence\": 1,\n      \"channel\": 0,\n      \"words\": [\n        { \"word\": \" Hello\", \"start\": 0.188, \"end\": 0.735, \"confidence\": 1 },\n        { \"word\": \" world\", \"start\": 0.736, \"end\": 1.284, \"confidence\": 1 }\n      ]\n    }\n  }\n}\n```\n\nIf you're not interested in new messages and simply want the ones from the V1 API, you can configure what messages you receive:\n\n```json\n{\n  \"messages_config\": {\n    \"receive_partial_transcripts\": false,\n    \"receive_final_transcripts\": true,\n    \"receive_speech_events\": false,\n    \"receive_pre_processing_events\": false,\n    \"receive_realtime_processing_events\": false,\n    \"receive_post_processing_events\": false,\n    \"receive_acknowledgments\": false,\n    \"receive_lifecycle_events\": false\n  }\n}\n```\n\n## End the live session\n\n**V1:**\n```json\n{\n  \"event\": \"terminate\"\n}\n```\n\n**V2:**\n```json\n{\n  \"type\": \"stop_recording\"\n}\n```"
    },
    {
      "title": "Multiple Channels",
      "path": "multiple-channels",
      "url": "https://docs.gladia.io/chapters/live-stt/features/multiple-channels",
      "keywords": [
        "multiple channels",
        "multi-channel",
        "stereo audio",
        "channel separation",
        "audio tracks",
        "channel key",
        "WebSocket",
        "billing per channel"
      ],
      "use_cases": [
        "How to transcribe multi-channel audio streams?",
        "How to identify which channel an utterance came from?",
        "How to merge multiple audio tracks into one WebSocket?",
        "How is multi-channel audio billed?"
      ],
      "tags": [
        "live",
        "realtime",
        "features",
        "channels",
        "audio"
      ],
      "priority": 7,
      "content": "# Multiple Channels\n\nIf you have multiple channels in your audio stream, specify the count in the configuration:\n\n```json\n{\n  \"channels\": 2\n}\n```\n\n## How It Works\n\nGladia's real-time API will automatically split the channels and transcribe them separately. For each utterance, you'll get a `channel` key corresponding to the channel the utterance came from.\n\n## Billing\n\nTranscribing an audio stream with multiple channels is billed per channel. For example, an audio stream with 2 channels will be billed as double the audio duration, even if the channels are identical.\n\n## Merging Multiple Audio Tracks\n\nFor a detailed guide on how to merge multiple audio tracks into a single multi-channel stream and send it over a WebSocket, see the [Sending multiple audio tracks over a single WebSocket](https://docs.gladia.io/chapters/live-stt/quickstart#merging-multiple-audio-tracks-into-one-multi-channel-websocket) section."
    },
    {
      "title": "Dual or Multiple Channels",
      "path": "multiple-channels",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/multiple-channels",
      "keywords": [
        "multiple channels",
        "dual channel",
        "multi-channel audio",
        "channel transcription",
        "stereo audio",
        "channel key",
        "utterance channel",
        "audio channels"
      ],
      "use_cases": [
        "How to transcribe multi-channel audio files?",
        "How does billing work for multiple channels?",
        "How to identify which channel an utterance came from?",
        "When to use multiple channel transcription?",
        "How are duplicate channels handled?"
      ],
      "tags": [
        "pre-recorded",
        "features"
      ],
      "priority": 7,
      "content": "# Dual or Multiple Channels\n\nIf your audio file has multiple distinct channels, Gladia will transcribe them automatically. In the result, each utterance includes a `channel` key corresponding to the source channel.\n\n## Billing\n\nSending an audio with two different channels (with different content) will be billed as two audios. If your audio has multiple channels with the same content, it will only be billed once.\n\n**TL;DR:** We charge every unique channel in an audio file; we do not charge if channels are duplicates."
    },
    {
      "title": "Name Consistency",
      "path": "name-consistency",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/features/name-consistency",
      "keywords": [
        "name consistency",
        "name_consistency",
        "consistent spelling",
        "name recognition",
        "proper names",
        "spelling normalization",
        "recruitment calls",
        "name handling"
      ],
      "use_cases": [
        "How to ensure consistent name spelling in transcripts?",
        "When to use name consistency feature?",
        "How to handle unknown names in transcription?",
        "What is the difference between name consistency and custom vocabulary?",
        "How to transcribe recruitment calls with name consistency?"
      ],
      "tags": [
        "pre-recorded",
        "features"
      ],
      "priority": 7,
      "content": "# Name Consistency\n\nYou can ask the model to enforce consistent spelling of names using the `name_consistency` parameter. This ensures the same name is spelled the same way throughout the transcript, at the cost of a small amount of added processing time.\n\nThis is especially useful where people's names may be mentioned multiple times but are not known in advance (e.g., recruitment call recordings).\n\n**Note:** To ensure correct spelling of names known in advance, use [custom vocabulary](https://docs.gladia.io/chapters/pre-recorded-stt/features/custom-vocabulary).\n\n## Usage\n\n```json\n{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"name_consistency\": true\n}\n```"
    },
    {
      "title": "Pre-recorded Migration from V1 to V2",
      "path": "pre-recorded-stt/migration-from-v1",
      "url": "https://docs.gladia.io/chapters/pre-recorded-stt/migration-from-v1",
      "keywords": [
        "migration",
        "v1",
        "v2",
        "upgrade",
        "api changes",
        "breaking changes",
        "input changes",
        "output changes",
        "deprecation",
        "pre-recorded"
      ],
      "use_cases": [
        "how to migrate from Gladia V1 to V2 API",
        "what changed between V1 and V2 pre-recorded API",
        "how to update transcription parameters for V2",
        "when to use the new callback vs webhook system"
      ],
      "tags": [
        "pre-recorded",
        "stt",
        "speech-to-text",
        "migration",
        "v2"
      ],
      "priority": 7,
      "content": "## General flow changes\n\nIn the first version of Gladia API, to get your transcription through an HTTP call, you had to send everything (audio file/url, parameters, etc) in a single HTTP call, and then keep the connection open until you get your result. This was not ideal for many scenarios that could lead to longer waiting time to get your results, or in case of connection errors, not getting your results at all despite the transcription being successful.\n\nIn V2, we addressed this by decomposing the process in multiple steps, and have merged both audio & video endpoints:\n\n### 1. Upload your file\n\nThis step is **optional** if you are already working with **audio URLs**.\n\nIf you're working with audio or video files, you'll need to upload it first using our `/upload` endpoint with `multipart/form-data` content-type since Gladia `/v2/pre-recorded` endpoint only accept audio URLs now.\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/upload \\\n  --header 'Content-Type: multipart/form-data' \\\n  --header 'x-gladia-key: YOUR_GLADIA_API_TOKEN' \\\n  --form audio=@/path/to/your/audio/conversation.wav\n```\n\nExample response:\n\n```json\n{\n  \"audio_url\": \"https://api.gladia.io/file/636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n  \"audio_metadata\": {\n    \"id\": \"636c70f6-92c1-4026-a8b6-0dfe3ecf826f\",\n    \"filename\": \"conversation.wav\",\n    \"extension\": \"wav\",\n    \"size\": 99515383,\n    \"audio_duration\": 4146.468542,\n    \"number_of_channels\": 2\n  }\n}\n```\n\n### 2. Transcribe\n\nWe'll now make the transcription request to Gladia's API. Instead of `/audio/text/audio-transcription` now we'll use `/v2/pre-recorded`.\n\nSince `/v2/pre-recorded` does not accept any `audio` file, `Content-Type` is not `multipart/form-data` anymore, but `application/json`.\n\n**V2 Example:**\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/pre-recorded \\\n  --header 'Content-Type: application/json' \\\n  --header 'x-gladia-key: YOUR_GLADIA_API_TOKEN' \\\n  --data '{\n  \"audio_url\": \"YOUR_AUDIO_URL\",\n  \"diarization\": true,\n  \"diarization_config\": {\n    \"number_of_speakers\": 3,\n    \"min_speakers\": 1,\n    \"max_speakers\": 5\n  },\n  \"subtitles\": true,\n  \"subtitles_config\": {\n    \"formats\": [\"srt\", \"vtt\"]\n  },\n  \"translation\": true,\n  \"translation_config\": {\n    \"target_languages\": [\"fr\"]\n  }\n}'\n```\n\n- **Old V1**: The HTTP connection is kept opened until you get your transcription result, and there's no third step.\n- **New V2**: You get an instant response from the request with an `id` and a `result_url`.\n\nThe `id` is your transcription ID that you will use to get your transcription result once it's done. You don't have to keep any HTTP connection open on your side.\n\n### 3. Get the transcription result\n\nAs on V1 you get the transcription results in the previous step, this step is only relevant for V2. You can get your transcription results via:\n\n- **Polling**: GET continuously on the given `result_url` until the status is `done`\n- **Webhook**: Configure webhooks at https://app.gladia.io/webhooks\n- **Callback URL**: Use the `callback` feature to receive the result to a specified endpoint\n\n## Transcription Input & Output changes\n\n### Input changes\n\n| V1 | V2 |\n| --- | --- |\n| `toggle_diarization` | `diarization` |\n| `language_behaviour` | `detect_language`, `enable_code_switching`, `language` |\n| `output_format` | `subtitles` + `subtitles_config` |\n| `webhook_url` | `callback_url` |\n\n### Output changes\n\nThe output structure has changed significantly:\n\n- `prediction` is now `result.transcription.utterances`\n- `time_begin` is now `start`\n- `time_end` is now `end`\n- `transcription` (text) is now `text`\n- `channel` is now an integer instead of string (e.g., `0` instead of `\"channel_0\"`)\n- `prediction_raw` no longer exists\n- Metadata structure simplified under `result.metadata`"
    },
    {
      "title": "Sentiment Analysis Callback",
      "path": "api-reference/v2/live/callback/sentiment-analysis",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/sentiment-analysis",
      "keywords": [
        "sentiment analysis",
        "live.sentiment_analysis",
        "emotion detection",
        "websocket callback",
        "realtime sentiment",
        "utterance analysis",
        "callback event",
        "live transcription"
      ],
      "use_cases": [
        "how to receive sentiment analysis results in real-time",
        "when to use live.sentiment_analysis callback event",
        "how to handle sentiment and emotion data from WebSocket",
        "what fields are included in sentiment analysis callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime-messages",
        "sentiment-analysis"
      ],
      "priority": 6,
      "content": "# Sentiment Analysis Callback\n\nPayload definition for the callback event `live.sentiment_analysis`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.sentiment_analysis\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"error\": null,\n    \"type\": \"sentiment_analysis\",\n    \"data\": {\n      \"utterance_id\": \"00-00000011\",\n      \"utterance\": {\n        \"start\": 123,\n        \"end\": 123,\n        \"confidence\": 123,\n        \"channel\": 1,\n        \"words\": [\n          {\n            \"word\": \"<string>\",\n            \"start\": 123,\n            \"end\": 123,\n            \"confidence\": 123\n          }\n        ],\n        \"text\": \"<string>\",\n        \"language\": \"en\",\n        \"speaker\": 1\n      },\n      \"results\": [\n        {\n          \"sentiment\": \"<string>\",\n          \"emotion\": \"<string>\",\n          \"text\": \"<string>\",\n          \"start\": 123,\n          \"end\": 123,\n          \"channel\": 123\n        }\n      ]\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.sentiment_analysis`\n- Required: yes\n- Available options: `live.sentiment_analysis`\n- Example: `\"live.sentiment_analysis\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket"
    },
    {
      "title": "Speech End Callback",
      "path": "api-reference/v2/live/callback/speech-end",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/speech-end",
      "keywords": [
        "speech end",
        "live.speech_end",
        "websocket callback",
        "voice activity detection",
        "VAD",
        "speech detection",
        "callback event",
        "realtime messages"
      ],
      "use_cases": [
        "how to detect when speech ends in real-time",
        "when to use live.speech_end callback event",
        "how to handle speech end detection from WebSocket",
        "what fields are included in speech end callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime-messages",
        "speech-detection"
      ],
      "priority": 6,
      "content": "# Speech End Callback\n\nPayload definition for the callback event `live.speech_end`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.speech_end\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"speech_end\",\n    \"data\": {\n      \"time\": 12.56,\n      \"channel\": 1\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.speech_end`\n- Required: yes\n- Available options: `live.speech_end`\n- Example: `\"live.speech_end\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, type, and data with time and channel information"
    },
    {
      "title": "Speech Start Callback",
      "path": "api-reference/v2/live/callback/speech-start",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/speech-start",
      "keywords": [
        "speech start",
        "live.speech_start",
        "websocket callback",
        "voice activity detection",
        "VAD",
        "speech detection",
        "callback event",
        "realtime messages"
      ],
      "use_cases": [
        "how to detect when speech starts in real-time",
        "when to use live.speech_start callback event",
        "how to handle speech start detection from WebSocket",
        "what fields are included in speech start callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime-messages",
        "speech-detection"
      ],
      "priority": 6,
      "content": "# Speech Start Callback\n\nPayload definition for the callback event `live.speech_start`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.speech_start\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"speech_start\",\n    \"data\": {\n      \"time\": 12.56,\n      \"channel\": 1\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.speech_start`\n- Required: yes\n- Available options: `live.speech_start`\n- Example: `\"live.speech_start\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, type, and data with time and channel information"
    },
    {
      "title": "Start Recording Callback",
      "path": "api-reference/v2/live/callback/start-recording",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/start-recording",
      "keywords": [
        "start recording",
        "live.start_recording",
        "websocket callback",
        "lifecycle message",
        "recording start",
        "callback event",
        "session lifecycle"
      ],
      "use_cases": [
        "how to detect when recording starts",
        "when to use live.start_recording callback event",
        "how to handle recording lifecycle events from WebSocket",
        "what fields are included in start recording callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "lifecycle-messages",
        "recording"
      ],
      "priority": 6,
      "content": "# Start Recording Callback\n\nPayload definition for the callback event `live.start_recording`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.start_recording\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"start_recording\"\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.start_recording`\n- Required: yes\n- Available options: `live.start_recording`\n- Example: `\"live.start_recording\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, and type"
    },
    {
      "title": "Start Session Callback",
      "path": "api-reference/v2/live/callback/start-session",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/start-session",
      "keywords": [
        "start session",
        "live.start_session",
        "websocket callback",
        "lifecycle message",
        "session start",
        "callback event",
        "session lifecycle"
      ],
      "use_cases": [
        "how to detect when a session starts",
        "when to use live.start_session callback event",
        "how to handle session lifecycle events from WebSocket",
        "what fields are included in start session callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "lifecycle-messages",
        "session"
      ],
      "priority": 6,
      "content": "# Start Session Callback\n\nPayload definition for the callback event `live.start_session`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.start_session\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"start_session\"\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.start_session`\n- Required: yes\n- Available options: `live.start_session`\n- Example: `\"live.start_session\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, and type"
    },
    {
      "title": "Stop Recording Acknowledge (ack) Callback",
      "path": "api-reference/v2/live/callback/stop-recording-ack",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/stop-recording-ack",
      "keywords": [
        "stop recording",
        "live.stop_recording",
        "websocket callback",
        "acknowledgment message",
        "recording stop",
        "callback event",
        "ack",
        "recording duration"
      ],
      "use_cases": [
        "how to receive stop recording acknowledgment",
        "when to use live.stop_recording callback event",
        "how to handle recording stop acknowledgment from WebSocket",
        "what fields are included in stop recording ack callback payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "acknowledgment-messages",
        "recording"
      ],
      "priority": 6,
      "content": "# Stop Recording Acknowledge (ack) Callback\n\nPayload definition for the `stop_recording` acknowledgment.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.stop_recording\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"acknowledged\": true,\n    \"error\": null,\n    \"type\": \"stop_recording\",\n    \"data\": {\n      \"recording_duration\": 344.45,\n      \"recording_left_to_process\": 11.23\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.stop_recording`\n- Required: yes\n- Available options: `live.stop_recording`\n- Example: `\"live.stop_recording\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, acknowledged, error, type, and data with recording_duration and recording_left_to_process"
    },
    {
      "title": "Transcript Callback",
      "path": "api-reference/v2/live/callback/transcript",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/transcript",
      "keywords": [
        "transcript",
        "live.transcript",
        "websocket callback",
        "realtime transcription",
        "utterance",
        "is_final",
        "callback event",
        "speech-to-text"
      ],
      "use_cases": [
        "how to receive transcription results in real-time",
        "when to use live.transcript callback event",
        "how to handle transcript data from WebSocket",
        "what fields are included in transcript callback payload",
        "how to differentiate final vs interim transcripts"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime-messages",
        "transcription"
      ],
      "priority": 6,
      "content": "# Transcript Callback\n\nPayload definition for the callback event `live.transcript`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.transcript\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"transcript\",\n    \"data\": {\n      \"id\": \"00-00000011\",\n      \"is_final\": true,\n      \"utterance\": {\n        \"start\": 123,\n        \"end\": 123,\n        \"confidence\": 123,\n        \"channel\": 1,\n        \"words\": [\n          {\n            \"word\": \"<string>\",\n            \"start\": 123,\n            \"end\": 123,\n            \"confidence\": 123\n          }\n        ],\n        \"text\": \"<string>\",\n        \"language\": \"en\",\n        \"speaker\": 1\n      }\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.transcript`\n- Required: yes\n- Available options: `live.transcript`\n- Example: `\"live.transcript\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, type, and data with id, is_final flag, and utterance details including words, text, language, and speaker"
    },
    {
      "title": "Translation Callback",
      "path": "api-reference/v2/live/callback/translation",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/translation",
      "keywords": [
        "translation",
        "live.translation",
        "websocket callback",
        "realtime translation",
        "utterance",
        "target language",
        "original language",
        "callback event"
      ],
      "use_cases": [
        "how to receive translation results in real-time",
        "when to use live.translation callback event",
        "how to handle translation data from WebSocket",
        "what fields are included in translation callback payload",
        "how to get original and translated utterances"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime-messages",
        "translation"
      ],
      "priority": 6,
      "content": "# Translation Callback\n\nPayload definition for the callback event `live.translation`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.translation\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"error\": null,\n    \"type\": \"translation\",\n    \"data\": {\n      \"utterance_id\": \"00-00000011\",\n      \"utterance\": {\n        \"start\": 123,\n        \"end\": 123,\n        \"confidence\": 123,\n        \"channel\": 1,\n        \"words\": [\n          {\n            \"word\": \"<string>\",\n            \"start\": 123,\n            \"end\": 123,\n            \"confidence\": 123\n          }\n        ],\n        \"text\": \"<string>\",\n        \"language\": \"en\",\n        \"speaker\": 1\n      },\n      \"original_language\": \"af\",\n      \"target_language\": \"af\",\n      \"translated_utterance\": {\n        \"start\": 123,\n        \"end\": 123,\n        \"confidence\": 123,\n        \"channel\": 1,\n        \"words\": [\n          {\n            \"word\": \"<string>\",\n            \"start\": 123,\n            \"end\": 123,\n            \"confidence\": 123\n          }\n        ],\n        \"text\": \"<string>\",\n        \"language\": \"en\",\n        \"speaker\": 1\n      }\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- Type: `string<uuid>`\n- Required: yes\n- Description: Id of the job\n- Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- Type: `enum<string>`\n- Default: `live.translation`\n- Required: yes\n- Available options: `live.translation`\n- Example: `\"live.translation\"`\n\n### payload\n- Type: `object`\n- Required: yes\n- Description: The live message payload as sent to the WebSocket\n- Contains: session_id, created_at, error, type, and data with utterance_id, original utterance, original_language, target_language, and translated_utterance"
    },
    {
      "title": "End Recording Webhook",
      "path": "api-reference/v2/live/webhook/end-recording",
      "url": "https://docs.gladia.io/api-reference/v2/live/webhook/end-recording",
      "keywords": [
        "webhook",
        "end recording",
        "live.end_recording",
        "live transcription",
        "recording complete",
        "stop recording",
        "notification",
        "payload",
        "event"
      ],
      "use_cases": [
        "How to receive notifications when recording ends",
        "How to handle the live.end_recording webhook event",
        "When to use the end recording webhook for cleanup",
        "How to parse the end recording webhook payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "webhook",
        "recording",
        "event"
      ],
      "priority": 6,
      "content": "# End Recording Webhook\n\nPayload definition for the webhook event `live.end_recording`.\n\n## Example Payload\n\n```json\n{\n  \"event\": \"live.end_recording\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Schema\n\n### event\n\n- **Type:** `enum<string>`\n- **Default:** `live.end_recording`\n- **Required:** Yes\n- **Available options:** `live.end_recording`\n- **Example:** `\"live.end_recording\"`\n\n### payload\n\n- **Type:** `object`\n- **Required:** Yes\n\n#### Payload Attributes\n\n- **id** - The unique identifier of the live transcription session"
    },
    {
      "title": "End Session Webhook",
      "path": "api-reference/v2/live/webhook/end-session",
      "url": "https://docs.gladia.io/api-reference/v2/live/webhook/end-session",
      "keywords": [
        "webhook",
        "end session",
        "live.end_session",
        "live transcription",
        "session complete",
        "close session",
        "notification",
        "payload",
        "event"
      ],
      "use_cases": [
        "How to receive notifications when a live session ends",
        "How to handle the live.end_session webhook event",
        "When to use the end session webhook for cleanup",
        "How to parse the end session webhook payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "webhook",
        "session",
        "event"
      ],
      "priority": 6,
      "content": "# End Session Webhook\n\nPayload definition for the webhook event `live.end_session`.\n\n## Example Payload\n\n```json\n{\n  \"event\": \"live.end_session\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Schema\n\n### event\n\n- **Type:** `enum<string>`\n- **Default:** `live.end_session`\n- **Required:** Yes\n- **Available options:** `live.end_session`\n- **Example:** `\"live.end_session\"`\n\n### payload\n\n- **Type:** `object`\n- **Required:** Yes\n\n#### Payload Attributes\n\n- **id** - The unique identifier of the live transcription session"
    },
    {
      "title": "Start Recording Webhook",
      "path": "api-reference/v2/live/webhook/start-recording",
      "url": "https://docs.gladia.io/api-reference/v2/live/webhook/start-recording",
      "keywords": [
        "webhook",
        "start recording",
        "live.start_recording",
        "live transcription",
        "recording event",
        "real-time",
        "audio recording",
        "notification",
        "payload"
      ],
      "use_cases": [
        "How to receive notifications when recording starts",
        "How to handle the live.start_recording webhook event",
        "When to use the start recording webhook for tracking",
        "How to parse the start recording webhook payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "webhook",
        "recording",
        "event"
      ],
      "priority": 6,
      "content": "# Start Recording Webhook\n\nPayload definition for the webhook event `live.start_recording`.\n\n## Example Payload\n\n```json\n{\n  \"event\": \"live.start_recording\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Schema\n\n### event\n\n- **Type:** `enum<string>`\n- **Default:** `live.start_recording`\n- **Required:** Yes\n- **Available options:** `live.start_recording`\n- **Example:** `\"live.start_recording\"`\n\n### payload\n\n- **Type:** `object`\n- **Required:** Yes\n\n#### Payload Attributes\n\n- **id** - The unique identifier of the live transcription session"
    },
    {
      "title": "Start Session Webhook",
      "path": "api-reference/v2/live/webhook/start-session",
      "url": "https://docs.gladia.io/api-reference/v2/live/webhook/start-session",
      "keywords": [
        "webhook",
        "start session",
        "live.start_session",
        "live transcription",
        "session event",
        "real-time",
        "websocket",
        "notification",
        "payload"
      ],
      "use_cases": [
        "How to receive notifications when a live session starts",
        "How to handle the live.start_session webhook event",
        "When to use the start session webhook for tracking",
        "How to parse the start session webhook payload"
      ],
      "tags": [
        "api-reference",
        "live",
        "webhook",
        "session",
        "event"
      ],
      "priority": 6,
      "content": "# Start Session Webhook\n\nPayload definition for the webhook event `live.start_session`.\n\n## Example Payload\n\n```json\n{\n  \"event\": \"live.start_session\",\n  \"payload\": {\n    \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\"\n  }\n}\n```\n\n## Schema\n\n### event\n\n- **Type:** `enum<string>`\n- **Default:** `live.start_session`\n- **Required:** Yes\n- **Available options:** `live.start_session`\n- **Example:** `\"live.start_session\"`\n\n### payload\n\n- **Type:** `object`\n- **Required:** Yes\n\n#### Payload Attributes\n\n- **id** - The unique identifier of the live transcription session"
    },
    {
      "title": "Audio Chunk Acknowledge (ack)",
      "path": "audio-chunk-ack",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/audio-chunk-ack",
      "keywords": [
        "audio chunk",
        "acknowledgment",
        "ack",
        "websocket",
        "live streaming",
        "byte range",
        "time range",
        "callback",
        "real-time"
      ],
      "use_cases": [
        "how to confirm audio chunk was received",
        "when to handle audio_chunk acknowledgment events",
        "how to track audio byte and time ranges",
        "how to verify audio streaming is working correctly",
        "when audio chunk acknowledgment fails"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "acknowledgment",
        "websocket"
      ],
      "priority": 6,
      "content": "# Audio Chunk Acknowledge (ack)\n\nPayload definition for the `audio_chunk` acknowledgment.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.audio_chunk\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"acknowledged\": true,\n    \"error\": null,\n    \"type\": \"audio_chunk\",\n    \"data\": {\n      \"byte_range\": [1024, 2048],\n      \"time_range\": [0.8, 0.9]\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.audio_chunk`\n- **Available options:** `live.audio_chunk`\n- **Example:** `\"live.audio_chunk\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket"
    },
    {
      "title": "End Recording",
      "path": "end-recording",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/end-recording",
      "keywords": [
        "end recording",
        "stop recording",
        "recording duration",
        "lifecycle",
        "websocket",
        "live session",
        "callback",
        "recording complete"
      ],
      "use_cases": [
        "how to detect when recording has ended",
        "when recording stops in a live session",
        "how to get recording duration",
        "how to handle end_recording callback event",
        "when to finalize recording data"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "lifecycle",
        "recording"
      ],
      "priority": 6,
      "content": "# End Recording\n\nPayload definition for the callback event `live.end_recording`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.end_recording\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"end_recording\",\n    \"data\": {\n      \"recording_duration\": 344.45\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.end_recording`\n- **Available options:** `live.end_recording`\n- **Example:** `\"live.end_recording\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, type, data with recording_duration"
    },
    {
      "title": "End Session",
      "path": "end-session",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/end-session",
      "keywords": [
        "end session",
        "session complete",
        "lifecycle",
        "websocket",
        "live session",
        "callback",
        "session termination",
        "close session"
      ],
      "use_cases": [
        "how to detect when a session has ended",
        "when live session terminates",
        "how to handle end_session callback event",
        "how to cleanup after session ends",
        "when to close websocket connection"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "lifecycle",
        "session"
      ],
      "priority": 6,
      "content": "# End Session\n\nPayload definition for the callback event `live.end_session`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.end_session\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"end_session\"\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.end_session`\n- **Available options:** `live.end_session`\n- **Example:** `\"live.end_session\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, type"
    },
    {
      "title": "Migration from AssemblyAI to Gladia",
      "path": "migrations/from-assembly",
      "url": "https://docs.gladia.io/chapters/migrations/from-assembly",
      "keywords": [
        "migration",
        "AssemblyAI",
        "realtime transcription",
        "WebSocket",
        "SDK",
        "parameter mapping",
        "code migration",
        "switch provider"
      ],
      "use_cases": [
        "how to migrate from AssemblyAI to Gladia",
        "what are the equivalent parameters between AssemblyAI and Gladia",
        "how to switch realtime transcription from AssemblyAI",
        "how to map AssemblyAI events to Gladia events",
        "how to install Gladia SDK for migration"
      ],
      "tags": [
        "migration",
        "assemblyai",
        "realtime",
        "sdk",
        "websocket"
      ],
      "priority": 6,
      "content": "# Quick Migration Guide: Realtime STT from AssemblyAI to Gladia\n\nThis step-by-step guide shows how to switch your realtime transcription from AssemblyAI to Gladia with minimal code changes. It highlights equivalences, subtle differences, and drop-in replacements so you can migrate quickly and confidently, without any regressions.\n\n## Step-by-step Guide\n\n### Install the SDK\n\nInstall the official SDKs to enable realtime streaming. That's all you need to get started in Python or TypeScript.\n\n**For AssemblyAI:**\n```bash\npip install assemblyai\n```\n\n**For Gladia:**\n```bash\npip install gladiaio-sdk\n```\n\n### Initialize the Client\n\nCreate and authenticate the client that manages your live connection. The shape is the same idea across providers - just swap the client and key.\n\n**For AssemblyAI:**\n```python\nfrom assemblyai.streaming.v3 import StreamingClient, StreamingClientOptions\n\napi_key = \"<YOUR_ASSEMBLYAI_API_KEY>\"\n\nassemblyClient = StreamingClient(\n    StreamingClientOptions(\n        api_key=api_key,\n        api_host=\"streaming.assemblyai.com\",\n    )\n)\n```\n\n**For Gladia:**\n```python\nfrom gladiaio_sdk import GladiaClient\n\ngladia_client = GladiaClient(api_key=\"<YOUR_GLADIA_API_KEY>\")\n```\n\n### Configure the Session\n\nChoose the model, audio format, and language options your app needs. Most parameters map one-to-one, so your existing settings carry over naturally.\n\n#### AssemblyAI to Gladia Parameter Mapping\n\n| AssemblyAI | Gladia | Notes / Example |\n| --- | --- | --- |\n| `model` | `model` | Choose the latest Gladia model (\"solaria-1\"). |\n| `encoding` | `encoding` | Match the actual audio format (e.g., `linear16` <-> `wav/pcm`). |\n| | `bit_depth` | Choose the bit depth value from your audio |\n| `sample_rate` | `sample_rate` | Same unit (Hz). |\n| `channels` | `channels` | Same meaning. |\n| `interim_results` | `messages_config.receive_partial_transcripts` | Set `true` to receive partials messages. |\n| `endpointing` | `endpointing`; `maximum_duration_without_endpointing` | Port thresholds and consider a hard cap. |\n| `language` | `language_config.languages` (+ `code_switching`) | Pass one or more languages; enable switching when multiple languages are spoken. |\n\n**Gladia Config Example:**\n```json\n{\n  \"model\": \"solaria-1\",\n  \"encoding\": \"wav/pcm\",\n  \"bit_depth\": 16,\n  \"sample_rate\": 16000,\n  \"channels\": 1,\n  \"language_config\": { \"languages\": [\"en\"], \"code_switching\": false },\n  \"messages_config\": {\n    \"receive_partial_transcripts\": true,\n    \"receive_final_transcripts\": true\n  },\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 30,\n  \"realtime_processing\": {\n    \"custom_vocabulary\": false,\n    \"custom_spelling\": false\n  }\n}\n```\n\n### Start the Transcription Session\n\nOpen a live transcription session using your configuration.\n\n**For AssemblyAI:**\n```python\nfrom assemblyai.streaming.v3 import StreamingParameters\n\nassemblyClient.connect(\n    StreamingParameters(\n        sample_rate=16000,\n        format_turns=True,\n    )\n)\n```\n\n**For Gladia:**\n```python\ngladia_session = gladia_client.live_v2().start_session(gladia_config)\n```\n\n### Send Audio Chunks\n\nStream audio frames to the session as they are produced. Both SDKs accept small chunks continuously.\n\n**For AssemblyAI:**\n```python\nassemblyClient.stream(audio_chunk)\n```\n\n**For Gladia:**\n```python\ngladia_session.send_audio(audio_chunk)\n```\n\n### Read Transcription Messages\n\nEvent mapping from AssemblyAI to Gladia:\n- `Transcript` -> listen to Gladia `message` and branch on `message.data.is_final` to separate partial vs final results.\n- `Open`/`Close`/`Error` -> map to Gladia `started`/`ended`/`error`.\n\n**For Gladia:**\n```python\nfrom gladiaio_sdk import (\n    LiveV2WebSocketMessage,\n    LiveV2InitResponse,\n    LiveV2EndedMessage,\n)\n\n@live_session.on(\"message\")\ndef on_message(message: LiveV2WebSocketMessage):\n    # Partial and final transcripts are delivered here\n    # filter them with message.data.is_final field\n    print(message)\n\n@live_session.once(\"started\")\ndef on_started(_response: LiveV2InitResponse):\n    print(\"Session started. Listening...\")\n\n@live_session.once(\"ended\")\ndef on_ended(_ended: LiveV2EndedMessage):\n    print(\"Session ended.\")\n\n@live_session.on(\"error\")\ndef on_error(error: Exception):\n    print(f\"Error: {error}\")\n```"
    },
    {
      "title": "Migration from Deepgram to Gladia",
      "path": "migrations/from-deepgram",
      "url": "https://docs.gladia.io/chapters/migrations/from-deepgram",
      "keywords": [
        "migration",
        "Deepgram",
        "realtime transcription",
        "WebSocket",
        "SDK",
        "parameter mapping",
        "code migration",
        "switch provider"
      ],
      "use_cases": [
        "how to migrate from Deepgram to Gladia",
        "what are the equivalent parameters between Deepgram and Gladia",
        "how to switch realtime transcription from Deepgram",
        "how to map Deepgram events to Gladia events",
        "how to install Gladia SDK for migration"
      ],
      "tags": [
        "migration",
        "deepgram",
        "realtime",
        "sdk",
        "websocket"
      ],
      "priority": 6,
      "content": "# Quick Migration Guide: Realtime STT from Deepgram to Gladia\n\nThis step-by-step guide shows how to switch your realtime transcription from Deepgram to Gladia with minimal code changes. It highlights equivalences, subtle differences, and drop-in replacements so you can migrate quickly and confidently, without any regressions.\n\n## Step-by-step Guide\n\n### Install the SDK\n\nInstall the official SDKs to enable realtime streaming. That's all you need to get started in Python or TypeScript.\n\n**For Deepgram:**\n```bash\npip install deepgram-sdk\n```\n\n**For Gladia:**\n```bash\npip install gladiaio-sdk\n```\n\n### Initialize the Client\n\nCreate and authenticate the client that manages your live connection. The shape is the same idea across providers - just swap the client and key.\n\n**For Deepgram:**\n```python\nfrom deepgram import DeepgramClient\n\ndeepgram_client = DeepgramClient(api_key=\"<YOUR_DEEPGRAM_API_KEY>\")\n```\n\n**For Gladia:**\n```python\nfrom gladiaio_sdk import GladiaClient\n\ngladia_client = GladiaClient(api_key=\"<YOUR_GLADIA_API_KEY>\")\n```\n\n### Configure the Session\n\nChoose the model, audio format, and language options your app needs. Most parameters map one-to-one, so your existing settings carry over naturally.\n\n#### Deepgram to Gladia Parameter Mapping\n\n| Deepgram | Gladia | Notes / Example |\n| --- | --- | --- |\n| `model` | `model` | Choose the latest Gladia model (\"solaria-1\"). |\n| `encoding` | `encoding` | Match the actual audio format (e.g., `linear16` <-> `wav/pcm`). |\n| | `bit_depth` | Choose the bit depth value from your audio |\n| `sample_rate` | `sample_rate` | Same unit (Hz). |\n| `channels` | `channels` | Same meaning. |\n| `interim_results` | `messages_config.receive_partial_transcripts` | Set `true` to receive partials messages. |\n| `endpointing` | `endpointing`; `maximum_duration_without_endpointing` | Port thresholds and consider a hard cap. |\n| `language` | `language_config.languages` (+ `code_switching`) | Pass one or more languages; enable switching when multiple languages are spoken. |\n\n**Gladia Config Example:**\n```json\n{\n  \"model\": \"solaria-1\",\n  \"encoding\": \"wav/pcm\",\n  \"bit_depth\": 16,\n  \"sample_rate\": 16000,\n  \"channels\": 1,\n  \"language_config\": { \"languages\": [\"en\"], \"code_switching\": false },\n  \"messages_config\": {\n    \"receive_partial_transcripts\": true,\n    \"receive_final_transcripts\": true\n  },\n  \"endpointing\": 0.8,\n  \"maximum_duration_without_endpointing\": 30,\n  \"realtime_processing\": {\n    \"custom_vocabulary\": false,\n    \"custom_spelling\": false\n  }\n}\n```\n\n### Start the Transcription Session\n\nOpen a live transcription session using your configuration.\n\n**For Deepgram:**\n```python\nwith deepgram_client.listen.v1.connect(deepgram_config) as deepgram_connection:\n    # Further code to add event handlers\n    deepgram_connection.start_listening()\n```\n\n**For Gladia:**\n```python\ngladia_session = gladia_client.live_v2().start_session(gladia_config)\n```\n\n### Send Audio Chunks\n\nStream audio frames to the session as they are produced. Both SDKs accept small chunks continuously.\n\n**For Deepgram:**\n```python\ndeepgram_connection.send(audio_chunk)\n```\n\n**For Gladia:**\n```python\ngladia_session.send_audio(audio_chunk)\n```\n\n### Read Transcription Messages\n\nEvent mapping from Deepgram to Gladia:\n- `Transcript` -> listen to Gladia `message` and branch on `message.data.is_final` to separate partial vs final results.\n- `Open`/`Close`/`Error` -> map to Gladia `started`/`ended`/`error`.\n\n**For Deepgram:**\n```python\nfrom deepgram.core.events import EventType\n\ndef on_open(_):\n    print(\"Deepgram connection opened\")\n\ndef on_message(data, **kwargs):\n    print(data[\"channel\"][\"alternatives\"][0])\n\ndef on_close(data, **kwargs):\n    print(\"Deepgram connection closed\")\n\ndef on_error(error, **kwargs):\n    print(\"Error:\", error)\n\ndeepgram_connection.on(EventType.OPEN, on_open)\ndeepgram_connection.on(EventType.MESSAGE, on_message)\ndeepgram_connection.on(EventType.CLOSE, on_close)\ndeepgram_connection.on(EventType.ERROR, on_error)\n```\n\n**For Gladia:**\n```python\nfrom gladiaio_sdk import (\n    LiveV2WebSocketMessage,\n    LiveV2InitResponse,\n    LiveV2EndedMessage,\n)\n\n@live_session.on(\"message\")\ndef on_message(message: LiveV2WebSocketMessage):\n    # Partial and final transcripts are delivered here\n    # filter them with message.data.is_final field\n    print(message)\n\n@live_session.once(\"started\")\ndef on_started(_response: LiveV2InitResponse):\n    print(\"Session started. Listening...\")\n\n@live_session.once(\"ended\")\ndef on_ended(_ended: LiveV2EndedMessage):\n    print(\"Session ended.\")\n\n@live_session.on(\"error\")\ndef on_error(error: Exception):\n    print(f\"Error: {error}\")\n```"
    },
    {
      "title": "Named Entity Recognition",
      "path": "named-entity-recognition",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/named-entity-recognition",
      "keywords": [
        "named entity recognition",
        "NER",
        "entity extraction",
        "realtime",
        "websocket",
        "live streaming",
        "callback",
        "entity type",
        "utterance"
      ],
      "use_cases": [
        "how to extract named entities in real-time",
        "when to use NER callback event",
        "how to identify entity types in transcription",
        "how to process named entity recognition results",
        "when entities are detected in live audio"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "realtime",
        "NER",
        "audio-intelligence"
      ],
      "priority": 6,
      "content": "# Named Entity Recognition\n\nPayload definition for the callback event `live.named_entity_recognition`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.named_entity_recognition\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"error\": null,\n    \"type\": \"named_entity_recognition\",\n    \"data\": {\n      \"utterance_id\": \"00-00000011\",\n      \"utterance\": {\n        \"start\": 123,\n        \"end\": 123,\n        \"confidence\": 123,\n        \"channel\": 1,\n        \"words\": [\n          {\n            \"word\": \"<string>\",\n            \"start\": 123,\n            \"end\": 123,\n            \"confidence\": 123\n          }\n        ],\n        \"text\": \"<string>\",\n        \"language\": \"en\",\n        \"speaker\": 1\n      },\n      \"results\": [\n        {\n          \"entity_type\": \"<string>\",\n          \"text\": \"<string>\",\n          \"start\": 123,\n          \"end\": 123\n        }\n      ]\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.named_entity_recognition`\n- **Available options:** `live.named_entity_recognition`\n- **Example:** `\"live.named_entity_recognition\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, error, type, data with utterance_id, utterance, and results"
    },
    {
      "title": "Chapterization",
      "path": "post-chapterization",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/post-chapterization",
      "keywords": [
        "chapterization",
        "chapters",
        "post-processing",
        "websocket",
        "live session",
        "callback",
        "headline",
        "gist",
        "summary",
        "keywords"
      ],
      "use_cases": [
        "how to receive chapterization results",
        "when post-processing chapterization completes",
        "how to get chapter headlines and summaries",
        "how to extract keywords from chapters",
        "when to use chapter segmentation in live audio"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "post-processing",
        "chapterization"
      ],
      "priority": 6,
      "content": "# Chapterization\n\nPayload definition for the callback event `live.post_chapterization`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.post_chapterization\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"error\": null,\n    \"type\": \"post_chapterization\",\n    \"data\": {\n      \"results\": [\n        {\n          \"headline\": \"<string>\",\n          \"gist\": \"<string>\",\n          \"keywords\": [\"<string>\"],\n          \"start\": 123,\n          \"end\": 123,\n          \"sentences\": [\n            {\n              \"sentence\": \"<string>\",\n              \"start\": 123,\n              \"end\": 123,\n              \"words\": [\n                {\n                  \"word\": \"<string>\",\n                  \"start\": 123,\n                  \"end\": 123,\n                  \"confidence\": 123\n                }\n              ]\n            }\n          ],\n          \"text\": \"<string>\",\n          \"abstractive_summary\": \"<string>\",\n          \"extractive_summary\": \"<string>\",\n          \"summary\": \"<string>\"\n        }\n      ]\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.post_chapterization`\n- **Available options:** `live.post_chapterization`\n- **Example:** `\"live.post_chapterization\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, error, type, data with results array containing headline, gist, keywords, sentences, summaries"
    },
    {
      "title": "Final Transcript",
      "path": "post-final-transcript",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/post-final-transcript",
      "keywords": [
        "final transcript",
        "post-processing",
        "complete transcription",
        "websocket",
        "live session",
        "callback",
        "utterances",
        "metadata",
        "translation",
        "summarization"
      ],
      "use_cases": [
        "how to get the final complete transcript",
        "when post-processing final transcript is ready",
        "how to access full transcription with all features",
        "how to get audio metadata with transcript",
        "when final transcript includes translation and summarization"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "post-processing",
        "transcript"
      ],
      "priority": 6,
      "content": "# Final Transcript\n\nPayload definition for the callback event `live.post_final_transcript`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.post_final_transcript\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"post_final_transcript\",\n    \"data\": {\n      \"metadata\": {\n        \"audio_duration\": 3600,\n        \"number_of_distinct_channels\": 1,\n        \"billing_time\": 3600,\n        \"transcription_time\": 20\n      },\n      \"transcription\": {\n        \"full_transcript\": \"<string>\",\n        \"languages\": [\"en\"],\n        \"utterances\": [...],\n        \"sentences\": [...],\n        \"subtitles\": [...]\n      },\n      \"translation\": {...},\n      \"summarization\": {...},\n      \"named_entity_recognition\": {...},\n      \"sentiment_analysis\": {...},\n      \"chapterization\": {...}\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.post_final_transcript`\n- **Available options:** `live.post_final_transcript`\n- **Example:** `\"live.post_final_transcript\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, type, data with metadata, transcription, translation, summarization, named_entity_recognition, sentiment_analysis, chapterization"
    },
    {
      "title": "Summarization",
      "path": "post-summarization",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/post-summarization",
      "keywords": [
        "summarization",
        "summary",
        "post-processing",
        "websocket",
        "live session",
        "callback",
        "text summary",
        "audio intelligence"
      ],
      "use_cases": [
        "how to receive summarization results",
        "when post-processing summarization completes",
        "how to get summary of live transcription",
        "how to handle summarization callback event",
        "when to use summarization in live audio"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "post-processing",
        "summarization"
      ],
      "priority": 6,
      "content": "# Summarization\n\nPayload definition for the callback event `live.post_summarization`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.post_summarization\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"error\": null,\n    \"type\": \"post_summarization\",\n    \"data\": {\n      \"results\": \"<string>\"\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.post_summarization`\n- **Available options:** `live.post_summarization`\n- **Example:** `\"live.post_summarization\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, error, type, data with results string"
    },
    {
      "title": "Post Transcript",
      "path": "post-transcript",
      "url": "https://docs.gladia.io/api-reference/v2/live/callback/post-transcript",
      "keywords": [
        "post transcript",
        "transcript",
        "post-processing",
        "websocket",
        "live session",
        "callback",
        "utterances",
        "sentences",
        "subtitles"
      ],
      "use_cases": [
        "how to receive post-processed transcript",
        "when transcript post-processing completes",
        "how to get utterances and sentences from transcript",
        "how to generate subtitles from live transcript",
        "when to handle post_transcript callback event"
      ],
      "tags": [
        "api-reference",
        "live",
        "callback",
        "events",
        "post-processing",
        "transcript"
      ],
      "priority": 6,
      "content": "# Post Transcript\n\nPayload definition for the callback event `live.post_transcript`.\n\n## Example\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"event\": \"live.post_transcript\",\n  \"payload\": {\n    \"session_id\": \"4a39145c-2844-4557-8f34-34883f7be7d9\",\n    \"created_at\": \"2021-09-01T12:00:00.123Z\",\n    \"type\": \"post_transcript\",\n    \"data\": {\n      \"full_transcript\": \"<string>\",\n      \"languages\": [\"en\"],\n      \"utterances\": [\n        {\n          \"start\": 123,\n          \"end\": 123,\n          \"confidence\": 123,\n          \"channel\": 1,\n          \"words\": [\n            {\n              \"word\": \"<string>\",\n              \"start\": 123,\n              \"end\": 123,\n              \"confidence\": 123\n            }\n          ],\n          \"text\": \"<string>\",\n          \"language\": \"en\",\n          \"speaker\": 1\n        }\n      ],\n      \"sentences\": [...],\n      \"subtitles\": [\n        {\n          \"format\": \"srt\",\n          \"subtitles\": \"<string>\"\n        }\n      ]\n    }\n  }\n}\n```\n\n## Schema\n\n### id\n- **Type:** string<uuid>\n- **Required:** yes\n- **Description:** Id of the job\n- **Example:** `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n### event\n- **Type:** enum<string>\n- **Required:** yes\n- **Default:** `live.post_transcript`\n- **Available options:** `live.post_transcript`\n- **Example:** `\"live.post_transcript\"`\n\n### payload\n- **Type:** object\n- **Required:** yes\n- **Description:** The live message payload as sent to the WebSocket\n- **Contains:** session_id, created_at, type, data with full_transcript, languages, utterances, sentences, subtitles"
    },
    {
      "title": "Delete transcription (Deprecated)",
      "path": "transcription-delete",
      "url": "https://docs.gladia.io/api-reference/v2/transcription/delete",
      "keywords": [
        "transcription",
        "delete",
        "DELETE",
        "remove",
        "cleanup",
        "deprecated",
        "data"
      ],
      "use_cases": [
        "How to delete a transcription from Gladia?",
        "How to remove transcription data?",
        "When to cleanup old transcriptions?",
        "How to delete audio files from Gladia?"
      ],
      "tags": [
        "api-reference",
        "transcription",
        "deprecated",
        "DELETE"
      ],
      "priority": 3,
      "content": "# Delete transcription (Deprecated)\n\n**DEPRECATED**: Prefer the more specific [pre-recorded endpoint](/api-reference/v2/pre-recorded/delete).\n\nDelete a transcription and all its data (audio file, transcription).\n\n## Endpoint\n\n```\nDELETE /v2/transcription/{id}\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the transcription job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Request\n\n```bash\ncurl --request DELETE \\\n  --url https://api.gladia.io/v2/transcription/{id} \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response\n\n- **202**: The transcription job has been successfully deleted\n\n## Error Responses\n\n- **401**: Unauthorized - Invalid API key\n  ```json\n  {\n    \"timestamp\": \"2023-12-28T09:04:17.210Z\",\n    \"path\": \"/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab\",\n    \"request_id\": \"G-821fe9df\",\n    \"statusCode\": 401,\n    \"message\": \"gladia key not found\"\n  }\n  ```\n- **403**: Forbidden\n- **404**: Not Found - Transcription not found"
    },
    {
      "title": "Get transcription result (Deprecated)",
      "path": "transcription-get",
      "url": "https://docs.gladia.io/api-reference/v2/transcription/get",
      "keywords": [
        "transcription",
        "get",
        "result",
        "status",
        "GET",
        "metadata",
        "deprecated",
        "polling"
      ],
      "use_cases": [
        "How to get transcription results from Gladia?",
        "How to check transcription job status?",
        "How to poll for transcription completion?",
        "When is a transcription job done?",
        "How to retrieve transcription metadata?"
      ],
      "tags": [
        "api-reference",
        "transcription",
        "deprecated",
        "GET",
        "result"
      ],
      "priority": 3,
      "content": "# Get transcription result (Deprecated)\n\n**DEPRECATED**: Prefer the more specific [pre-recorded endpoint](/api-reference/v2/pre-recorded/get).\n\nGet transcription's status, parameters and result.\n\n## Endpoint\n\n```\nGET /v2/transcription/{id}\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the transcription job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/transcription/{id} \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response (200 - OK)\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"request_id\": \"G-45463597\",\n  \"version\": 2,\n  \"kind\": \"pre-recorded\",\n  \"created_at\": \"2023-12-28T09:04:17.210Z\",\n  \"status\": \"queued\",\n  \"file\": {\n    \"id\": \"f0dcZE10-23d8-47f0-a25d-74a6eed88721\",\n    \"filename\": \"split_infinity.wav\",\n    \"source\": \"http://files.gladia.io/example/audio-transcription/split_infinity.wav\",\n    \"audio_duration\": 20,\n    \"number_of_channels\": 1\n  },\n  \"request_params\": {\n    \"audio_url\": \"http://files.gladia.io/example/audio-transcription/split_infinity.wav\",\n    \"subtitles\": false,\n    \"diarization\": false,\n    \"translation\": false,\n    \"summarization\": false\n  },\n  \"completed_at\": null,\n  \"custom_metadata\": null,\n  \"error_code\": null,\n  \"result\": null\n}\n```\n\n### Response Fields\n\n- **id** (string, uuid, required): Id of the job\n- **request_id** (string, required): Debug id\n- **version** (integer, required): API version\n- **status** (enum, required): Job status\n  - Available options: `queued`, `processing`, `done`, `error`\n- **created_at** (string, datetime, required): Creation date\n- **kind** (enum, default: pre-recorded, required): Available options: `pre-recorded`\n- **completed_at** (string, datetime | null): Completion date when status is \"done\" or \"error\"\n- **custom_metadata** (object): Custom metadata given in the initial request\n- **error_code** (integer | null): HTTP status code of the error if status is \"error\" (range: 400-599)\n- **file** (object): The file data you uploaded. Can be null if status is \"error\"\n- **request_params** (object): Parameters used for this pre-recorded transcription\n- **result** (object): Pre-recorded transcription's result when status is \"done\"\n\n## Status Values\n\n- **queued**: The job has been queued\n- **processing**: The job is being processed\n- **done**: The job has been processed and the result is available\n- **error**: An error occurred during the job's processing\n\n## Error Responses\n\n- **401**: Unauthorized - Invalid API key\n- **404**: Not Found - Transcription not found"
    },
    {
      "title": "Download audio file (Deprecated)",
      "path": "transcription-get-audio",
      "url": "https://docs.gladia.io/api-reference/v2/transcription/get-audio",
      "keywords": [
        "transcription",
        "audio",
        "download",
        "file",
        "GET",
        "binary",
        "deprecated"
      ],
      "use_cases": [
        "How to download the audio file from a transcription?",
        "How to retrieve the original audio from Gladia?",
        "When to download transcription audio files?"
      ],
      "tags": [
        "api-reference",
        "transcription",
        "deprecated",
        "GET",
        "audio"
      ],
      "priority": 3,
      "content": "# Download audio file (Deprecated)\n\n**DEPRECATED**: Prefer the more specific [pre-recorded endpoint](/api-reference/v2/pre-recorded/get-audio).\n\nDownload the audio file used on a transcription.\n\n## Endpoint\n\n```\nGET /v2/transcription/{id}/file\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Path Parameters\n\n- **id** (string, required): Id of the transcription job\n  - Example: `\"45463597-20b7-4af7-b3b3-f5fb778203ab\"`\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url https://api.gladia.io/v2/transcription/{id}/file \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response (200 - OK)\n\n- **Content-Type**: application/octet-stream\n- **Response**: Binary file data\n\n## Error Responses\n\n- **401**: Unauthorized - Invalid API key\n- **404**: Not Found - Transcription or file not found"
    },
    {
      "title": "Initiate a transcription (Deprecated)",
      "path": "transcription-init",
      "url": "https://docs.gladia.io/api-reference/v2/transcription/init",
      "keywords": [
        "transcription",
        "init",
        "POST",
        "audio_url",
        "diarization",
        "translation",
        "summarization",
        "callback",
        "deprecated",
        "pre-recorded"
      ],
      "use_cases": [
        "How to start a transcription job with Gladia?",
        "How to enable diarization in transcription?",
        "How to configure callbacks for transcription results?",
        "When to use custom vocabulary in transcription?",
        "How to enable translation during transcription?"
      ],
      "tags": [
        "api-reference",
        "transcription",
        "deprecated",
        "POST",
        "pre-recorded"
      ],
      "priority": 3,
      "content": "# Initiate a transcription (Deprecated)\n\n**DEPRECATED**: Prefer the more specific [pre-recorded endpoint](/api-reference/v2/pre-recorded/init).\n\nInitiate a pre-recorded transcription job. Use the returned id and the GET /v2/transcription/:id endpoint to obtain the results.\n\n## Endpoint\n\n```\nPOST /v2/transcription\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Request Body (application/json)\n\n### Required Parameters\n\n- **audio_url** (string, uri, required): URL to a Gladia file or to an external audio or video file\n  - Example: `\"http://files.gladia.io/example/audio-transcription/split_infinity.wav\"`\n\n### Optional Parameters\n\n- **context_prompt** (string, deprecated): Context to feed the transcription model with for possible better accuracy\n- **custom_vocabulary** (boolean, default: false): Enable custom vocabulary for this audio\n- **custom_vocabulary_config** (object): Custom vocabulary configuration\n  - **vocabulary** (array): List of vocabulary items\n  - **default_intensity** (number): Default intensity value\n- **detect_language** (boolean, default: true, deprecated): Use `language_config` instead. Detect the language from the given audio\n- **enable_code_switching** (boolean, default: false, deprecated): Use `language_config` instead. Detect multiple languages\n- **language** (enum, deprecated): Use `language_config` instead. Set the spoken language (ISO 639 standard)\n- **callback_url** (string, uri, deprecated): Use `callback`/`callback_config` instead\n- **callback** (boolean, default: false): Enable callback for this transcription\n- **callback_config** (object): Customize the callback behaviour (url and http method)\n- **subtitles** (boolean, default: false): Enable subtitles generation\n- **subtitles_config** (object): Configuration for subtitles generation\n- **diarization** (boolean, default: false): Enable speaker recognition (diarization)\n- **diarization_config** (object): Speaker recognition configuration\n- **translation** (boolean, default: false, beta): Enable translation\n- **translation_config** (object, beta): Translation configuration\n- **summarization** (boolean, default: false, beta): Enable summarization\n- **summarization_config** (object, beta): Summarization configuration\n- **moderation** (boolean, default: false, alpha): Enable moderation\n- **named_entity_recognition** (boolean, default: false, alpha): Enable named entity recognition\n- **chapterization** (boolean, default: false, alpha): Enable chapterization\n- **name_consistency** (boolean, default: false, alpha): Enable names consistency\n- **custom_spelling** (boolean, default: false, alpha): Enable custom spelling\n- **structured_data_extraction** (boolean, default: false, alpha): Enable structured data extraction\n- **sentiment_analysis** (boolean, default: false): Enable sentiment analysis\n- **audio_to_llm** (boolean, default: false, alpha): Enable audio to LLM processing\n- **custom_metadata** (object): Custom metadata you can attach to this transcription\n- **sentences** (boolean, default: false): Enable sentences\n- **display_mode** (boolean, default: false, alpha): Change output display mode\n- **punctuation_enhanced** (boolean, default: false, alpha): Use enhanced punctuation\n- **language_config** (object): Specify the language configuration\n\n## Example Request\n\n```bash\ncurl --request POST \\\n  --url https://api.gladia.io/v2/transcription \\\n  --header 'Content-Type: application/json' \\\n  --header 'x-gladia-key: <api-key>' \\\n  --data '{\n    \"audio_url\": \"http://files.gladia.io/example/audio-transcription/split_infinity.wav\",\n    \"diarization\": true,\n    \"subtitles\": true\n  }'\n```\n\n## Response (201 - Created)\n\n```json\n{\n  \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n  \"result_url\": \"https://api.gladia.io/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab\"\n}\n```\n\n### Response Fields\n\n- **id** (string, uuid, required): Id of the job\n- **result_url** (string, uri, required): Prebuilt URL with your transcription id to fetch the result\n\n## Error Responses\n\n- **400**: Bad Request\n- **401**: Unauthorized - Invalid API key\n- **422**: Unprocessable Entity"
    },
    {
      "title": "List transcriptions (Deprecated)",
      "path": "transcription-list",
      "url": "https://docs.gladia.io/api-reference/v2/transcription/list",
      "keywords": [
        "transcription",
        "list",
        "GET",
        "pagination",
        "filter",
        "status",
        "deprecated",
        "query"
      ],
      "use_cases": [
        "How to list all transcriptions in Gladia?",
        "How to filter transcriptions by status?",
        "How to paginate through transcription results?",
        "How to filter transcriptions by date?",
        "How to find transcriptions with custom metadata?"
      ],
      "tags": [
        "api-reference",
        "transcription",
        "deprecated",
        "GET",
        "list"
      ],
      "priority": 3,
      "content": "# List transcriptions (Deprecated)\n\n**DEPRECATED**: Prefer the more specific [pre-recorded endpoint](/api-reference/v2/pre-recorded/list).\n\nList all the transcriptions matching the parameters.\n\n## Endpoint\n\n```\nGET /v2/transcription\n```\n\n## Authorization\n\n- **x-gladia-key** (string, header, required): Your personal Gladia API key\n\n## Query Parameters\n\n- **offset** (integer, default: 0): The starting point for pagination. A value of 0 starts from the first item. (min: 0)\n- **limit** (integer, default: 20): The maximum number of items to return. (min: 1)\n- **date** (string, datetime): Filter items relevant to a specific date in ISO format (YYYY-MM-DD)\n  - Example: `\"2026-01-12\"`\n- **before_date** (string, datetime): Include items that occurred before the specified date in ISO format\n  - Example: `\"2026-01-12T00:00:09.657Z\"`\n- **after_date** (string, datetime): Filter for items after the specified date. Use with `before_date` for a range\n- **status** (enum[]): Filter the list based on item status. Accepts multiple values\n  - Available options: `queued`, `processing`, `done`, `error`\n- **custom_metadata** (object): Filter by custom metadata\n  - Example: `{ \"user\": \"John Doe\" }`\n- **kind** (enum[]): Filter the list based on the item type\n  - Available options: `pre-recorded`, `live`\n\n## Example Request\n\n```bash\ncurl --request GET \\\n  --url 'https://api.gladia.io/v2/transcription?limit=20' \\\n  --header 'x-gladia-key: <api-key>'\n```\n\n## Response (200 - OK)\n\n```json\n{\n  \"first\": \"https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20\",\n  \"current\": \"https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20\",\n  \"next\": \"https://api.gladia.io/v2/transcription?status=done&offset=20&limit=20\",\n  \"items\": [\n    {\n      \"id\": \"45463597-20b7-4af7-b3b3-f5fb778203ab\",\n      \"request_id\": \"G-45463597\",\n      \"version\": 2,\n      \"status\": \"done\",\n      \"created_at\": \"2023-12-28T09:04:17.210Z\",\n      \"kind\": \"pre-recorded\",\n      \"completed_at\": \"2023-12-28T09:04:37.210Z\",\n      \"file\": {\n        \"id\": \"<string>\",\n        \"filename\": \"<string>\",\n        \"source\": \"<string>\",\n        \"audio_duration\": 3600,\n        \"number_of_channels\": 1\n      },\n      \"result\": {\n        \"metadata\": {\n          \"audio_duration\": 3600,\n          \"number_of_distinct_channels\": 1,\n          \"billing_time\": 3600,\n          \"transcription_time\": 20\n        },\n        \"transcription\": {\n          \"full_transcript\": \"<string>\",\n          \"languages\": [\"en\"],\n          \"utterances\": []\n        }\n      }\n    }\n  ]\n}\n```\n\n### Response Fields\n\n- **first** (string, uri, required): URL to fetch the first page\n- **current** (string, uri, required): URL to fetch the current page\n- **next** (string, uri | null, required): URL to fetch the next page\n- **items** (array, required): List of transcriptions\n\n## Error Responses\n\n- **401**: Unauthorized - Invalid API key"
    }
  ]
}